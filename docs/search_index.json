[["index.html", "Case Studies Preface", " Case Studies 2021-05-13 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw and ML Case Studies during a case study a year ago. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References "],["explainable-artificial-intelligence.html", "Chapter 1 Explainable Artificial Intelligence", " Chapter 1 Explainable Artificial Intelligence Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. References "],["xai1-explainable-cards.html", "1.1 Explaining Credit Card Customers churns", " 1.1 Explaining Credit Card Customers churns Authors: Katarzyna Solawa, Przemysław Chojecki, Bartosz Sawicki (Warsaw University of Techcnology) "],["ml-in-predition-of-real-estate-prices.html", "1.2 ML in predition of real estate prices", " 1.2 ML in predition of real estate prices Authors: Sebastian Deręgowski, Maciej Gryszkiewicz, Paweł Morgen (Warsaw University of Technology) 1.2.1 Abstract Lorem ipsum 1.2.2 Introduction Lorem ipsum 1.2.3 Related Work Lorem ipsum 1.2.4 Methodology Lorem ipsum 1.2.5 Results Lorem ipsum 1.2.6 Summary and conclusions Lorem ipsum "],["xai-heart-disease.html", "1.3 How not to have broken heart &lt;3", " 1.3 How not to have broken heart &lt;3 Authors: Przybyłek Paulina, Rólkiewicz Renata, Słowakiewicz Patryk 1.3.1 Introduction "],["xai1-explainable-wine.html", "1.4 Wines", " 1.4 Wines Authors: Jakub Kosterna, Bartosz Siński, Jan Smoleń "],["xai1-explainable-hotels.html", "1.5 eXplaining predictions of booking cancelations", " 1.5 eXplaining predictions of booking cancelations Authors: Mateusz Krzyziński, Anna Urbala, Artur Żółkowski (Warsaw University of Technology) 1.5.1 Introduction Introduction 1.5.2 Dataset and models 1.5.3 Local explanations In order to explain model output for a particular guest and their booking, we used instance-level exploration methods, such as Break-down, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), CP (Ceteris Paribus). We decided to investigate noteworthy predictions, i.e. false positive and false negative (respectively canceled bookings predicted as not canceled and vice versa), the most valid (the predictions the model was most sure of), and the closest to decision boundary. 1.5.3.1 False positive and false negative predictions We might discover that the model is providing incorrect predictions. The key is to find the reasons for this, that is, to answer the question what has driven the wrong prediction. We used local explanations methods for the observations in both groups with the worst predictions, i.e. the lowest probability of proper classification. FIGURE 1.1: A plot of Shapley values for random forest model and misclassified observation (false positive) with the highest probability of cancellation. The green and red bars correspond to the contribution of the variable to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). On the x-axis, there is the model prediction value, while on the y-axis there are variables and their values for the observation. Informations with the biggest contribution to the final prediction are guest’s country of origin (Portugal), a total number of special requests equals zero, and the fact that customer type related to the given booking is Transient-Party. This is an indication that the model may be slightly biased due to the country of origin. It is the property of the customer and not of the booking itself. Thus, depending on the application, it is worth considering whether this response is satisfactory and meets ethical standards. With every value contributes to the misprediction, the only feature with the correct contribution is customer type (Transient-Party guests are the most popular type of customers, accounting for as much as 75% of bookers). FIGURE 1.2: A plot of Shapley values for random forest model and misclassified observation (false negative) with the lowest probability of cancellation. The elements of the plot have the same meaning as in the previous case. It can be seen that the largest contributions are related to the country of origin of the booker and the type of guest assigned to them. It is worth noting that the values of these variables are the same as in the case of the observation analyzed above. Again, they contribute to the same side of the prediction, but the contribution values are different. In this case, the type of client turns out to be the most important. Most of the values also affect the prediction of no cancellation. It is interesting that a slightly later booking in relation to the date of stay (lead time) has the opposite effect than in the previous example. The reason is the dependencies between the variables. FIGURE 1.3: A plot of LIME model values for the random forest model and the most misslassified observations. The similarity between the observations is also noticeable in the lime method. The first five variables are identical and have almost the same coefficients. Therefore, it is these less significant variables that influence the final prediction. In the case of these two observations with similar characteristics, but completely different predictions, the use of the SHAP method (generalizing the breakdown method) gives a better picture. The Glass-box model selected in LIME method to approximate the black-box model, and not the data themselves, is not able to capture dependencies between variables. 1.5.3.2 The most valid predictions The considered model returns an appropriate prediction in over 89% of cases. However, the level of certainty of the model with respect to the prediction (i.e. the probability that an observation is assigned to a class) may be different. Thus, it is worth considering why the model is almost sure of some outputs and how would the model’s predictions change if the values of some of the explanatory variables changed. We used local explanations methods for the observations in both groups with the best predictions, i.e. the highest probability of proper classification (equal to 1.0). FIGURE 1.4: A plot of Shapley values for random forest model and observation with sure negative prediction. The elements of the plot were described above. Like in the previous examples - the largest contribution has the country, in that case: France. Again, this is a Transient-Party customer and that also affected the prediction. Also, no special requests affect negatively to prediction (more than eg. no previous cancellations). Only one of the top variables affected positively: it was no required car parking spaces, but this impact was unnoticeable in the final prediction. FIGURE 1.5: A plot of Shapley values for random forest model and observation with sure positive prediction. The elements of the plot were described above. Again, Portugal as a country of origin affected positively the probability of cancelation (keep in mind that the hotel is in Portugal, so we can assume that compatriots cancel their reservations more often). Also, no special requests affected positively on prediction (although in the previous case it had a negative effect). We can notice that for positive prediction other factors have the biggest impact than for negative. Eg. a longer lead time moved up to third place and now has a positive impact. FIGURE 1.6: Ceteris-paribus profiles for the selected continuous explanatory variables and label encoded country variable for the random forest model and observations with the sure prediction. Dots indicate the values of the variables and the values of the predictions for observations. Green profiles are for sure positive prediction (a cancellation), while blue profiles are for sure negative prediction. Looking at the ceteris paribus profiles, it is intuitive to see that the prediction for the observation classified as the not canceled stay is more stable, i.e. less sensitive to changes in the values of explanatory variables. In the case of observation of canceled reservations, a change of the arrival date by a few weeks would cause a significant decrease in the certainty of the prediction. It is related to the seasonality of bookings (the decrease occurs at the beginning of July - the holiday period). However, the biggest changes in the prediction for this observation could be due to noting the fact of additional booking requirements (required car parking spaces and a total number of special requests). Changing these values to non-zero would change the prediction completely. Moreover, the huge changes depend on the country of origin of the booker, which in this case is Portugal. When considering the prediction for an observation classified as not canceled, we see that the only explanatory variable whose change would have a significant impact on the certainty of the prediction is the number of previously canceled reservations. A change to any non-zero value would change the prediction, but its certainty would be close to the decision boundary. 1.5.3.3 The closest to decision boundary predictions We analyzed the situations when the model is sure of the returned output. However, the observations for which the prediction was uncertain, close to the decision limit, are also worth considering. We might want to know the answer to the question of whether it is a matter of similar numbers of explanatory variables shifting the prediction in different directions, or maybe there are variables that do not fit the whole picture, and therefore the model is not certain. It is also worth checking how much such predictions fluctuate depending on the changes in the explanatory variables. (#fig:shap_negative_50)A plot of Shapley values for random forest model and observation classified as negative with probability near 50%. The elements of the plot were described above. This is a very interesting example. One variable fixed prediction. Very short lead time (one day) opposed all other factors like Portugal as country of origin or no special requests and made the model predict correctly. It is amazing, that one factor can change everything. (#fig:shap_positive_50)A plot of Shapley values for random forest model and observation classified as positive with probability near 50%. The elements of the plot were described above. This observation is not so exciting as the previous one, but it is the next evidence that the special requests decrease the probability that the client will cancel the reservation. Nevertheless, this observation was classified as positive. Agent was the most important positive variable although in the previous examples he did not have such a contribution. But… agent has that huge contribution only in Shapley. (#fig:lime_positive_50)A plot of LIME model values for the random forest model and the same observation. Here agent has much less impact. This is a reminder for us that each method works differently and takes different variables into account. It is worth remembering this. In this method for that (and a lot of other) observation the most important factor is no previous cancelations, but it is not enough for the model to make a negative decision. 1.5.4 Global explanations 1.5.5 Summary and conclusions "],["explainable-artificial-inteligence-r.html", "Chapter 2 Explainable artificial inteligence (R)", " Chapter 2 Explainable artificial inteligence (R) "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 "],["axondeepseg-automatic-axon-and-myelin-segmentation-from-microscopy-data-using-convolutional-neural-networks.html", "4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks", " 4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks Authors: Jakubowski Mikołaj, Tomaszewski Patryk, Ziemła Mateusz 4.1.1 Introduction "],["ara-cnn-a-bayesian-deep-learning-model-intended-for-histopathological-image-classification-.html", "4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification.", " 4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification. *Authors: Wojciech Szczypek, Jakub Lis, Jan Gąska (Warsaw University of Techcnology) "],["rethinking-the-u-net-architecture-for-multimodal-biomedical-image-segmentation.html", "4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation", " 4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation Grudzień Adrianna, Łukaszyk Marcin, Piasecki Michał 4.3.1 Introduction Tutaj będzie tekst kiedyś "],["dl2-rmdl-unet.html", "4.4 Analyzing Reproducibility churns", " 4.4 Analyzing Reproducibility churns Authors: Marceli Korbin, Szymon Szmajdziński, Paweł Wojciechowski (Warsaw University of Techcnology) "],["title.html", "4.5 Title", " 4.5 Title Authors: Filip Chrzuszcz, Szymon Rećko, Mateusz Sperkowski (Warsaw University of Technology) 4.5.1 Title, Authors, Abstract, Keywords 4.5.2 Introduction 4.5.3 Related Literature 4.5.4 Methods 4.5.5 Result Skrot do pierwszego projektu: Despite lower results than in the first paper, in most datasets we still achieved better results than the baselines paper attempted to beat. The ones that we weren’t able to reproduce where either limits of processing power, or could be assigned to effect off randomness which is basis od this paper. The authors unfortunately didn’t include their randomness results, therefore their exact calculations aren’t reproducible. 4.5.6 Discussion 4.5.7 Conclusion 4.5.8 References 4.5.9 Random Multimodel Deep Learning for Classification Results .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset WOS-5736 WOS-11967 WOS-46985 Reuters-21578 Score Source Paper Repr. Paper Repr. Paper Repr. Paper Repr. RMDL 3 RDLs 90.86 89.37 87.39 84.25 78.39 — 89.10 87.64 9 RDLs 92.60 89.28 90.65 — 81.92 — 90.36 89.83 15 RDLs 92.66 — 91.01 — 81.86 — 89.91 — 30 RDLs 93.57 — 91.59 — 82.42 — 90.69 — Table 1 4.5.9.1 Reuters-21578 Paper’s Plots Our Reproduction Figure 1 4.5.9.2 WOS-5736 Paper’s Plots Our Reproduction Figure 2 .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset IMDB 20NewsGroup Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 89.91 88.49 86.73 — 9 RDLs 90.13 — 87.62 — 15 RDLs 90.79 — 87.91 — Table 2 ERROR RATE 1-Accuracy .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset MNIST CIFAR-10 Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 0.51 0.55 9.89 38.23 9 RDLs 0.41 0.65 9.1 36.91 15 RDLs 0.21 — 8.74 — 30 RDLs 0.18 — 8.79 — Table 3 4.5.9.3 CIFAR 10 Paper’s Plots Our Reproduction Figure 3 4.5.9.4 MNIST Paper’s Plots Our Reproduction Figure 4 4.5.10 Adversarial Attacks Against Medical Deep Learning Systems "],["machine-learning.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following chapters introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: 1. TBA 2. TBA 3. TBA 4. TBA "],["validation-and-comparison-of-covid-19-mortatility-prediction-models-on-multi-source-data.html", "5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data", " 5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data Authors: Komorowski Michał, Olender Przemysław, Sieńko Piotr, Welkier Konrad 5.1.1 Introduction "],["one-model-to-fit-them-all-covid-19-mortality-prediction-using-multinational-data.html", "5.2 One model to fit them all: COVID-19 mortality prediction using multinational data", " 5.2 One model to fit them all: COVID-19 mortality prediction using multinational data Authors: Kurek Marcelina, Stączek Mateusz, Wiśniewski Jakub, Zdulska Hanna 5.2.1 Introduction "],["transparent-machine-learning-to-support-predicting-covid-19-infection-risk-based-on-chronic-diseases.html", "5.3 Transparent Machine Learning to Support Predicting COVID-19 Infection Risk Based on Chronic Diseases", " 5.3 Transparent Machine Learning to Support Predicting COVID-19 Infection Risk Based on Chronic Diseases Authors: Hubert Ruczyński, Dawid Przybyliński, Kinga Ulasik 5.3.1 Introduction "],["comparison-of-deep-learning-and-tree-based-models-in-the-clinical-prediction-of-the-course-of-covid-19-illness.html", "5.4 Comparison of deep learning and tree-based models in the clinical prediction of the course of COVID-19 illness", " 5.4 Comparison of deep learning and tree-based models in the clinical prediction of the course of COVID-19 illness Authors: Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk 5.4.1 Abstract The COVID-19 pandemic overwhelmed medical staff around the world, showing that effective and explainable models are needed to help allocate limited resources to those in need. Many published models for predicting COVID-19 related ICU admission and mortality were tree-based models (Yan et al. 2020) or neural network models (Li et al. 2020). We compared the two architectures in effectiveness, explainability and reproducibility. The two architectures appear to be similar with regards to their effectiveness, but the DNN model had significant reproducibility issues and worse explainability. 5.4.2 Introduction 5.4.3 Methods 5.4.4 Results 5.4.5 Discussion References "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML "],["topic.html", "6.1 Topic", " 6.1 Topic Authors: Adrian Stańdo, Maciej Pawlikowski, Mariusz Słapek (Warsaw University of Technology) 6.1.1 Literature review Rashomon is a intriguing Japanese movie in which four people witness an incident from different vantage points. When they come to testify in court, they all report the same facts, but their stories of what happened are very different. In machine learning Rashomon set is used to characterise problem in which many different models offer accurate results describing the same data. However, not every accurate model gives a right conclusion as described in (Breiman and others 2001): “If the model is a poor emulation of nature, the conclusion might be worng”. Herein authors also explain basics of Rashomon sets on example. Much more in depth and mathematical description is provided in (Semenova, Rudin, and Parr 2019). Another important topic related to Rashomon sets is analysing the feature importance of the model. It was described in this article (Fisher, Rudin, and Dominici 2019), where authors suggested to study the maximum and minimum of variable importance across all models included in the Rashomon set. This technique was called MCR (Model Class Reliance). Furthermore, (Dong and Rudin 2020) presented technique to visualise the “cloud” of variable importance for models in the set, which could help us understand the Rashomon set and choose the one which give the best interpretation. The last question stated in the article (Rudin et al. 2021) was about choosing model from the Rashomon set. It might be a difficult task, especially when we lack good exploration tools. (Das et al. 2019) created a system called BEAMS that allows to choose the most important features. Next, the program searches the hypothesis space in order to find model which fits best to given constraints. Since this system works only with linear regression classifiers, (Rudin et al. 2021) stated a question if it is possible to design a simmilar system which will search only models within the Rashomon set. References "],["title-1.html", "6.2 Title", " 6.2 Title Authors: Jan Borowski, Konstanty Kraszewski, Krzysztof Wolny 6.2.1 Literature review In 1950 Japanese director, Akira Kurosawa, presented film Rashomon. Movie resolves around four witnesses, that describes the same crime in four different ways. This situation was called Rashomon effect after the name of the movie. In other words Rashomon effect is a situation when we have multiple different descriptions to the same event. This term is commonly used in multiple sciences like sociology, psychology or history. At the begging of the 21st century Rashomon effect was introduced to predictive modelling by Leo Breiman and his work ‘Statistical modeling: The Two Cultures’(Breiman and others 2001). In this article he named Rashomon effect situation, where there are many approximately-equally accurate models. Although these models have similar results, they can differ, when it comes to the way they managed to achieve it. Breiman called for closer examination of the Rashomon effect and conclusions that can be drawn from it. Recently, we can observe growing interest in Rashomon effect, although there is still a lot to be discovered. One of the articles, that bring closer the problem is ‘A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning’[6-0-rashomon-intro]. It provides several approaches for estimating the size of the Rashomon effect as well as the usefulness of the Rashomon curve in model selection. References "],["roshomon-sets-of-in-hospital-mortality-prediction-random-forest-models.html", "6.3 Roshomon sets of in-hospital mortality prediction random forest models", " 6.3 Roshomon sets of in-hospital mortality prediction random forest models Authors: Jeugeniusz Winiczenko, Mikołaj Malec, Patryk Wrona (Warsaw University of Techcnology) 6.3.1 Related work Rashomon sets are sets of models performing extraordinarily well on a given task. In machine learning, this term was used for the very first time by Leo Breiman in his paper issued in 2001 (Breiman and others 2001). Just as the task could be any, like in our case predicting patient’s mortality, the use of given features in order to explain vary among many highly accurate models. Moreover, Leo Breiman also described this situation as the Rashomon Effect and explained details using exemplary models. Until recently, the Rashomon sets have been rarely a subject of scientific research. In 2019 (Semenova, Rudin, and Parr 2019) approached the issue creating mathematical and statistical definitions and notations regarding such sets of models. They described Rashomon sets as subspaces of the hypothesis space, that is subsets of models having comparable performance as the best model with respect to a loss function. In order to define well the problem, they introduced Rashomon ratio (fraction of models in rashomon set and all models from hypothesis space) and shattering coefficient - the maximum number of ways any n data points can be classified using functions from the hypothesis space. Another outstanding remark concerning Rashomon set was made when in 2019 (Fisher, Rudin, and Dominici 2019) emphasized the analysis of features’ importance within Rashomon sets. The authors suggested Model Class Reliance - a new variable importance (VI) tool to study the the range of VI values across all highly accurate models - models included in rashomon sets. Later, (Rudin et al. 2021) provided basic rules for interpretable machine learning and identified 10 technical challenge areas in interpretable machine learning. They emphasized the troubleshooting and easiness of using glass-box models today as well as their advantage over black-box models due to their inscrutable nature. In this article, Challenge number 9 involves understanding, exploring, and measuring the Rashomon set. The authors address questions about how to characterize and visualize rashomon sets, and finally, how to pick the best model out of rashomon set. The Variable Importance Clouds, introduced in (Dong and Rudin 2020), are an excellent tool that one can use to address the above problems. Sush cloud maps every variable to its importance for every well-performing model. In our work, we choose and visualize the Rashomon sets built on a set of features as well as their subset. We address the problem of searching the most crucial predictive variables among those Rashomon sets and investigate the impact of choosing subsets of input features on the whole process of determining Rashomon sets and their characteristics. References "],["rashomon-on-mimic-draft.html", "6.4 Rashomon on MIMIC - draft", " 6.4 Rashomon on MIMIC - draft Authors: Degórski Karol, Fic Piotr, Kamiński Adrian (Warsaw University of Techcnology) 6.4.1 Review of the literature The term Rashomon effect was created to describe a situation when there are many different models with quite similar predictions. Very often there are many different descriptions giving about the same minimum error rate, so that we cannot point one model as the best Breiman and others (2001). As an example of this effect in reality they gave Linear Regression model and finding 5 from 30 best describing variables of a given problem. In this case there are approximately 140,000 such subsets. The authors explained that usually we choose the model which has best results on a test set, although there may be also different subsets of 5 variables that give very similar results. They also noticed that this effect occurs in different models, such as decision trees or neural networks. Furthermore, Semenova, Rudin, and Parr (2019) contributed to expand the study about Rashomon effect. They defined Rashomon set as a subset of models that have similar performance to the best model in terms of loss function. Moreover they introduced Rashomon ratio, that represents the fraction of models that fit our data equally well. Also they explained that Rashomon curve is a function of empirical risk versus the Rashomon ratio. They saw that there is a good generalization of the Rashomon curve’s elbow model when choosing between performance and simplicity of the model. They found out that interpretability of model is connected with Rashomon sets. Accordingly, when the Rashomon set is large there may exist simpler and higher performing model. Analysis of the Rashomon effect is still a new and open for developement field of the interpretable machine learning. Because of that, there are remaining challanges and problems, which are missing a state-of-the-art approach. Some of them are a proper measure of the Rashomon set, the best techniques of its visualization and optimal choice of the model from the Rashomon set Rudin et al. (2021). One of useful tools for the mentioned tasks is a framework called Variable Importance Clouds Dong and Rudin (2020), which can be used for studying the variable importance among Rashomon set. References "],["roshomon-sets-on-death-prediction-xgb-models-using-mimic-iii-database.html", "6.5 Roshomon sets on death prediction XGB models using MIMIC-III database", " 6.5 Roshomon sets on death prediction XGB models using MIMIC-III database Authors: Ada Gąssowska, Elżbieta Jowik (Warsaw University of Techcnology) 6.5.1 An initial literature review The Rashomon Effect is not a common concept, therefore all references to it in the literature are limited to some extent. In general it is said to occur when the same phenomenon can be explained in many different ways. The Rashomon set in turn is introduced as the set of almost-equally-accurate models for a given problem. With regard to Machine Learning the above term was first used by Leo Breiman in his 2001 paper (Breiman and others 2001) to describe a class of problems where many differing, accurate models exist to describe the same data i.e. to describe the case where we have many models that are different but are approximately-equally accurate. The name concept came from the title of the movie by Kurosawa from 1950 in which different characters have different perspectives on the same crimes. Breiman emphasized that the observation of many different accurate models on specific datasets is a common phenomenon. Since 2001 the topic was rarely discussed. While doing research on different machine learning models, data was quite often not taken into consideration at all. As stated in the recent article “A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning” (Semenova, Rudin, and Parr 2019) Rashomon effect is directly linked to the topic of Interpretable Machine Learning. The authors mention that often when the Rashomon set is large it implies that there may be an explainable model that performs well on the dataset. In the paper they focus on analysing Rashomon effect on different datasets and trying to generalize the statements about what the size of Rashomons set tells us about the machine learning problem. The other important topic connected to Rashomon effect is analysing the variable importance of the model. The question why models with similar performance on the dataset take different variables into considaration for prediction needs to be adressed. This area of research is described in an article called “All Models are Wrong, but Many are Useful: Learning a Variables’ Importance by Studying an Entire Class of Prediction Models Simultaneously.” (Fisher, Rudin, and Dominici 2019) It is emphasised there that there are fields, for example criminal recidivism prediction, where Explainable Machine Learning (including Rashomon effect) is specially important, as the non-explainable models may rely on unacceptable data (sex, race etc). In another article (Dong and Rudin 2020) the authors emphasize the fact that only by comparing many models of similar performance the importance of a variable compared to other variables can be throughly understood. They present the concept of variable importance cloud and draw connection of it to other areas. They research showed that the variable importance may dramatically differ in approximately equally good models. "]]
