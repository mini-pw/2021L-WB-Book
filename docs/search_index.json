[["xai1-explainable-wine.html", "1.4 Red wine quality mystery", " 1.4 Red wine quality mystery Authors: Jakub Kosterna, Bartosz Siski, Jan Smole (Warsaw University of Technology) 1.4.1 Abstract Wine is, on the one hand, an important element of the culture of a large part of the world, but on the other, in its own way, a mysterious drink whose quality assessment remains a mystery to the absolute majority of people. Although there are many varieties of this drink, it is not really known what causes the advantage of some wines over others. In this paper, we have looked at this issue by a fresh look at the Red Wine Quality from Kaggle comunity dataset1. Much to our initial surprise, despite the fact that a dozen or so chemical factors were taken into account, there is one that strongly goes through the series and seems to be the main predictor of the drinks assessment by experts - it is alcohol. The study used four black box models interpreted through modern methods of explainable artificial intelligence. 1.4.2 Introduction and Motivation Interpretation of the glass boxs models is by definition simple. The case is completely different in the case of the black box models. There are many projects on the Internet that look at Red Wine Quality dataset1 from both perspectives, but it is different when it comes to using and interpreting these more advanced methods. The idea of this chapter is to compile XAI solutions in terms of the analysis of the above-mentioned valuable dataset, together with the confrontation with the previous research on the quality of red wine. 1.4.3 Methodology 1.4.3.1 Dataset The original collection contains 1 600 observations, each representing one portuguese Vinho Verde of the red variety. It is a proper to analyze and respected set, as evidenced by its verification, multiple use, as well as a very high rating of usability of 8.8 on the website. It consists of eleven predictors: fixed acidity - most acids involved with wine or fixed or nonvolatile (do not evaporate readily) volatile acidity - the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste citric acid - found in small quantities, citric acid can add freshness and flavor to wines residual sugar - the amount of sugar remaining after fermentation stops, its rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet chlorides - the amount of salt in the wine free sulfur dioxide - the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion total sulfur dioxide - amount of free and bound forms of S02 density - the density of water is close to that of water depending on the percent alcohol and sugar content pH - describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic) sulphates - a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant alcohol - the percent alcohol content of the wine The decision variable was originally quality - the average rating of experts who made their classification on a scale from 0 to 10. Due to the capabilities of the analyzed XAI tools, our team decided to make it shallow to the binary dimension - transforming the wines of rating &lt;= 5 to 0, while the others - to 1. Thie division resulted in a simple intuition of bad / good wine, which resulted in 855 good wines and 744 bad wines. 1.4.3.2 Machine learning algorithms used In order to look at the nature of this data, four well-known algorithms have been trained from data divided into: 1199 observations for the training set and 400 for the test set. XGBoost (gbm) - powerful modern method based on AdaBoost2 and gradient boosting, imported from xgboost package3, with tuned hyperparameters using the randomized search method from sklearn package4, with the best values obtained: min_child_weight - 1, max_depth - 12, learning_rate - 0.05, gamma - 0.2 and colsample_bytree - 0.7 Support Vector Machine (svm) - algorithm, in which we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well; imported from sklearn package5, with tuned hyperparameters using the grid search method from sklearn package6, with the best values obteined: C - 10000, gamma - 0.0001 and kernel - rbf Random Forest (rfm) - method building multiple decision trees7 and merges them together to get a more accurate and stable prediction; imported from sklearn package8, with tuned hyperparameters using the randomized search method from sklearn package4, with the best values obtained: n_estimators - 2000, min_samples_split - 2, min_samples_leaf - 2, max_features - auto, max_depth - 100 and bootstrap - True Gradient Boosting (xgm) - a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error; imported from sklearn package9, with tuned hyperparameters using the grid search method from sklearn package6, with the best values obteined: learning_rate - 0.1, max_depth - 7 and n_estimators - 50 All solutions have been implemented in Python with random states set to 42. The choice of these solutions was made on the basis of their popularity, diversity and practicality, taking into account the essence of the problem under consideration. After checking the operation of the models on the test set, the following quality measures were obtained: 1.4.4 Results TODO: po podrozdziale na kad metod; po kilka wykresów i par-kilkanacie zda 1.4.5 Summary 1.4.6 Conclusions 1.4.7 References 1Red Wine Quality dataset: simple and clean practice dataset for regression or classification modelling https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009 2AdaBoost: AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire, who won the 2003 Gödel Prize for their work https://en.wikipedia.org/wiki/AdaBoost 3XGBoost Python package https://xgboost.readthedocs.io/en/latest/python/index.html 4RandomizedSearchCV from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html 5Support Vector Machine from sklearn https://scikit-learn.org/stable/modules/svm.html 6GridSearchCV from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html 7Decision tree: a flowchart-like structure in which each internal node represents a test on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes) https://en.wikipedia.org/wiki/Decision_tree 8Random Forest classifier from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html 9Gradient Boosting classifier from sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html "]]
