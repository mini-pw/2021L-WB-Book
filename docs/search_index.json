[["index.html", "Case Studies Preface", " Case Studies Faculty of Mathematics and Information Science, Warsaw University of Technology 2021-06-06 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw, and ML Case Studies during the last year’s course. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ "],["explainable-artificial-intelligence-1.html", "Chapter 1 Explainable Artificial Intelligence 1", " Chapter 1 Explainable Artificial Intelligence 1 Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. References Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ "],["xai1-explainable-cards.html", "1.1 Explaining Credit Card Customers churns", " 1.1 Explaining Credit Card Customers churns Authors: Przemysław Chojecki, Bartosz Sawicki, Katarzyna Solawa (Warsaw University of Technology) 1.1.1 Introduction and Motivation eXplainable Artificial Intelligence has become popular in recent years. Both the academia and buisness are interested in development in this field. Therefore, a lot of software packages enabling in-depth model analysis have been deployed. One of them is Dalex (Baniecki et al. 2020a). We will use this Python package to explain Credit Card Customers attrition. We will base our work on Credit Card customers (Goyal 2020) dataset. Additionally to explaining factors that make customers churn, we want to compare black-box models and white-box model using XAI methods. We intend to examine if there is a trade-off between accuracy and explainability, how it is stated in (Loyola-González 2019). We want to check if feature importance in L1 regularized Logistic Regression varies with a change in regularization strength. Similar study comparing feature importance measures was made (Saarela and Jauhiainen 2021). We extend that comparison and include XGBoost model. 1.1.2 Methodology 1.1.2.1 Dataset We will deliver our analysis on the data about resignation from using the credit card. The original collection contains information about 10 127 of the bank’s customers. The goal of this data was to predict whether the customer decides to discontinue using a credit card. This data set consists of 19 predictors: Customer_Age - Customer’s Age in Years Gender - Gender of a Customer Dependent_count - Number of people dependant to a customer Education_Level - Educational qualification of the account holder Marital_Status - Customer’s Marital Status Income_Category - Customer’s Annual Income Category Card_Category - Type of Card (Blue, Silver, Gold, Platinum) Months_on_book - Number of months the account exists Total_Relationship_Count - Number of bank products held by the customer Months_Inactive_12_mon - Number of months with no transfers in the last 12 months Contacts_Count_12_mon - Number of times the account holder contacted the bank in the last 12 months (phone, mail, visit in facility) Credit_Limit - The maximum amount of credit that a Card owner can take Total_Revolving_Bal - Total amount of credit that a Card owner did not pay at the end of the billing cycle Avg_Open_To_Buy - Average of 12 months of the maximum possible amount of cash available to the account holder to spend Total_Amt_Chng_Q4_Q1 - Change in Transaction Amount between last and first quarter of the year Total_Trans_Amt - Sum of all Transaction Amounts in last 12 months Total_Trans_Ct - Number of all Transaction Amounts in last 12 months Total_Ct_Chng_Q4_Q1 - Change in number of Transactions between last and first quarter of the year Avg_Utilization_Ratio - Average use of possible credit in the last 12 months The target variable in the set is Attrition_Flag, which determines whether the customer did close its Credit Cards service. 16% of customers in Dataset decided to stop using their Cards. The other 84% will continue to be the bank’s customers. Figure 1.1: Numbers of customers who decided to closed their Credit Cards service 1.1.2.1.1 Data preparation We have observed some of the columns had a giant correlation with others. Namely, Months_on_book, Total_Trans_Ct and Credit_Limit each had another column which it was correlated with (Pearson correlation over \\(0.75\\)). That’s why we decided to drop those columns from models. Some of the data had missing values. For every such case, we imputed the missing values with a median and made a new column informing whether there was a missing value or not. For the categorial columns with order (like Card_Category: Blue &lt; Silver &lt; Gold &lt; Platinum), we encoded it with growing numbers. For other categorical columns, we applied one-hot encoding. 1.1.2.2 Machine learning To find the differences between tree-based models and a white-box these algorithms were selected: XGBoost Random forest Logistic Regression with L1 penalty The first two are commonly used, black-box, tree-based models. The last is a well known generalized linear model for classification. Tree-based models tend to be overfitting. What is more, they tend to choose 4 or 5 columns to be deeply dependent on and almost completely ignore the rest. That is why the Logistic Regression analyzed in this article was modified with the L1 penalty. This modification makes a model “select” some of the predictors to predict from and the rest is ignored. It is possible to adjust the strength of a penalty with a C parameter. The smaller the C, the fewer columns are selected to be proper predictors for a model. Number of Logistic Regression models were trained and the model with the highest accuracy score was selected to be described in all parts, except Permutational Feature Importance. In that part we use numerous Logistic Regression models with different regularization strenght. 1.1.2.3 Assumptions of Logistic Regression A Logistic Regression model is derived from assumptions on data. Those assumptions can be summarised by a sentence: “Probability of being in a certain class is a logit function of a linear combination of predictors.” The exact assumption states: A data has \\(n\\) predictors. Let \\(x\\in \\mathbb{R}^n\\) be a value of those predictors and \\(p(x)\\) be a probalility that the target value is \\(1\\) provided those values of predictors. Then there exist \\(\\beta_0\\in\\mathbb{R}\\) and \\(\\beta\\in\\mathbb{R}^n\\) that \\[\\log(\\frac{p(x)}{1-p(x)}) = \\beta_0 + \\sum_{i=1}^{n}(\\beta_i \\cdot x_i)\\] The Logistic Regression model finds the best fitting values of \\(\\beta_0\\) and \\(\\beta\\). In practice, this assumption states, that the best fitting line to the data shown in Figure 1.2 is well fitted and that the points arrange around it. Figure 1.2: Figures showing the assumptions of Logistic Regression on some of the columns of the data Appropriate graphs of all of the variables can be examined on this article’s GitHub repository. Those graphs look similar to the five shown in 1.2, which are: Total_Ct_Chng_Q4_Q1 - satisfies the assumption; Customer_Age - looks like a data-blob; Total_Trans_Amt - provides a more complicated dependency than a straight line; Marital_Married - a binary column, therefore it is hard to conclude if it is well fitted or not; Contacts_Count_12_mon - multilabel discrete column, but rightly fitted. We concluded some of the columns may be slightly inappropriate to model linearly, but overall the Assumptions of Logistic Regression are mostly satisfied. 1.1.2.4 Methods of Explainable Artificial Intelligence In the article, we used some Explainable Artificial Intelligence (XAI) methods to explain models. Those methods are: Local methods: Break Down Shapley Values Ceteris Paribus Global methods: Permutational Feature Importance Partial Dependence Profiles (PDP) Accumulated Local Effects (ALE) 1.1.3 Local explanations In this section we describe our discoveries made by using explanatory methods locally. For analysis were used three types of observations: misclassified, correctly classified and uncertainly classified. 1.1.3.1 Logistic Regression Model 1.1.3.1.1 Misclassified The customer resigned, but the Logistic Regression (LR) model predicted his resignation only with a probability of 31%. Figure 1.3: Break Down decomposition of obervation misclassified by LR model According to Break Down, the variables Total_Revolving_Bal = 0 and Total_Ct_Chng_Q4_Q1 = 0.5 had the greatest contribution to the correct prediction. This means that the client did not leave an unpaid overdraft and in the fourth quarter made two time less transactions as in the first. Figure 1.4: Ceteris Paribus Profiles of obervation misclassified by LR model According to Ceteris Paribus, the biggest changes in prediction were caused by the variables Total_Revolving_Bal, Total_Ct_Chng_Q4_Q1, Total_Relationship_Count, Contacts_Count_12_mon, Card_Category and Months_Inactive_12_mon. 1.1.3.1.2 Correctly classified The Logistic Regression model predicted that the client would churn with a probability of almost 0%. Figure 1.5: Shapley Values decompositon of obervation correctly classified by LR model According to Shapley Values, the variable Total_Ct_Chng_Q4_Q1 = 2.5 has the highest contribution. This means that the client carries out 2.5 times more transactions. Figure 1.6: Ceteris Paribus Profiles of obervation correctly classified by LR model The Ceteris Paribus profiles shows that this is the only variable that can change the prediction for this observation. To increase the churn prediction, Total_Ct_Chng_Q4_Q1 would need to be less than 1. 1.1.3.1.3 Uncertain classified The selected customer did not resign from the service, but the model prediction was uncertain with a result of 54%. Figure 1.7: Break Down decomposition of obervation uncertain classified by LR model According to Break Down variables of the largest contribution are Total_Revolving_Bal and Total_Relationship_Count. Customer has no unpaid loans at the end of the months, and have few relationships with a bank. There is no important sings of churn but at the same time there is no important sings of being interested in bank services, so it is hard for LR model to make correct prediction. Figure 1.8: Ceteris Paribus Profiles of obervation uncertain classified by LR model Using Ceteris Paribus profiles, we can see that a slight change of one of the variables: Total_Revolving_Bal, Total_Relationship_Count, Contacts_Count_12_mon, Months_Inactive_12_mon, Total_Trans_Amt , Total_Ct_Chng_Q4_Q1 is enough for the prediction to be more decisive. 1.1.3.2 XGBoost and Random Forest model 1.1.3.2.1 Misclassified The customer terminated the services, but both XGBoost (XGB) and Random Forest (RF) got it wrong. They predicted chances of churn of 0.5% and 4%. Figure 1.9: Shapley values decompositon of obervation misclassified classified by XGB and RF model The Total_Trans_Amt column affects the result in the right direction, but its contribution is obscured by Total_Revolving_Bal, Months_Inactive_12_mon,Total_Relationship_Count, and Contacts_Count_12_mon. Figure 1.10: Ceteris Paribus Profiles of obervation misclassified classified by XGB and RF model According to Ceteris Paribus, in order for the result of the models to agree with the label, one of the following events would have to occur: Contact_Count_12_mon = 6 Total_Trans_Amt &lt; 850 1.1.3.2.2 Correctly classified The client continues to use the service, and both XGBoost and Random Forest were fully convinced that this would be the case (about 0% churn prediction). Figure 1.11: Break Down decompositon of obervation correctly classified by XGB and RF model Looking at the Break Down , we suspect that due to Total_Realtionship_Count = 1 the bank is afraid that it is not holding the client tightly enough and that the client is considering leaving. Despite this, the models returned a good prediction caused by a large amount of money circulating in the customer’s account and large loans not repaid on time . 1.1.3.2.3 Uncertain classified The customer resigned from the service. Both models will slightly lean towards customer resignation (52-53%), but this is a very uncertain prediction. Figure 1.12: Break Down decompositon of obervation uncertain classified by XGB and RF model Similar to the previous observations, the variables Total_Revolving_Bal and Total_Trans_Amt have the largest contribution. Figure 1.13: Ceteris Paribus Profiles of obervation uncertain classified by XGB and RF model According to the Ceteris Paribus profiles, as for the LR observations, there are a few variables for which small changes can significantly improve the predictions. 1.1.4 Global explanations In this section we describe our discoveries made by using explenatory methods globally. 1.1.4.1 Permutational Feature Importance We calculated Permutational Feature Importance for XGBoost model, Random Forest model and a group of Logistic Regression models. The Regression models were created with L1 regularization and different C coefficient was applied among the group. This coefficient is an inverse of a penalty term in L1 regularization, which means the smaller it is, the more weights shrinkage we expect. We examined if such shrinkage is noticeable in Permutational Feature Importance (PFI) method. Then, we compared PFI obtained from different models. 1.1.4.1.1 Logistic Regression Models Figure 1.14: Permutational Feature Impotance of Total_Ct_Chng_Q4_Q1 for the group of Logistic Regression models. This variable represents the change in number of transactions between Quarter 4 and Quarter 1. Its importance decreases when L1 regularization incerases. The variable with the highest (among the group of Logistic Regressions) drop-out loss is shown in Figure 1.14. The drop-out increases with the increase of C coefficient. The feature is more important for models with low regularization parameter, therefore it was shrinked by the Lasso. Figure 1.15: Permutational Feature Impotance of Total_Revolving_Bal for the group of Logistic Regression models. It is more important for models with high regularization. Figure 1.15 presents a variable importance plot of Total_Revolving_Bal (Total amount of credit that a Card owner did not pay at the end of the billing cycle) feature, which has the second highest drop-out loss. It was not regularized, because drop-out loss decreases with the increase of C. It is the only column, which has this property. Figure 1.16: Permutational Feature Impotance of Gender (Gender of a Customer) for the group of Logistic Regression models. This variable was clearly regularized by L1 regresssion. Importance increases with a decrease of regularization strength. Figure 1.17: Permutational Feature Impotance of Avg_Utilization_Ratio, which stands for an average use of possible credit in the last 12 months, for the group of Logistic Regression models. The stronger regularization gets, the less importatn this variable is. On the other plots, such as Figure 1.16 and Figure 1.17, the shrinkage made by L1 regularization is clearly visable. Models with high regularization parameter, and accordingly low C parameter, have smaller drop-out losses, which indicates lower importance of features. Drop-out loss increases proportionally to C parameter in nearly all of the columns. This shows that the effects of Lasso regularization can be observed in Permutational Feature Importance plots of Logistic Regression models. Despite all of the plots in this section are barplots, bars do not start in 0. We believe it is not a problem, because trend is more important than absolute values. Nevertheless, we would like to draw Reader’s attention to that fact. 1.1.4.1.2 XGBoost and Random Forest models In Figure 1.18 we can observe that the most important column for both models is Total_Trans_Amt (sum of all transactions’ amount in last 12 months). This outcome can be logically explained: customers who do not use their credit card to execute many valuable transactions probably do not need that service, consequently resign. However, the drop-out loss for that column for the XGBoost is over 2 times higher than for the Random Forest, which means that the prior model bases its prediction on this column more than the latter model. Furthermore, more features are important for the XGBoost than for the Random Forest. We suppose this is a result of the models different training processes. New iterations (trees) in XGB are based on observations that were previously predicted incorrectly, thus new columns are taken into consideration to represent the differences between the observations. On the other hand, the Random Forest model selects the subset of the features randomly in each tree. Figure 1.18: Top 9 most important features in XGBoost (XGB) and Random Forest (RF) feature importance comparison. XGBoost has more important variables than Random Forest, but the importance of Total_Trans_Amt is over 2 times higher. 1.1.4.1.3 Models comparison We compared the Permutational Feature Importance of the group of Logistic Regression models, XGBoost model and Random Forest model. We can see in Figure 1.19 the drop-out loss for Total_Trans_Amt in XGBoost is similar to drop-out in Logistic Regression models. Figure 1.19: Permutational Feature Impotance of 4 chosen variables for all models. Tree-based models generally have lower feature importance than Regression models. Effects of L1 regularization are neglectable in comparison to differences between models. If we compare the importance of Total_Revolving_Bal in Figure 1.19, we see a huge difference between tree based models and Regression models. The drop-out loss for the first ones is around 20 times lower than for the latter. We can also examine some of the less important features such as Gender (see Figure 1.19 ) and Avg_Utilization_Ratio (see Figure 1.19). In comparison to Regression models importance of these variables in XGBoost and Random Forest is neglectable. Therefore, we conclude that although the effects of L1 regularization in Logistic Regression are observable, tree-based models such as XGBoost and Random Forest select the most important features more restrictively. 1.1.4.2 PDP profiles We created Partial Dependence Plots of all variables in the dataset for XGBoost, Random Forest and Logistic Regression with L1 models. Many of the plots turned out to be a horizontal line located on the level of the mean prediction of the models. An example of such a variable is shown in Figure 1.20, predictions of models does not change with the change of Gender. However, features that have high importance do have more complex plots. One can observe prediction varying with the change of Total_Trans_Amt, Total_Revolving_Bal or Total_Ct_Chng_Q4_Q1. Figure 1.20: Partial Dependence Plots of chosen features. Changing some variables does not affect the models’ predictions (example of such a variable is Gender). There are 4 features (Total_Trans_Amt, Total_Revolving_Bal, Total_Ct_Chng_Q4_Q1, Contacts_Count_12_mon), which after beeing changed may influence the prediction. What we find interesting in Figure 1.20 is an unobserved in PFI effect of the Contacts_Count_12_mon variable. The plot is steady for values 1-5 and raises rapidly when the feature takes the value of 6. We examined this case and figured out, that only approx. 0.58% of all observations have value 6 in Contacts_Count_12_mon column (see distribution of this variable in Figure 1.21). What is more, all of them describe attrited customers. We concluded there are two possible solutions: The dataset is not balanced for this feature. Indeed, the 6th contact with the bank representative is a breakthrough in the relationship with the customer. Figure 1.21: Having calculated PDP, it is useful to compare the results with variables distribution. We compared PDP and features distributions (see Figure 1.21). We can observe numerous attrited customers have Total_Trans_Amt between 2000 and 3000. It explains why there is an increase in prediction in this area of PDP plot for this variable. Customers who did not churn have usually do not have Total_Revolving_Bal between 100 and 400. That is why prediction incresase for low values of Total_Revolving_Bal. 1.1.4.3 ALE profiles Figure 1.22: Accumulated-local Profiles Plots of chosen features. ALE plots seem to be parallel to PDP plots. This may indicate additive models. Accumulated-local Profiles for XGBoost, Random Forest and Logistic Regression with L1 were calculated. The results for chosen variables are shown in Figure 1.22. ALE plots seem to be very similar to PDP profiles. It may suggest there are no interactions between variables in the models. To examine that we plotted both PDP and ALE profiles in Figure 1.23, Figure 1.24. We skip these plots for Logistic Regression because, by definition, there are no variables interactions in this class of models. ALE and PDP plots are parallel thus models detected no interactions between features and they can be additive. Figure 1.23: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for XGBoost. Parallel lines indicate that this model can be additive. Figure 1.24: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for Random Forest. Parallel lines indicate that this model can be additive. 1.1.5 Summary In the article we analyzed the dataset and models trained on it. We explored the differences between the black-box, tree-based models and an explainable Logistic Regression. Various XAI methods were used for the comparison of the ML models. Local explanations showed that the misclassified observations are outliers. Therefore, the misclassification of those is not strange. Moreover, some of the observations are hard to classify by models, but a slight change of some of the predictors would make a prediction more confident. Global explanations of models gave us insight into how the models work. This lead to conclusions about the differences between those models. Firstly, tree-based models make their’s predictions on 5 or 6 values while a Logistic Regression (despite being modified with L1 penalty) uses more predictors. Secondly, tree-based models were overfitted on some of the columns. Lastly, with Logistic Regression it is impossible to model more complicated relationship in data because this method assumes linearity. References Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Goyal, S. (2020, November). Credit card customers. Kaggle. https://www.kaggle.com/sakshigoyal7/credit-card-customers Loyola-González, O. (2019). Black-box vs. White-box: Understanding their advantages and weaknesses from a practical point of view. IEEE Access, 7, 154096–154113. https://doi.org/10.1109/ACCESS.2019.2949286 Saarela, M., &amp; Jauhiainen, S. (2021). Comparison of feature importance measures as explanations for classification models. SN Applied Sciences, 3(2). https://doi.org/10.1007/s42452-021-04148-9 "],["xai-in-real-estate-pricing-a-case-study.html", "1.2 XAI in real estate pricing: A case study", " 1.2 XAI in real estate pricing: A case study Authors: Sebastian Deręgowski, Maciej Gryszkiewicz, Paweł Morgen (Warsaw University of Technology) 1.2.1 Abstract Real estate market is a very complex structure. In order to properly predict a price of a given property, many various factors need to be taken into account. This case study focuses on usage of explainable artificial intelligence (XAI) methods in making a prediction of California households’ prices. For this purpose, XGBoost regression model of supervised machine learning had been used and developed. In addition to high efficiency of predictions, many detailed factors had been studied and explained, such as high correlation between price and median income of household or impact of geographic location of given estate. The achieved results certainly contribute to a better understanding of the market. 1.2.2 Introduction Real estate plays a key role in past and present economy. In 2011 it was valued in total $25 trillion in the US only, $16 trillion of that figure comes from residential households. To compare, at the and of that year the capitalisation of US stock market was around $18 trillion. In its prices are interested both ordinary people, who treat real estate as basic good, and investors, who view the real estate as asset. The task of determining a relationship between dependent factors and final price is non-trivial and requires a vast, domain-specific knowledge. However, machine learning (ML) methods allow to surpass this requirement by determining the interaction in data themselves and leaving only the task of data selection and learning supervision to the human user. This comes with a cost, though: the most efficient ML models are of the so-called black box family and they are complex beyond human understastanding. If they are to be fair, trusted and fully usable, their decision process has to be brought closer to the human user. This article describes research performing a case study based on the actual US Census data from California from 1990. An ML model tasked with predicting real estate value is developed and then its decisions are made as clear as possible, using various XAI methods. 1.2.3 Related Work 1.2.3.1 Characteristic of real estate market The real estate market is not a typical one, as is pointed out in the introduction of (Ghysels et al. 2013). It is extremely heterogenous - goods offered vary between themselves in location, state and other physical attributes. It is also nonliquid, since transactions are costly and irreversible. This makes the task of forecasting a price of a given house particularly difficult. Instead, certain indicators of the price are predicted, the simplest one of them being the median house value. 1.2.3.2 Determinants of real estate value The factors influencing the real estate prices can be linked with four phenonena (Belke and Keil 2017): Future expected revenue, Accessibility, Hedonic factors, State of global and local economy. In an asset pricing approach expected revenue plays a key role. Here factors such as forecasted future rent, upkeep cost and taxes are taken into account. A second approach focuses on accessibility. It takes into account affordability and sustainability of house prices. It brings, among others, the montlhy income of potential owners into equation. Hedonic factors are object-specific and neighbourhood-specific characteristics, which contribute to the real estate value. Their examples are local population, housing density and infrastracture accessibility (such as public transportation, healthcare or education facilities). Finally, the state of global economy contributes to the price, and the state itself is measured by indicators like GDP, inflation rate, unemployment rate and construction activity. 1.2.3.3 Machine Learning uses Machine learning has been used in numerous scientific papers regarding house prices prediction allowing us to take inspiration from some of them. One of these papers titled “Model Research on Forecast of Second-Hand House Price in Chengdu Based on XGBoost Algorithm” (Peng et al. 2019) presents problem very alike to ours. As the title implies authors are trying to predict prices of houses in China using machine learning algorithms and data that are similar to what we are dealing with in California-Housing data frame. For us, the main takeaway from said article were conclusions about XGBoost algorithm. First of all, authors point out that it is robust and to great extent immune to overfitting phenomena. They also perform comparison between three different models from which XGBoost is definitely the most effective. This article in approachable way shown us how powerful XGBoost is and pointed out the importance of hyperparameters tuning. Often machine learning algorithms are paired with more complex neural networks. this allows for even better, more accurate predictions. One of the example of said practice can be “House Price Prediction Using Machine Learning and Neural Networks” paper (Varma et al. 2018). In this article authors use output of three ML algorithms, namely Linear Regression, Forest Regression and Boosted Regression, as an input for neural network that makes the final prediction. Although the article lacks actual results and comparison to other models, it presented us with an interesting idea and convinced us that there is room for improvement after we have built enough knowledge of neural networks. Even more comprehensive and complex model is presented in “Deep Learning with XGBoost for Real Estate Appraisal” paper (Y. Zhao et al. 2019). Apart from basic attributes such as size or number of bedrooms authors have incorporated ability for the model to process images of house to better estimate its price. Although their approach was much more sophisticated than ours it further reassured us that machine learning, and XGBoost model in particular, will be a superb tool for our task of predicting California’s houses’ prices. All in all machine learning models with application in regression tasks around houses’ prices prediction appear in countless papers so we had vast possibilities in terms of drawing inspiration from others’ work. 1.2.4 Methodology 1.2.4.1 Data description For this project we used the housing dataset which we acquired from Kaggle repository. This is a modified version of dataset provided by StatLib. It has been extracted from 1990 US census. Each row represents a single house in California area and contains information about its location and details about bloc it is situated in. Our target variable for this regression task is median_house_value. We present the structure of said dataset in detail in the table below. Figure 1.25: Housing data frame details Housing consists of 20640 records and missing values appears only in total_bedrooms column. There are total of 207 rows with missing values in total_bedrooms field, which makes up for approximately 1% of all records. Our data will evolve over the course of next sections, however whenever we implement any changes to dataset we will clarify them fully. 1.2.4.2 Exploratory Data Analysis In order to better understand data we were working with, we conducted exploratory data analysis, which mainly consisted of visualizations. The very first step of our EDA was figuring out if we have to deal with any missing values. As stated in Data description section, we have 207 rows with missing values in total_bedrooms field. We decided to impute these fields using median of total_bedrooms from all non-missing rows. Figure 1.26: housing.info() function output After that, we drew a target variable distribution plot. It proved invaluable, as it shown us that the target value had most likely been capped at $500.000. Every house with price greater than $500.000 had been given a $500.001 price tag. As it was not possible to recreate actual price of these estates we decided to remove corresponding rows. This way, we ended up removing 965 rows from the original data frame. Figure 1.27: Target variable distribution Next we took a closer look at our only categorical variable, ocean_proximity. What we wanted to see is what are the 5 categories and how many records each of them contains. Simple diagram (Fig. 3) made us realize that ISLAND category consists of only 5 rows. We made a decision to remove these rows. Figure 1.28: Ocean proximity categories Then, in order to discover correlations between different variables we constructed diagram in the form of matrix. It made us aware off high correlations between some of the factors, problems connected with this will be discussed more in details later on. Figure 1.29: Correlation matrix Finally we superimposed houses locations on California’s map. On the first map we plotted all of the houses while on the second map we plotted only houses with median_house_value over $490.000. This allowed us to find two clusters of the most expensive estates around San Francisco and Los Angeles. Figure 1.30: Map of all houses Figure 1.31: Map of houses worth more than $490.000 EDA turned out to be invaluable, it pointed out numerous interesting facts but also made us aware of some underlying problems with data frame such as missing values. It is also worth noting that our EDA consisted of more visualizations, however it felt more natural to skip those which did not provide any valuable insight. 1.2.4.3 Data validity Data accessible to us sets the boundaries of our research. It contains characteristics of households in California at one, specific point in time, so incorporating global economic indicators makes no sense. Many desirable predictors, such as number of bathrooms in a household or the exact state of local infrastracture are simply not available. In this paper we focus on accessibility and hedonic factors. The presence of exact location is vital to the credibility of this research. Factors connected with household location, which could have influence on final price, are numerous. Leaving some of them out could lead to selection bias present in the results of our work (Ghysels et al. 2013). Thankfully, any factors connected with location - such as the local unemployment or state of infrastracture - are tied with geographical coordinates and are present in our data, although implicitly. 1.2.4.4 Model In order to choose our prediction tool we have prepared 6 different regression models. We decided to compare results of Linear Regression, Random Forest, Support Vector Regression (SVR), Gradient Boosting, Multilayer Perceptron (MLP) and finally XGBoost Regressor. Each of these models had been individually tuned mainly using GridSearchCV. After models had been tuned we proceeded to comparing models in an effort to find the best one. We used root mean squared error (RMSE) as our main metric to sort models by their performance. Additionally, in order to maximize objectivity of this metric we used 5-fold cross-validation and calculated the average RMSE along with RMSE standard deviation. The achieved results are rather unambiguous as XGBoost is characterized by the lowest mean RMSE and the lowest standard deviation of RMSE. This was to be expected taking into consideration related works performed around similar problems. After taking into consideration RMSE results and explainability of the models we decided to use XGBoost as our primary prediction tool. The final values for its hyperparameters are: eta (learning rate) = 0.15, max_depth = 6, n_estimators = 150, gamma = 0, min_child_weight = 1 (any unmentioned parameter has been left with default value). 1.2.5 Results After successful training of the aformentioned model, we employed XAI methods of local and global explanation in order to get a firmer grasp over model’s decision process. 1.2.5.1 Local explanations At this stage of our work, we were particulary interested in observations that were somehow unusual or characteristic. In order to study them, we used prediction decomposition methods (LIME, Break Down, Shapley Values or Ceteris Paribus). 1.2.5.1.1 Most missed predictions The first record that caught our attention was the one with the most inaccurate prediction. Observation No. 18563, as it is in question, was worth $450,000. Our model priced it at just $121,105. A Shapley Values plot for this observation (1.32) threw some light at the matter: Figure 1.32: Shapley Values for observation No. 18563 population variable has the biggest impact on price in this case, it has increased the median price by nearly $70,000. This result was quite puzzling, since this variable was not considered as majorly significant impact on the predictions. Even more suprising was the fact that the variable median_income, which in most cases decided about the prices of an apartment almost by itself (with high efficiency), turned out to be grossly mistaken. In order to explore it in more detail, the area where this flat was located and visualised using Google Maps service. Figure 1.33: Street View for observation No. 18563 Although the property itself is located near a state road, there is nothing more that would let us convince that this flat is worth its high price, even when taking the 30 years gap between the data collection time and present time into account. In this case, a wrong prediction of the model was not surprising. The second most misguided prediction (No. 15804) also significantly lowers the actual price of the flat ($147,083 to $475,000). If we have a look at a plot below (1.34) we can note that it is similar to previous one. Figure 1.34: Shapley Values for observation No. 15804 Again, the most important variable is population, which correctly attempts to increase the price of the home. Again, the value of the flat is underestimated by tagging the property with the INLAND category. This time however the median_income variable has very little impact on the prediction. Checking the view of the area in Google Maps (1.33) once again clarifies the situation. Here we can see an estate of luxurious villas. If the area was similar 30 years ago, it could indeed be worth $475,000. However, it is quite unusual for such a rich estate to be constructed in that kind of location, so again the underestimation is not a surprise. Figure 1.35: Street View for observation No. 15804 Next in line, we looked at the observation for which our model overestimates the real value of the apartment the most. Observation No. 5168, as it is in question, turned out to be located in Los Angeles, 10 minutes far from the beach, in a rather rich-looking housing estate. Considering this, it is not surprising that our model valued this property at $302,868 while it was really worth only $55,000. Figure 1.36: Street View for observation No. 5168 Figure 1.37: Location of observation No. 5168 in Los Angeles If we take a closer look at the Shapley Values decomposition plot for this prediction (1.38), we can notice that this time the two most important variables turned out to be latitude and londitude. Figure 1.38: Street View for observation No. 5168 As we can see, our model has learned that apartments located close to or in Los Angeles itself are usually worth much more than those outside of the city. Only a low value of the median_income variable could suggest that the apartment is not as expensive as it might seem (in fact, it is not expensive at all), but still there were more hints that it is rather a luxurious property. 1.2.5.1.2 Same area, different price At the end of our work on local explanations, we looked at two examples of houses, which, despite their proximity, turn out to be significantly different in terms of their price. Our focus is in the Los Angeles area, and our observations are the ones numbered 4526 and 4537 (1.39). To the nearest hundredths of a degree they are in the same place, this is the Korean district of Los Angeles, near one of UCLA campuses (figure below). Still the price difference is as high as $400,000. Figure 1.39: Location of observation No. 4526 and 4537 in Los Angeles In order to explore these results, Break Down decompositions for these observations were created(1.40 and 1.41). The model did not fully detect the difference between the households. The cheaper was overestimated ($205,000 instead of $75,000), and the more expensive was underestimated ($320,000 instead of $475,000). Nevertheless, the impact of individual variables can be observed. The key differences are the households and rooms_per_household variables. Interestingly, the model behaved differently for the longitude variable - its contribution is negative for the cheaper one, and positive for the more expensive one. Figure 1.40: Break Down for observation No. 4537 Figure 1.41: Break Down for observation No. 4526 1.2.5.2 Global explanations At this stage, an the analysis of the impact of entire variables on a trained model was performed. Global explanation methods (permutation significance, utility and partial dependency profile (PDP)) were used to study them. 1.2.5.3 Importance of variables The permutational significance method was used to determine which variables actually contributed to the performance of the trained model (1.42). We have observed that the most significant variable is median_income. Figure 1.42: Variables importance Real estate is about location, location, location. One of our first interests was the influence of location on the value of an apartment. The PDP for the longitude and latitude variables is shown in the figure below (1.43). Figure 1.43: PDP for longitude and latitude variables Properties to the west (shorter longitude) and south (lower latitude) are preferred. The first result is quite intuitive - to the west it is closer to the ocean, there are large agglomerations located there. The second is less expected - perhaps related to a warmer climate or better economic indicators. Let’s take a closer look at the dependencies (see: 1.44). We see a large drop in the value around 122°20’W (San Francisco area), then similar values up to a slow decline from 118°20’W to 117°20’W (Los Angeles and San Diego). As for the width, the location of these cities is again very important - we see a decrease between 33°30’N and 34°30’N (San Diego, Los Angeles) and a huge decrease between 37°30’N and 38°N (San Fransisco). We can see that the places of declines in the charts can be translated into the location of large agglomerations, but this does not explain the trend. Local growth, and not global decline, would be expected. Figure 1.44: Map of California Next in line we were interested in the most important variable for the model - median_income. Its influence on the model is shown in the figure below (1.45). Figure 1.45: PDP for median_income variable It was no suprise that residents’ earnings have a considerable impact on the price of an apartment. It is worth noting to what extent does the median_income variable influence the final result of the prediction. For the lowest-income households, the model will predict a property value of around $133,000, while for the most affluent households, the prediction will be around $353,000. So it is more than 2.5 times increase in the expected price. Moreover, the relationship is not completely linear - from a certain point, the increase in earnings does not translate into an increase in the price of the apartment. In the next step, we checked how other, less significant variables influence the model response (1.46). Although the graphs look very varied, the ranges of values on the vertical axis are small, as is the overall importance of these variables. For instance, population variable has the largest range of values, from $177,000 to $226,000. This range is inferior to ones observed in previous variables. Nevertheless, some interesting dependencies on the basis of the plots can be learned. From the plot for housing_median_age, a general trend that older properties tend to be a bit more expensive can be observed. The total_bedrooms plot shows that houses in blocks with a very low combined total of bedrooms are noticeably cheaper. The population variable distinguishes houses in households with a very low total population. These houses are usually more expensive. In turn, the households plot shows that a significantly low number of houses in the block reduces the predicted house price. This is in apparent opposition to the conclusions of the population variable. However, it should be noted that those considerations apply only when the values of the remaining variables are the same. If the number of houses increases, the amount of space per capita decreases and the price goes down. On the other hand, when the number of households increases with a dedicated population, the amount of space per inhabitant increases and the price goes up. This apparent abberation is explored in next section further. rooms_per_household variable has a small impact on house prices, but a decline is noticeable for houses with a low average number of rooms. Figure 1.46: PDP for other continuous variables 1.2.5.4 Doubts about household and population variables During the EDA, we found a correlation between households and population variables. This could have a negative impact on the interpretability of the PDP plotss and the meaningfulness of the information they convey. To verify that, we performed the following procedure: The population variable was divided into 5 intervals so that a similar number of observations fell into each interval, A PDP curve was determined for each interval, Differences between the determined curves were observed. If there were no differences, interactions in the model between variables would be non-existent and their correlation would only account for results noted on PDPs. The results of the verification in the figure below (1.47). Figure 1.47: Importance of population variable wrt. household variable We observed a different appearance of profiles for large values of households than for small ones. Hence, we concluded that there are interactions between these variables in the model, so the conclusions presented in the previous section are valid. 1.2.6 Summary and conclusions Over the course of this case study, after basic EDA and training of an exemplary ML model, various XAI methods were used to make the process of real estate pricing clearer. Local explanation of odd outcomes gave detailed reasons for which the model gave incorrect results. When compared with intuition and with actual photos of the households, it was clear, that these observations were unusual and the model did as expected. Global explanations allowed us to get a sense of the relationship between individual variables and the median house value. We conclude, that the methods employed made the decisions made more transparent and believe, that these methods have high potential of use in other fields. References Belke, A., &amp; Keil, J. (2017). Fundamental determinants of real estate prices: A panel study of german regions, (731). Ruhr Economic Papers. https://doi.org/10.4419/86788851 Ghysels, E., Plazzi, A., Valkanov, R., &amp; Torous, W. (2013). Chapter 9 - forecasting real estate prices, 2, 509–580. https://doi.org/https://doi.org/10.1016/B978-0-444-53683-9.00009-8 Peng, Z., Huang, Q., &amp; Han, Y. (2019). Model research on forecast of second-hand house price in chengdu based on XGboost algorithm, 168–172. https://doi.org/10.1109/ICAIT.2019.8935894 Varma, A., Sarma, A., Doshi, S., &amp; Nair, R. (2018). House price prediction using machine learning and neural networks, 1936–1939. https://doi.org/10.1109/ICICCT.2018.8473231 Zhao, Y., Chetty, G., &amp; Tran, D. (2019). Deep learning with XGBoost for real estate appraisal, 1396–1401. https://doi.org/10.1109/SSCI44817.2019.9002790 "],["coronary-artery-disease-is-it-worth-trusting-ml-when-it-comes-to-our-health.html", "1.3 Coronary artery disease: Is it worth trusting ML when it comes to our health?", " 1.3 Coronary artery disease: Is it worth trusting ML when it comes to our health? Authors: Paulina Przybyłek, Renata Rólkiewicz, Patryk Słowakiewicz (Warsaw University of Technology) 1.3.1 Abstract Coronary artery disease is a dangerous heart sickness and the number of people suffering from it is significant around the world. In medicine research this disease, artificial intelligence are used more and more often. We decided to create a machine learning model which gonna try to predict the danger of coronary artery disease base on the results of few medical examinations. An important part of this paper is an attempt to explain predictions made by the model, using explainable artificial intelligence methods, and compare it with the medical approach. We will try to answer the question of whether it is worth trusting classification algorithms when making a diagnosis, or maybe in this case only doctors opinion has to be used? 1.3.2 Introduction and Motivation The heart is an organ on which the life and proper functioning of all other organs depend. For this reason, everyone should keep their heart healthy. However, it is not easy as we are not always aware of the impact of our actions on future health. How many times have we heard or witnessed a heart attack from someone close to us? Coronary artery disease (CAD) is very often behind the infarction. CAD, also known as ischemic heart disease - is a disease syndrome characterized by insufficient blood supply (and thus insufficient oxygen supply) to the heart, caused by a significant narrowing of the coronary arteries that nourish the heart muscle. CAD is the most common cardiovascular disease in developed countries, and heart attacks and sudden cardiac death are the most common causes of death. In Poland, it affects over one and a half million people and is the cause of about 20% of all deaths. Hence, it is important to detect this disease in a patient early and implement appropriate treatment before it is too late. More and more often when looking for a solution, we reach to artificial intelligence (AI) and machine learning (ML). Although in medicine AI has been used for decades, it still raises many doubts and is often criticized because of the lack of trustworthiness and questionable quality of the results. In medicine, human experts must be able to retrace the decision-making process of the machine. This is why it is crucial for models to be transparent, interpretable, and explainable. Unfortunately, successful models are often black-box ones. Explaining them is not so easy, but this is what explainable artificial intelligence (XAI) was made for. Building a model to predict coronary artery disease based on test results and medical history to be used to detect this disease in new patients could reduce the number of serious medical complications. However, trusting such a model completely in terms of health and even life is unconvincing. After all, you must put your life in the hands of the machine. The purpose of this article is to check to what extent and whether it is worth trusting ML algorithms when making a diagnosis of suffering from CAD. To answer this, we used XAI methods to understand models built on a set of patients with or without CAD. We checked the relationships between data science and the medical approach and saw which approach is better. 1.3.3 Related Work Usage of AI in medicine and the importance of XAI methods are widely researched. Countless approaches, methods, models, or measures are considered and are discussed ways how to build explainable AI systems for the medical domain (Holzinger et al. 2017). In medicine designing an algorithm that can automatically learn without any human impact is impossible. The integration of the knowledge of a domain expert can often be indispensable. That is why interactive machine learning (iML), human-in-the-loop approach (Holzinger 2016) and the extension of that - doctor-in-the-loop (Kieseberg et al. 2015) are considered. However, most research still focuses on classic ML solutions like supervised learning. One of the fields in medicine in which machine learning is frequently used in cardiology. AI is being increasingly applied in cardiovascular medicine for identifying new genotypes and phenotypes, enabling cost-effectiveness, and importantly, risk stratification (Krittanawong et al. 2017). Often those are deep learning models that are difficult to interpret, so XAI methods are needed (Khedkar et al. 2019) (Kaladharan et al. 2020). On the other hand, using black-box models and XAI methods in high-stakes decisions like those in medicine have also opponents that prefer design models that are inherently interpretable (Rudin 2019a). There are also approaches that believe that XAI is not enough and to reach the next level in explainable medicine we need causability that encompasses measurements for the quality of explanations (Holzinger et al. 2019) (Holzinger 2021). The variety and abundance of articles in the field of usage AI in medicine confirm our belief that this is a current and needed topic. 1.3.4 Methodology In our article, we analyze a dataset about CAD with XAI methods. We use global and local explanations and describe results using our ML and medical knowledge. This section of the article contains information about the dataset, data preparation for an experiment, ML algorithms, and using XAI methods. Also, at the end of the section, the workflow of our experiment is briefly presented. 1.3.4.1 Dataset In the experiment, we used a heart disease dataset from UC Irvine Machine Learning Repository (Dua and Graff 2017). The dataset was originated in 1988 and designed for the classification task (Detrano et al. 1989). It contains information about 303 of the patients from Cleveland hospital and 14 attributes describing theirs. Details of this data collections are described by Detrano et al. (1984). These attributes are both categorical and numeric, and the “goal” field refers to the presence of heart disease in the patient. Before we made our explanations, we discussed the attributes of this dataset with the cardiologist and prepared a description of them to better understand our data. The information about these attributes are given below: age - patient’s age in years sex - gender of the patient: male female cp - chest pain type: typical angina - pain indicating CAD atypical angina non-anginal pain asymptomatic - no pain trestbps - resting blood pressure (in mm Hg on admission to the hospital) chol - serum cholestoral in mg/dl fbs - fasting blood sugar &gt; 120 mg/dl: true - diabetes false restecg - resting electrocardiographic results: normal having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV) - changes indicating ischemia of the heart muscle (repolarization period) showing probable or definite left ventricular hypertrophy by Estes’ criteria - possible hypertrophy of the heart muscle thalach - maximum heart rate achieved during exercise testing exang - exercise-induced angina: yes no oldpeak - ST depression induced by exercise relative to rest slope - the slope of the peak exercise ST segment: upsloping flat downsloping ca - calcification, number of major vessels (0-3) colored by flourosopy thal - MPS, myocardial perfusion scintigraphy using thallium 201, two tests - rest and exercise: normal fixed defect - permanent defect, present in both images (irreversible change, e.g., post-infarction scar) reversable defect - post-exercise loss with the redistribution, only in the exercise image, reversible ischemia (risk of ischemic heart disease) target - the goal of this dataset (the predicted attribute), diagnosis of heart disease (angiographic disease status): &lt; 50% diameter narrowing - patient with CAD &gt; 50% diameter narrowing - patient without CAD, but he can have another heart disease The cardiologist paid special attention to some variables and their influence on the incidence of coronary artery disease. These variables include cp, thal, ca, oldpeak, age, and sex. In medicine, typical angina (cp), fixed defect (thal), high ca and oldpeak values, being male, and having a certain age make the patient more likely to suffer from coronary heart disease. Additionally, doctors use a pre-test probability of CAD (PTP) (Genders et al. 2011) to estimate the likelihood of coronary heart disease based on the patient’s age, sex, and chest pain type. If PTP is &lt;15%, coronary artery disease is unlikely, if &gt;85% certain. There is no point in performing diagnostics in these ranges. The group between 15-65% may have a non-invasive test for CAD diagnosis, and the 65-84% group should have a functional diagnosis performed. The PTP table used by cardiologists was presented in Figure 1.48. Figure 1.48: Clinical pre-test probabilities in patients with stable chest pain symptomps When analyzing the dataset, we noticed that some variables had distributions that quite clearly separate healthy and sick patients. The variables that caught our most attention were also mentioned by the cardiologist as factors influencing the presence or absence of coronary artery disease. Figure 1.49 shows the distributions of the variables ca, oldpeak, thal, cp, age, and sex. The legend specifies which color corresponds to patients with and without CAD. Figure 1.49: Clinical pre-test probabilities in patients with stable chest pain symptomps The dependencies described by the cardiologist were shown on the plots - namely, for higher ca or oldpeak values or fixed defect (thal) or typical angina pain in the chest or age of around 60 or having male sex, the probability of CAD increases. At the time of this analysis, it was puzzling for us whether the explanations of the model and the data would capture this. 1.3.4.2 Preparation of the dataset for an experiment Before modeling, we properly prepared the dataset. The original set contained five target categories, which had to be divided respectively - CAD (values 1,2,3,4) and no CAD (value 0). Categorical attributes that had only two values were coded as 0 and 1, while if they had it more, we used one-hot encoding to encode them. Some of the data had missing values. For every such case, we imputed the missing values with a mode. After consulting with a cardiologist, we decided to change the one attribute because we think it contained errors after being processed before it was uploaded to the UCI website. We swapped the values in the cp variable between typical angina and asymptotic. In addition, we combined the categories denoting heart problems in restecg due to the low number of occurrences of one of the categories. 1.3.4.3 Classification algorithms As part of the search for the optimal model, we tested many different algorithms using hyperparameter tuning. All models were based on classifiers from the sklearn (Fabian Pedregosa et al. 2011) package, with the accuracy being the summary metric that was used to select the optimal model. We chose accuracy because it measures how a good model is incorrectly predicting both positive and negative cases. Ultimately, we selected four models that gave the best results on the test dataset. Selected classification algorithms are both glass-boxes and black-boxes, which allowed us to compare the XAI approach to different types of models. They are listed below. Random Forest - very flexible, robust, and popular algorithm, with no assumptions about data. Multiple classification trees trained on bagged data. Selected hyperparameters in the model: max_depth=7, min_samples_split=20, n_estimators=1000. C-Support Vector Classification (SVC) - nonparametric clustering algorithm that does not make any assumption on the number or shape of the clusters in the data. Selected hyperparameters in the model: C=0.5, probability=True. k-Nearest Neighbors (KNN) - assumes that similar observations are close to each other in feature space, assumes a very local impact of features. Selected hyperparameters in the model: n_neighbors=17. Naive Bayes - nonlinear model with the assumption that all features are independent of each other. In the model, all hyperparameters were a default. In the case of SVC and KNN, the data from the dataset was standardized. For reproducible results, a seed equal to one was set (the random_state hyperparameter). On selected trained models, we used three other metrics besides accuracy to measures the quality of models: precision, recall, and area under the ROC curve. Precision quantifies the number of positive class predictions that belong to the positive class, while recall (also known as sensitivity) quantifies the number of positive class predictions made out of all positive examples in the dataset. The area under ROC curve (AUC) measures the area under a plot of sensitivity against specificity (measures the proportion of negatives that are correctly identified). The results of these measures on the test dataset are given in Table 1.1. Table 1.1: The results of measures on the test dataset for all classification algorithms Model name Accuracy Precision Recall AUC Random Forest 0.918 0.897 0.929 0.919 SVC 0.902 0.893 0.893 0.901 KNN 0.869 0.917 0.786 0.863 Naive Bayes 0.869 0.857 0.857 0.868 The results achieved by the models are good. The glass-box models achieved higher values everywhere. They will be good examples to compare the explanations on the black-box models, which also have high scores on the test dataset. 1.3.4.4 eXplainable Artificial Intelligence We used local and global explanations to check the model predictions. For a good understanding of the data, we looked at consider entire models first and then single predictions. All of the XAI methods we used are from the dalex (Baniecki et al. 2020a) package. Global explanations methods that we used: Permutation-based variable-importance (A. Fisher et al. 2019a). Partial Dependence Profiles (PDP) (Friedman 2000a). Aggregated Profiles (ALE) (D. W. Apley and Zhu 2020). And local explanations methods: Break Down (Staniak and Biecek 2018a). Shapley values (Scott M. Lundberg and Lee 2017). Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al. 2016a). Ceteris Paribus Profiles (CP) (Goldstein et al. 2014). For reproducible results, a seed equal to zero was set (the random_state hyperparameter). 1.3.4.5 Experiment To sum up, in the experiment we used 4 different classification algorithms and 7 different XAI methods to understand the predictions on the dataset. Random Forest was adopted as the main algorithm and other models were used to compare the XAI results. We decided so because we wanted to take the black-box model for the XAI so that the use of explanations would make sense. We rejected the SVC model because it gives a disproportionately high probability of disease at the extremes of some values. The procedure followed to understanding data and models with explanations methods was as follows: We have coded and processed the variables as described in the “Preparation of the dataset for an experiment” section. The dataset was divided into train and test subsets with a split ratio of 80/20. A seed equal to zero was set. We imputed values separately in train and test datasets. All imputed train and test datasets were used in four classification algorithms. The classifiers were built using a transformed train dataset. And measures of their quality were checked using a test dataset. We used learned models and a train dataset to apply the XAI methods. Then we described the explanations, trying to back them up with medical knowledge to have a benchmark against the results obtained. 1.3.5 Results Our motivation in using explanation methods was to understand which variables influence our model prediction the most and how they change along with the values of each variable. Our model is expected to predict CAD disease. It is crucial to comprehend the model’s decisions. What’s more, we would like our model to detect the same dependencies as doctors use to diagnose CAD. 1.3.5.1 Global explanations Global explanation methods gave us the overall view into the logic of our models, the importance of variables, and how they influence predictions. The first global method we used, was Permutation-based variable importance (presented in Figure 1.50) which works by permuting values of a particular variable and counting how much does the prediction change. We expected models to get high score on variables which are known as important for doctors. Figure 1.50: Permutation-based variable importance On the top of the list, we got thal what is not surprising because this medical examination is widely used by cardiologists. Conducting this test is prescribed in serious cases to ensure the doctor in his concerns about patient health. Also, ca is one of the most important and the case is similar to thal variable. The third is cp which is chest pain and it is classified during interview. With age and sex is used to estimate CAD possibility and choose a futher treatment plan (as mentioned in Figure 1.48). We have taught few models so now we going to show the differences in Variable Importance between them for each variable, to depict why the random forest is in our opinion the best. Figure 1.51: Comparation of permutation-based variable importance for 3 models It is important that this method not only showing importance but also how stable the model is what means the resistance of a model for bias in data. Even if for ca each model has a high score but for different variables Random Forest got is significantly smaller. In Figure 1.51 in contrast to KNN and SVC which have high and almost the same values for each, Random Forest lower importance score for variables we do not find so important. To understand better, how these variables change prediction, depends on the value of each one we used Partial Dependence Profiles which changing value of a particular variable and count predictions for each observation and then taking the mean of it so we can see how different values of variable change prediction outcome. In Figure 1.52, grey lines represent Ceteris Paribus Profiles of each observation but this is wider discussed in Local explanations. Figure 1.52: Ceteris Paribus Profiles for chosen variables Looking at the age we can see growth at the age of 55, this is correct with intuition and medical’s statements. Pay attention to a slight jump in the age of 45. It is a reflection of the age when increase probability of getting heart disease according to PTP. As we see for sex, being a male highly increases predictions. Good that our model finds this dependence. It is also one of the main factors taken into account by doctors in using PTP, mentioned before. Also looking at the gray lines we cannot spot a single one that goes down while changing to male. That is means that is true for every patient. Variable ca is one of the most important according to the previous XAI method. So thanks to that plot we see that having at least one calcification rapidly increases the prediction of the model and having more only ensures the model in the diagnosis. oldpeak as a deviation from normal, follow the rule the bigger the worse. For the right limit values, it becomes a straight line probably because further growth does not influence CAD more since is really big already. Also there is no much number of records having such big values. For the thal we chose to show an indicator of normal results of this medical examination. Thanks to that we can see if the patient has thal_n (normal) that is mean he does not have any other type which would be a sign of CAD. Pay attention to cp_ta (typical angina) because it is one of the main variables considered by doctors. Also for our model, it is meaningful. Figure 1.53: Ceteris Paribus Profiles devided into ca values. Using the same technique we will get deeper into relations age and oldpeak with ca because it is one of the most important variables. In Figure 1.53, we see Partial Dependence Profiles in the division for values of ca. The biggest differences are visible between having at least one calcification and not having it at all. In the age plot three top lines are almost the same shape. We can spot on all three of them that prediction is rising in the age of 45, the same age does not increase prediction for observations ca equal 0. In the second plot, we see how the value of prediction changes based on oldpeak values and ca values. If a patient is already in the high-risk group by having ca at least equal to one then even a slight aberrance of oldpeak (less than one) will rapidly increase the value of prediction. Meantime when ca is equal to 0, oldpeak has to reach much higher values to have a vivid influence on prediction (greater than 1.5). The last global method we used was ALE which works as CP Profiles but in addition, allows to spot dependencies between variables. We used it to check and compare all four models. This gonna give us a piece of information about differences and relationships between models. Figure 1.54: Ceteris Paribus Profiles devided into ca values. In Figure 1.54, looking at the age plot we see that Naive Bayes has a similar trend as Random Forest. Other models underestimate age. For ca we see why Random Forest was the best choice. Since all other models tried to suit straight lines into values 0 and 3 they underestimate values equal to 1 and 2. It is not what we would expect, since we know that difference between 0 and 1 is the most important. thalach as well as oldpeak have a more or less similar trend in each model. Interesting output is Naive Bayes’s prediction for high values. We can assume it is because of the small number of such big values. 1.3.5.2 Local explanations In global explanations, we have to remember that obtained values are often averaged. Sometimes, especially in medical cases, it is crucial to take a look at individual observations to catch interesting dependencies and draw conclusions. That is why the next step in examining our model was predict-level explanations. At first, we used Break Down, Shapley values, and LIME methods that show the decomposition of model prediction by particular variables to see which attributes had the greatest impact on model decisions. We examined the cases the model had no doubts about (both where predictions were correct and incorrect) and borderline cases. In Figure 1.55 and Figure 1.56 were presented Shapley values for respectively correct CAD classification and incorrect noCAD classification predictions. Figure 1.55: Shapley values for observation that model had no doubt about - correct CAD classification Figure 1.56: Shapley values for observation that model had no doubt about - incorrect noCAD classification In most cases where the model was sure about prediction regardless of whether the prediction was correct or not, on top were ca,thal,cp, and oldpeak. Those are the variables that also in global explanations were frequently highlighted, so we recognized them as very good CAD indicators. Like the doctor, our model classified patients based on their symptoms. Some of them got wrong, but probably the cardiologist would have made the same decision as well. A sick person, but with no symptoms is almost impossible to be diagnosed. Similarly, a healthy person with CAD symptoms will be considered sick and have further medical examinations. Figure 1.57: Shapley values and LIME for borderline case Figure 1.58: Shapley values and LIME for borderline case Borderline case was presented by using two methods - Shapley values in Figure 1.57 and LIME in Figure 1.58. For such observations, it turned out that usually half of the variables that were mentioned before as indicators (ca, cp, thal and oldpeak) had values indicative of CAD, and the other half not. That is why the prediction results were uncertain. By comparison to the previous cases, more often we could notice the greater influence of the other seemingly less important variables. Next, we used Ceteris Paribus Profiles to some what-if analyses. By changing the values of one explanatory variable we examined how would the model’s predictions change. Figure 1.59: Ceteris Paribus Profile for case with strong prediction and borderline observation For observations that contained strong indications of disease, a change in the value of one attribute had less impact on prediction than for healthy patients or borderline cases. With age, the risk of CAD increases. The more symptoms of the disease a given patient has, the smaller spike is usually observed on the chart (the blue line is almost flat, the green one clearly increases). The similar conclusions we can draw for ca - greater variance is observed for smaller prediction values. The presence of at least one calcified vessel significantly affects the prediction. We also analyzed CP profiles coupled with LIME to get a better understanding of changes in CP predictions. In Figure 1.58 was presented mentioned plots for two observations. Figure 1.60: Ceteris Paribus Profile and LIME for two borderline observations In the shown example we have two patients - they are both the same age, the predictions are also very similar (about 50%). For green, the main factors influencing the prediction result are typical angina and exang, for blue - the presence of a reversed defect and high oldpeak. The significant difference in the CP chart is because the variable that has the greatest influence on the prediction of the green patient - chest pain - becomes dangerous only with age. Hence, if the patient was 10 years younger, but her other symptoms were the same, the risk of the disease would be reduced by 10 percentage points. By analyzing these charts individually, we would not be able to catch this dependence. 1.3.6 Summary and conclusions To sum up we created the model which can, with great accuracy, predict Coronary artery disease from given results of medical examinations listed in the “Dataset”. To achieve it, we deeply analyzed data searching for relationships between variables and target. Knowing that we were able to present the first hypothesis and validate them with the cardiologist. The review we got, made us reconsider the meaning of variables and our working hypothesis. So mistakes spotted in data were fixed thanks to that. After that, we started to create many models. Using tuning algorithms we chose the best hyperparameters for a few of them and then compared them using more metrics. Having in mind our other goal (get know XAI methods in use) we had to choose the best black-box model. As we mentioned, our other goal was to learn about tools for explanation machine learning and then by using them explain if the model is suitable for our problem. Not only we did it but also by comparing it with medical knowledge we could reveal the importance of some variables in the occurrence of CAD disease. Using global methods allowed us to check if the model fining important variables we have known as important then by studying PD profiles we saw the patterns the model recognize. Being aware of the limitation of the model we decided to check particular patients and how the model acts trying to predict their sickness. In that matter, we could check what makes our model imperfect. We found out that some of the cases would be problematic not only for our model as also for human specialists. This problem is sadly unsolvable. Medical interpretation is similar to the data science approach and it indicates the importance of the same variables like cp, thal, age, and ca. Let’s keep in mind that the model doesn’t replace specialists and some unusual cases may appear, which the model will not solve. We assume that the model might be an advisor for doctors rather than a final diagnosis. References Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.-J., Sandhu, S., et al. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. The American Journal of Cardiology, 64(5), 304–310. https://doi.org/10.1016/0002-9149(89)90524-9 Detrano, R., Yiannikas, J., Salcedo, E. E., Rincon, G., Go, R. T., Williams, G., &amp; Leatherman, J. (1984). Bayesian probability analysis: A prospective demonstration of its clinical utility in diagnosing coronary disease. Circulation, 69(3), 541—547. https://doi.org/10.1161/01.CIR.69.3.541 Dua, D., &amp; Graff, C. (2017). UCI machine learning repository. http://archive.ics.uci.edu/ml Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Friedman, J. H. (2000a). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Genders, T. S. S., Steyerberg, E. W., Alkadhi, H., Leschka, S., Desbiolles, L., Nieman, K., et al. (2011). A clinical prediction rule for the diagnosis of coronary artery disease: Validation, updating, and extension. European Heart Journal, 32(11), 1316–1330. https://doi.org/10.1093/eurheartj/ehr014 Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2014). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Holzinger, A. (2016). Interactive machine learning for health informatics: When do we need the human-in-the-loop? Brain Informatics, 3, 119–131. https://doi.org/10.1007/s40708-016-0042-6 Holzinger, A. (2021). Explainable AI and multi-modal causability in medicine. i-com, 19(3), 171–179. https://doi.org/10.1515/icom-2020-0024 Holzinger, A., Biemann, C., Pattichis, C., &amp; Kell, D. (2017). What do we need to build explainable AI systems for the medical domain? Holzinger, A., Langs, G., Denk, H., Zatloukal, K., &amp; Müller, H. (2019). Causability and explainabilty of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9. https://doi.org/10.1002/widm.131 Kaladharan, S., Vishvanathan, S., Gopalakrishnan, E. A., &amp; Kp, S. (2020). Explainable artificial intelligence for heart rate variability in ECG signal. Healthcare Technology Letters, 7, 146–154. https://doi.org/10.1049/htl.2020.0033 Khedkar, S., Subramanian, V., Shinde, G., &amp; Gandhi, P. (2019). Explainable AI in healthcare. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3367686 Kieseberg, P., Schantl, J., Fruehwirt, P., Weippl, E., &amp; Holzinger, A. (2015). Witnesses for the doctor in the loop, 9250, 369–378. https://doi.org/10.1007/978-3-319-23344-4_36 Krittanawong, C., Zhang, H., Wang, Z., Aydar, M., &amp; Kitai, T. (2017). Artificial intelligence in precision cardiovascular medicine. Journal of the American College of Cardiology, 69(21), 2657–2664. https://doi.org/10.1016/j.jacc.2017.03.571 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Pedregosa, Fabian, Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016a). \"Why should I trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, KDD san francisco, CA (pp. 1135–1144). New York, NY: Association for Computing Machinery. Rudin, C. (2019a). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1, 206–215. https://doi.org/10.1038/s42256-019-0048 Staniak, M., &amp; Biecek, P. (2018a). Explanations of model predictions with live and breakDown packages. "],["xai1-explainable-wine.html", "1.4 Red wine mystery: using explainable AI to inspect factors behind wine quality", " 1.4 Red wine mystery: using explainable AI to inspect factors behind wine quality Authors: Jakub Kosterna, Bartosz Siński, Jan Smoleń (Warsaw University of Technology) 1.4.1 Abstract Wine is one of the most widespread and culturally significant drinks in the world. However, the factors determining its quality remain a mystery to the absolute majority of people. There are many variables contributing to the final effect and it seems unclear which ones are crucial in making some wines better than the others. In this paper, we have looked at this issue from a fresh perspective using Red Wine Quality from Kaggle community dataset. Much to our initial surprise, despite the fact that a dozen or so chemical factors were taken into account, there is one that stands out and seems to be the main predictor of the drink’s quality - it is alcohol. The study used four black box models interpreted through modern methods of explainable artificial intelligence to explore the subject. 1.4.2 Introduction and Motivation Term ‘glass box models’ refers to interpretable machine learning models - user can explicitly see how they work, and follow the steps from inputs to outputs. The situation is completely different in the case of the very advanced black box models. The goal of explainable machine learning is to allow a human user to inspect the factors behind results given by the model (Baniecki et al. 2020a). There are numerous projects on the Internet that look at the Red Wine Quality dataset from different perspectives. However, due to the nuanced nature of the problem, many of them don’t allow us to draw any constructive conclusions concerning the impact that various physicochemical properties have on the quality of wine. Our goal was to implement XAI solutions in terms of the analysis of the above-mentioned dataset and to confront results with previous research and literature on the subject. In this study, we will be taking a look at all the variables included in our dataset, while paying some special attention to one that seems to be standing out the most - the alcohol content. It is also the one that is the most recognizable to an average consumer and, in contrast to other physicochemical properties, is easily found on every wine label. While analysing the results, we have to keep in mind the obvious limitations associated with the subject. Not only is the perception of the quality of wine an inherently subjective property, but it is also affected by factors not included in the data, such as the color of the wine or the temperature in which the drink was served. 1.4.3 Methodology 1.4.3.1 Dataset The original collection contains 1 600 observations, each representing one Portuguese Vinho Verde of the red variety. It is a proper to analyze and respected set, as evidenced by its verification, multiple use, as well as a very high rating of “usability” of 8.8 on the website. It consists of eleven predictors: fixed acidity - most acids involved with wine or fixed or nonvolatile (do not evaporate readily) volatile acidity - the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste citric acid - found in small quantities, citric acid can add ‘freshness’ and flavor to wines residual sugar - the amount of sugar remaining after fermentation stops, it’s rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet chlorides - the amount of salt in the wine free sulfur dioxide - the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion total sulfur dioxide - amount of free and bound forms of S02 density - the density of water is close to that of water depending on the percent alcohol and sugar content pH - describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic) sulphates - a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant alcohol - alcohol by volume percentage The decision variable was originally quality - the median rating of an assembly of minimum 3 experts, who made their classification on a scale from 0 to 10. Due to the capabilities of the analyzed XAI tools, our team decided to make it a binary classification problem, assigning the wines rated &lt;= 5 value 0, and others - value 1. It resulted in an intuitive “bad / good” wine classification, which gave us 855 “good” and 744 “bad” wines. Figure 1.61: Distribution of target before and after transformation 1.4.3.2 Machine learning algorithms used In order to look at the nature of this data, four well-known algorithms have been trained from data divided into: 1199 observations for the training set and 400 for the test set. XGBoost (gbm) - powerful modern method based on AdaBoost and gradient boosting, imported from xgboost package, with tuned hyperparameters using the randomized search method from sklearn package, with the best values obtained: min_child_weight - 1, max_depth - 12, learning_rate - 0.05, gamma - 0.2 and colsample_bytree - 0.7 Support Vector Machine (svm) - algorithm, in which we plot each data item as a point in n-dimensional space (where n is number of features) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well; imported from sklearn package, with tuned hyperparameters using the grid search method from sklearn package, with the best values obteined: C - 10000, gamma - 0.0001 and kernel - rbf Random Forest (rfm) - method building multiple decision trees and merges them together to get a more accurate and stable prediction; imported from sklearn package, with tuned hyperparameters using the randomized search method from sklearn package4, with the best values obtained: n_estimators - 2000, min_samples_split - 2, min_samples_leaf - 2, max_features - auto, max_depth - 100 and bootstrap - True Gradient Boosting (xgm) - a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error; imported from sklearn package, with tuned hyperparameters using the grid search method from sklearn package, with the best values obteined: learning_rate - 0.1, max_depth - 7 and n_estimators - 50 All methods have been implemented in Python with random states set to 42. The choice of these methods was made on the basis of their popularity, diversity and practicality, taking into account the essence of the problem under consideration. After checking the operation of the models on the test set, the following quality measures were obtained: algorithm accuracy precision recall ROC AUC XGBoost 0.8050 0.806306 0.836449 0.802633 Support Vector Machine 0.7475 0.798942 0.705607 0.750653 Random Forest 0.7850 0.801887 0.794393 0.784293 Gradient Boosting 0.8075 0.818605 0.822430 0.806376 1.4.4 Global explanations 1.4.4.1 Permutation-based variable importance Figure 1.62: Permutation-based variable importance plot for tuned XGBoost model. In this case, size of the bar indicates the positive impact that a feature has on accuracy of the model Our starting point was rendering a permutation-based variable importance plot from DALEX package using our XGBoost model to examine which variables play the biggest role in model’s decision (Baniecki et al. 2020a). The goal of this method is to inspect which variables have positive impact on the accuracy of the prediction. Surprisingly, all of them seemed to provide information that benefited the accuracy of the model. This, combined with a relatively small size of our dataset, led us to decision not to exclude any variables in further proceedings. However, the importance of the variables varies greatly - alcohol and sulphates together overcome all other factors combined. 1.4.4.2 Mean absolute SHAP value Figure 1.63: Mean absolute SHAP value for tuned XGBoost model. Size of the bar indicates how much a feature influences the prediction on average In order to gain a different perspective, we examined a plot of mean absolute SHAP values of our model (Scott M. Lundberg and Lee 2017). SHAP method is explained in chapter Break Down and Shapley Additive Explanations (SHAP). Mean absoulte SHAP value can give us insight into which variables influence the predictions the most. Although this method differs significantly from the above-mentioned permutation-based variable importance, it provided us with a similar information on the hierarchy of importance of the variables - once again, alcohol seems to be the biggest factor, followed by sulphates. Analysing those plots encouraged us to take a closer look at the role that alcohol content plays on the prediction. 1.4.4.3 PDP Figure 1.64: Partial Dependence Plot for all models. Lines represent the average prediction by the model if all observations had a fixed value of a feature, in this case - alcohol content Next, we used Partial Dependence Plot from DALEX package (Baniecki et al. 2020a) to examine an overall effect the alcohol content has on our predictions. PDP tells us what would the average prediction be, if all observations had a fixed value of certain variable. This time, we used all of our models. As expected, tree-based model (XGBoost, GradientBoosting and Random Forest) behave very similarly, while SVM stands out from the rest. However, the main trend is the same - generally, stronger wines are more likely to be good than the weaker ones. Most models reach their lows at just below 10% alcohol content and plateau around 12%. We have to keep in mind that our data contains only one wine that exceeds 14% alcohol content, which may explain the flat line after that mark for tree-based models. It should be noted that this strongest wine was actually ranked as not good. The fact that the line representing SVM model keeps increasing in value throughout the whole range may indicate that this model is more prone to outliers and doesn’t capture complex interactions as well as other models. It coincides with our prior knowledge of the models and the accuracy scores they achieved. 1.4.4.4 ALE Figure 1.65: Accumulated Local Effects Plot for all models. Lines represent how predictions change in a small range around the value of a feature, in this case - alcohol content. In order to take into account possible interactions concerning our variable, we generated an ALE plot using all of the models (D. W. Apley and Zhu 2020). This methods shows us how model predictions change in a small “window” of the feature around certain value for data instances in that window. The results turned out to be quite similar to PDP. It makes sense, considering the fact that our dataset doesn’t contain many strongly corelated variables, and the one that is related the most to alcohol, the density, doesn’t have a very significant impact on the predictions made by the models. The most visible difference is probably the way SVM behaves for the biggest alcohol values - it doesn’t seem to differ as much from the other models as in the previous plot. 1.4.5 Local explanations 1.4.5.1 Break Down and Shapley Additive Explanations (SHAP) The first method that we have used for our local explanations is Shapley Additive Explanations (SHAP). It is based on another explanatory method called Break-Down, which similarly to SHAP is used to compute variables’ attribution to the model’s prediction (Staniak and Biecek 2018a). Break-Down method fixes values of variables in sequence and looks at changes in label value. Moreover, as we can see in figure Figure 1.66 this method is sensitive to the order of variables. Figure 1.66: BreakDown plots for one observation with different order of fixed variables. Size and color of bars indicates changes in model’s predictions. All three presented plots describe the same observation with different sequences of examined variables. When critic acid was fixed as the first one, it lowered the prediction. However, the second plot presents a situation where critic acid was second fixed variable and its value showed a positive impact on rising wine’s rating. This dependency on order made it harder for us to conclude about specific wines. Fortunately Shapley Additive Explanations method (SHAP) resolves this issue by averaging results for different permutations of variables (Scott M. Lundberg and Lee 2017). As an example, you can see computed Shapley values for same observation in Figure 1.67 . Figure 1.67: Plot of Shapley Values for good wine. Size and color of bars indicates mean change in model’s prediction over different orders of fixed variable. Here our model was certain about assigning wine to good ones. Nearly all factors contributed to increasing wine’s rating. Not surprisingly, alcohol and sulphates had the biggest influence on the model’s prediction. Only free sulfur dioxide and pH had worsened the quality of wine, all of which, according to global explanations, are practically irrelevant variables to our model. On the contrary Figure 1.68 presents an observation where all attributes except volatile acidity lowered the rating of our wine. From the values of particular variables, we can see that these wines are completely different, which explains model’s distant assessments. Furthermore, exact values that our model understands as beneficial for wine start to emerge. To get a closer look we will have to use new method described in the following section. Figure 1.68: Plot of Shapley Values for bad wine. 1.4.5.2 LIME In order to look at the explainability of individual predictions, the LIME method from DALEX package was used (Baniecki et al. 2020a). When looking at individual visualizations, it is easy to see that extreme alcohol values have the greatest impact on the prediction value. This is clearly visible on the example of incorrectly classified observations - in a significant number of cases it is the non-standard value of alcohol that largely determines the incorrect prediction. The situation is well illustrated by visualizations generated on the basis of two observations which all of our models misclassified. Figure 1.69: Visualization of LIME method for wine incorrectly classified as bad. Left section represents prediction probability. Middle section displays importance of features and right section their exact values. Blue color indicates that variable is supporting class 0 (bad wine) and orange that variable is supporting class 1 (good wine). Figure 1.70: Visualization of LIME method for wine incorrectly classified as good. However, in this case, alcohol does not always dominate, and the prediction result for the XGBoost model is indeed a component of many elements. However, it has to be considered that the LIME method in this case takes into account only certain automatically selected intervals, which, when combined, do not fully represent the essence of the algorithm. 1.4.5.3 Ceteris Paribus Analyzing results from LIME brought to light yet another issue. Is it possible to change wine’s rating by altering the alcohol content? Answers to this question were provided by a method called Ceteris Paribus. “Ceteris paribus” is a Latin phrase meaning “other things held constant” which accurately describes the main concept of method. Ceteris Paribus illustrates the response of a model to changing a single variable while fixing the values of others. As an example, we will use observation from the previous section which our model incorrectly labeled as the bad one. We quietly assumed that it was due to low alcohol content, but now we can verify our thesis. Figure 1.71: Ceteris-paribus profile for all models. Lines represent how the models’ predictions change over different alcohol contents while keeping other variables fixed. Dots represents actual value of alcohol content in examined misclassified wine. Indeed if our wine was less than a half percent stronger, the model would properly give it a higher rating. However, rising alcohol levels in not always an answer. For instance, very strong wine with 14.7% alcohol presented in Figure 1.72 would benefit from a reduction of alcohol content. Figure 1.72: Ceteris-paribus profile for alcohol content in a very strong wine. 1.4.6 Confrontation with science The subject of the quality of wines in relation to their alcohol content has been studied many times before by other scientists and enologists. The topic was covered, among others, on a post on Cult wines blog (England 2019). In 2019, the author pointed that “It is true that alcohol in wine tends to draw out more intense, bold flavours, so the higher the alcohol level, the fuller the body.” Certainly, the phenomenon is also confirmed by the popular fact of preferences of esteemed former wine critic Robert Parker, who was well known for awarding higher scores to higher alcohol wines. On the other hand, lower alcohol wines tend to offer greater balance and pair better with foods. Hence, too much content is also not an advantage in terms of quality. The phenomenon was also examined by the newspaper the Seattle Times, which published an article in 2003 entitled Does a higher alcohol content mean it’s a better drinking wine? (Gregutt 2003). The expert emphasizes that higher alcohol is an indication of better ripeness at harvest and fermentation to complete or near-complete dryness. With time, the wines are also getting stronger, which is the result of better vineyard practices, letting the grapes get more “hang time” and more efficient yeasts, which definitely has a positive effect on the quality. Here, however, the argument for not too high alcohol content is also emphasized, due to the fact that wines with more than 15 percent are almost never ageworthy. The high alcohol throws the balance off and is often accompanied by too much oak and too much tannin. These wines are also hard to drink, as they do not match well with most foods. It is also worth comparing an article from the Decanter website (Jefford 2010) with the results of our study. As a person well acquainted with the subject, the expert considers the alcohol threshold of a good wine, coming to a conclusion consistent with the results of our calculations. Professional research was conducted in 2015. A group of Portugese experts conducted a thorough research, the result of which was the article From Sugar of Grape to Alcohol of Wine: Sensorial Impact of Alcohol in Wine (Jordão et al. 2015). They indicated in it that the quality of grapes, as well as wine quality, flavor, stability, and sensorial characteristics depends on the content and composition of several different groups of compounds from grapes. One of these groups of compounds are sugars and consequently the alcohol content quantified in wines after alcoholic fermentation. During grape berry ripening, sucrose transported from the leaves is accumulated in the berry vacuoles as glucose and fructose resulting in a fuller taste of a final wine. The idea of a threshold of an alcohol content, beyond which wines lose their quality, is a common theme in literature dedicated to the topic. There is evidence suggesting that wines that have more than 14.5% of alcohol start to come off as herbaceous instead of fruity (GOLDNER et al. 2009). The research states that the sensory perception of the aroma changed dramatically with the level of ethanol content in the wine, but the change isn’t consistent among all chemical compounds. Another very interesting topic is the idea of reducing the alcohol content in wine after initial production process (Jordão et al. 2015). Motivation behind such actions is to try to keep all the good aromas and qualities associated with stronger wines, while simultaneously lowering excessive alcohol which causes bitter or sour tastes. Such attempts ended in mixed results, but evidence suggests that it may be possible to lower alcohol content by one percentage point without losing many aromas. 1.4.7 Summary The analysis of the results of modern methods of explainable artificial intelligence showed a relatively unambiguous conclusion - taking into account the proven data set of popular wines assessed by experts, the alcohol content definitely has a large impact on the average rating of wines. Despite taking into account many different chemical factors, as well as the variety of black box models and the differences between individual observations, alcohol is a factor that stands out from other predictors. This result is evidenced not only by the high correlation and simple conclusions from the visualizations, but also by the results of the algorithms explaining the results of the models used. However, the methodology has produced less obvious conclusions - although higher alcohol content is associated with higher alcohol quality (at least in a pseudo-objective sense), there is a threshold at which this property reverses. These considerations are the result of measuring not only one or two approaches, but a total of research based on as many as seven methods. It is a good argument to appreciate both them and modern solutions of explainable artificial intelligence. Indeed, the results of information and mathematical analysis are also consistent with what we can observe in nature - the effects of the calculations made are consistent with previous research related to the topic, and not necessarily based on methods related to the field of data science. 1.4.8 Conclusions The experiment is undoubtedly successful and with a high degree of certainty confirms two facts: firstly, the quality of the wines is dependent on their higher alcohol content, but up to a certain threshold and secondly, modern methods of explainable artificial intelligence provides us valuable results that bring new valuable information about the black box models, which at the same time are consistent with the nature of the data sets on which they were made. Different approaches provide us with various useful information and it is difficult to indicate better or worse solutions - the seven methods mentioned complement each other in their own way, but each allows us to look at the problem of analysis from a different perspective. Leaving aside our succesful implementation of XAI solutions, one must bear in mind, that the results of the experiment need to be put in a certain perspective. It must be remembered that the world of wines is a very diverse one. For example, artificially fortified wines like port wine or sherry should be considered separately. The discussed data set concerns only a certain group of wines and we don’t know enoguh about the experts or conditions of the tastings to call the data entirely representative. Therefore, we should remain cautious when drawing any far-reaching conclusions. Nevertheless, the conducted research approximates the idea of the overall effect of alcohol on the quality of wine and confirms previous research by experts from a biochemical point of view. References Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 England, R. (2019). Wine’s alcohol levels explained. https://www.wineinvestment.com/wine-blog/2019/05/wines-alcohol-levels-explained?fbclid=IwAR3xpQITEQZrQUPPaEt7-DbFHmvHE559-iVuLsgS6dDinOeWrl04MZiglbM. GOLDNER, M. C., ZAMORA, M. C., DI LEO LIRA, P., GIANNINOTO, H., &amp; BANDONI, A. (2009). EFFECT OF ETHANOL LEVEL IN THE PERCEPTION OF AROMA ATTRIBUTES AND THE DETECTION OF VOLATILE COMPOUNDS IN RED WINE. Journal of sensory studies, 24(2), 243–257. Gregutt, P. (2003). Does a higher alcohol content mean it’s a better drinking wine? The Seattle Times. https://archive.seattletimes.com/archive/?date=20031008&amp;slug=wineqanda08&amp;fbclid=IwAR3lBlpdwUCUWjWKaH4Px21b9fJQwBT0aMTa8bNWCbx4ipo4otWzvR9_mTc Jefford, A. (2010). Alcohol levels: The balancing act. https://www.decanter.com/features/alcohol-levels-the-balancing-act-246426/?fbclid=IwAR0bsIWug6-7l77rxb01Va8P1F_hVkaUTacNtlF-V-wRXb1HA3rJXpl74Pw. Jordão, A. M., Vilela, A., &amp; Cosme, F. (2015). From sugar of grape to alcohol of wine: Sensorial impact of alcohol in wine. Beverages, 1(4), 292–310. https://doi.org/10.3390/beverages1040292 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Staniak, M., &amp; Biecek, P. (2018a). Explanations of model predictions with live and breakDown packages. "],["xai1-explainable-hotels.html", "1.5 Explanatory approach to modeling the risk of hotel booking cancellations", " 1.5 Explanatory approach to modeling the risk of hotel booking cancellations Authors: Mateusz Krzyziński, Anna Urbala, Artur Żółkowski (Warsaw University of Technology) 1.5.1 Abstract Booking cancellations are a key aspect of hotel revenue management. Therefore, in order to better manage the reservation system and apply appropriate cancellation policies, decision support systems based on machine learning models are used. However, behavior of the described and introduced models has not been studied so far and they have not been tested in terms of the causes of their predictions. To fill this gap, we used explainable Artificial Intelligence methods to investigate predictive models based on booking data, both on a global level and for decisions related to individual customers. The purpose of our study is also to provide new insights into the factors that influence cancellation behavior. 1.5.2 Introduction One of the biggest problems and challenges facing the hospitality industry is the significant number of canceled reservations. Common reasons for cancellations include a sudden deterioration in health, accidents, bad weather conditions, schedule conflicts, or unexpected responsibilities (Falk and Vieru 2018). Interestingly, a noticeable group consists of customers who, after making a reservation, are still looking for new, better offers, and even make many reservations at the same time to be able to choose the most advantageous one (Antonio et al. 2017). The hospitality industry’s response to the above problem is hotel cancellation policies. They play a crucial role in determining various aspects of the hotel business, including the ultimate goal of revenues and profits optimization. In recent years (before the pandemic), there has been a clear tightening of these policies. Hotels do this, for example by shortening the free cancellation windows or increasing cancellation penalties (Riasi et al. 2019; Smith et al. 2015). The use of machine learning to forecast and identify potential cancellations is also playing an increasing role. There are many systems to support hotel management that use booking data. Various machine learning algorithms are used for this purpose, ranging from support vector machines, through artificial neural networks, to the most common tree-based models (Andriawan et al. 2020; Sánchez-Medina and C-Sánchez 2020). Most of the solutions and projects are only theoretical, while some have been tested in practice, enabling cancellations to be reduced by up to 37 percentage points (Antonio et al. 2019a). Unfortunately, most papers do not tackle the issue of the importance of the used explanatory variables and do not try to explain the model’s predictions. However, it is the exploration of trained models that should be treated as one of the key factors in the design of hotel management support systems. Business validation and ethical verification of solutions are necessary. Bearing in mind that a strict cancellation policy or overbooking strategy can have negative effects on both reputation and revenue, systems designers should be wary of unfairly biased behavior. At the same time, the use of explanatory artificial intelligence methods is helpful in creating models with better performance scores. In the following chapter, we present an analysis of predictive models for hotel booking cancellations. We answer questions about the reasons for the model prediction both in general view and in relation to individual reservations. The study was conducted in Python using the scikit-learn (F. Pedregosa et al. 2011a) machine learning library. Most of the eXplainable Artificial Intelligence (xAI) methods used in the article are implemented in the DALEX (Baniecki et al. 2020a) package. However, we also used methods implemented by ourselves, based on the triplot (Pekala et al. 2021) package available in the R language. 1.5.3 Dataset and models The dataset used to build the models and perform the analysis is the Hotel booking demand dataset, which is publicly available on Kaggle. This is a cleaned version of the datasets described in the article Hotel booking demand datasets (Antonio et al. 2019b). It contains information about bookings from two hotels in Portugal for the period from July 2015 to August 2017. One of the hotels is a resort hotel and the other is a city hotel. There are 119 390 observations in total (each of them describes one reservation). The key is that about 37% of them were canceled which is a pretty large number. The dataset provides 31 attributes that can be useful in prediction but we have selected only the 17 most promising, removing the target leakage or redundant features. The variables included in the modeling are described in the Table 1.2. Table 1.2: Descriptions of variables used in modeling. The alphabetical order of variable names has been preserved. Variable Description adr Average Daily Rate, calculated by dividing the sum of all lodging transactions by the total number of staying nights adults Number of adults agent ID of the travel agency that made the booking arrival_date_week_number Week number of the arrival date booking_changes Number of changes/amendments made to the booking from the moment the booking was entered on the Property Management System (PMS) until the moment of check-in or cancellation country Country of origin customer_type Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group - when the booking is associated to a group; Transient - when the booking is not part of a group or contract, and is not associated to other transient booking; Transient-party - when the booking is transient but is associated to at least another transient booking hotel Type of hotel lead_time Number of days that elapsed between the entering date of the booking into the PMS and the arrival date market_segment Market segment designation. In categories, the term \"TA\" means \"Travel Agents\" and \"TO\" means \"Tour Operators\" previous_bookings_not_canceled Number of previous bookings not canceled by the customer prior to the current booking previous_cancellations Number of previous bookings that were canceled by the customer prior to the current booking required_car_parking_spaces Number of car parking spaces required by the customer reserved_room_type Code of room type reserved. Code is presented instead of designation for anonymity reasons stays_in_week_nights Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) Already at the stage of exploratory data analysis (EDA), we managed to notice interesting relationships between the features of the reservation and the indicator of its cancellation. An example of the noticed dependencies is illustrated in Figure 1.73. The use of the methods of explainable artificial intelligence helped us, among other things, to confirm the assumptions we had made. Figure 1.73: Effect of lead time on cancellations. Quantile binning (with the deciles) was used to bin the observations into 10 groups. The ranges of each group are shown on the x-axis. The y-axis shows the percentage share of canceled reservations in a given group. There is a clear trend towards more frequent cancellations of bookings made in advance. As part of the research, we trained four models: Decision Tree, Random Forest, Logistic Regression, and XGBoost. In order to select the main model for the project, we compared different metrics. The values of the considered metrics are shown in the Table 1.3. The best model was Random Forest, so we focused on tuning its hyperparameters. We also prepared a version of Random Forest using the label encoding of categorical variables (all previous models used one-hot encoding). Both tuned Random Forest models achieved very similar results, so we decided to use the one with label encoding, which facilitated the use of xAI methods. Thus, in the rest of the article, when we refer to model, we mean the Random Forest model with label encoding, unless otherwise stated. Note also that this is a black-box model, so the use of xAI methods is especially important when trying to understand its predictions. Table 1.3: Values of considered metrics for trained predictive models. Values are rounded to two decimal places. Model Recall Accuracy ROC AUC F1 Decision Tree 0.77 0.82 0.82 0.76 Random Forest 0.76 0.87 0.93 0.81 Logistic Regression 0.54 0.75 0.78 0.62 XGBoost 0.71 0.85 0.91 0.77 Tuned Random Forest (with one-hot encoding) 0.82 0.89 0.96 0.84 Tuned Random Forest (with label encoding) 0.82 0.89 0.96 0.84 1.5.4 Local explanations In order to explain model output for a particular guest and their booking, we used instance-level exploration methods, such as Break-down (Staniak and Biecek 2018a), SHapley Additive exPlanations (SHAP) (Scott M. Lundberg and Lee 2017), Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al. 2016a), Ceteris Paribus (CP) (Goldstein et al. 2014). We decided to investigate noteworthy predictions, i.e. false positive and false negative (respectively canceled bookings predicted as not canceled and vice versa), the most valid (the predictions the model was most sure of), and the closest to decision boundary. 1.5.4.1 False positive and false negative predictions We might discover that the model is providing incorrect predictions. The key is to find the reasons for this, that is, to answer the question of what has driven the wrong prediction. We used local explanations methods for the observations in both groups with the worst predictions, i.e. the lowest probability of proper classification. Figure 1.74: A plot of Shapley values for the main Random Forest model and misclassified observation (false positive) with the highest probability of cancellation. The green and red bars correspond to the contribution of the variable to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). On the x-axis, there is the model prediction value, while on the y-axis there are variables and their values for the observation. Informations with the biggest contribution to the final prediction are guest’s country of origin (Portugal), a total number of special requests equals zero, and the fact that customer type related to the given booking is Transient-Party. This is an indication that the model may be slightly biased due to the country of origin. It is the property of the customer and not of the booking itself. Thus, depending on the application, it is worth considering whether this response is satisfactory and meets ethical standards. With every value contributes to the misprediction, the only feature with the correct contribution is customer type (Transient-Party guests are the most popular type of customers, accounting for as much as 75% of bookers). Figure 1.75: A plot of Shapley values for main Random Forest model and misclassified observation (false negative) with the lowest probability of cancellation. The elements of the plot have the same meaning as in the previous case. It can be seen that the largest contributions are related to the country of origin of the booker and the type of guest assigned to them. It is worth noting that the values of these variables are the same as in the case of the observation analyzed in Figure 1.74. Again, they contribute to the same side of the prediction, but the contribution values are different. In this case, the type of client turns out to be the most important. Most of the values also affect the prediction of no cancellation. It is interesting that a slightly later booking in relation to the date of stay (lead time) has the opposite effect than in the previous example. The reason is the dependencies between the variables. Figure 1.76: A plot of LIME model values for the main Random Forest model and the most missclassified observations. The similarity between the observations is also noticeable in the LIME method. The first five variables are identical and have almost the same coefficients. Therefore, it is these less significant variables that influence the final prediction. In the case of these two observations with similar characteristics, but completely different predictions, the use of the SHAP method (generalizing the Break-down method) gives a better picture. The glass box model selected in LIME method to approximate the black-box model, and not the data themselves, is not able to capture dependencies between variables. 1.5.4.2 The most valid predictions The considered model returns an appropriate prediction in over 89% of cases. However, the level of certainty of the model with respect to the prediction (i.e. the probability that an observation is assigned to a class) may be different. Thus, it is worth considering why the model is almost sure of some outputs and how would the model’s predictions change if the values of some of the explanatory variables changed. We used local explanations methods for the observations in both groups with the best predictions, i.e. the highest probability of proper classification (equal to 1.0). Figure 1.77: A plot of Shapley values for the main Random Forest model and observation with sure negative prediction. Like in the previous examples - the largest contribution has the country, in that case: France. Again, this is a Transient-Party customer and that also affected the prediction. Also, no special requests affect negatively prediction (more than eg. no previous cancellations). Only one of the top variables affected positively: it was no required car parking spaces, but this impact was unnoticeable in the final prediction. Figure 1.78: A plot of Shapley values for the main Random Forest model and observation with sure positive prediction. Again, Portugal as a country of origin affected positively the probability of cancellation (keep in mind that the hotel is in Portugal, so we can assume that compatriots cancel their reservations more often). Also, no special requests affected positively prediction (although in the previous case it had a negative effect). We can notice that for positive prediction other factors have the biggest impact than for negative. Eg. a longer lead time moved up to the third place and now has a positive impact. Figure 1.79: Ceteris Paribus profiles for the selected continuous explanatory variables and label encoded country variable for the main Random Forest model and observations with the sure prediction. Dots indicate the values of the variables and the values of the predictions for observations. Green profiles are for sure positive prediction (a cancellation), while blue profiles are for sure negative prediction. Looking at the Ceteris Paribus profiles, it is intuitive to see that the prediction for the observation classified as the not canceled stay is more stable, i.e. less sensitive to changes in the values of explanatory variables. In the case of observation of canceled reservations, a change of the arrival date by a few weeks would cause a significant decrease in the certainty of the prediction. It is related to the seasonality of bookings (the decrease occurs at the beginning of July - the holiday period). However, the biggest changes in the prediction for this observation could be due to noting the fact of additional booking requirements (required car parking spaces and a total number of special requests). Changing these values to non-zero would change the prediction completely. Moreover, the huge changes depend on the country of origin of the booker, which in this case is Portugal. When considering the prediction for an observation classified as not canceled, we see that the only explanatory variable whose change would have a significant impact on the certainty of the prediction is the number of previously canceled reservations. A change to any non-zero value would change the prediction, but its certainty would be close to the decision boundary. 1.5.4.3 The closest to decision boundary predictions We analyzed the situations when the model is sure of the returned output. However, the observations for which the prediction was uncertain, close to the decision limit, are also worth considering. We might want to know the answer to the question of whether it is a matter of similar numbers of explanatory variables shifting the prediction in different directions, or maybe there are variables that do not fit the whole picture, and therefore the model is not certain. It is also worth checking how much such predictions fluctuate depending on the changes in the explanatory variables. Figure 1.80: A plot of Shapley values for the main Random Forest model and observation classified as negative with probability near 50%. This is a very interesting example. One variable fixed prediction. Very short lead time (one day) opposed all other factors like Portugal as country of origin or no special requests and made the model predict correctly. It is amazing, that one factor can change everything. Figure 1.81: A plot of Shapley values for the main Random Forest model and observation classified as positive with probability near 50%. This observation is not so exciting as the previous one, but it is the next evidence that the special requests decrease the probability that the client will cancel the reservation. Nevertheless, this observation was classified as positive. Agent was the most important positive variable although in the previous examples he did not have such a contribution. In order to better understand why the predictions for these observations are close to the decision boundary, we used the method implemented by ourselves, based on the triplot, which made it possible to check the importance of thematically grouped variables, taking into account the correlation between them. Thanks to this, it was also possible to look more generally at the factors influencing a given prediction (note that it can be complicated to investigate the effects of particular variables if there are many, as in this case). The thematic groups of variables, i.e. the aspects, we have created are presented in the Table 1.4. Table 1.4: Created aspects and features grouped in them. Aspect name Features time lead_time, stays_in_weekend_nights, stays_in_week_nights, arrival_date_week_number previous previous_bookings_not_canceled, previous_cancellations booking_type agent, market_segment, customer_type hotel_information adr, hotel, reserved_room_type additional required_car_parking_spaces, total_of_special_requests, booking_changes personal country, adults Figure 1.82: A plot of Aspects Importance for the main Random Forest model and observation classified as negative with probability near 50%. It shows the group of variables contributions to the single prediction. The green and red bars correspond to the contribution of the aspects to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). Figure 1.83: A plot of Aspects Importance for the main Random Forest model and observation classified as positive with probability near 50%. Now we can clearly identify why the model predictions are close to the decision boundary. Note that the most important aspect for both observations affects the prediction in one direction, and the next three affect the prediction in the opposite. In the case of the non-canceled booking, the most important factors are time-related variables (of course, lead time is the key here, also indicated as the most important in the SHAP, see Figure 1.80). However, we can notice new insights regarding the canceled booking. It turns out that the model indicated its cancellation mainly due to factors related to the booking type. 1.5.5 Global explanations The second group of methods of explainable artificial intelligence is those concerning not a single observation, but the entire set of them. We used these model-level explanations to provide information about the quality of the model performance and infer how the model behaves in general. The methods we used for this purpose are Permutational Variable Importance (A. Fisher et al. 2019a), Partial Dependence Profile (PDP) (Friedman 2000a), Accumulated Local Effects (ALE) (D. W. Apley and Zhu 2020). We applied them not only to the main Random Forest model but also to other trained models to compare the obtained results. 1.5.5.1 Importance of explanatory variables First, we decided to check which variables are important for our main model and compare the obtained results with the intuitions we had after conducting the exploration at the prediction level. Figure 1.84: A plot of variable importance for the main Random Forest model. The length of each bar represents the difference between the loss function (1-AUC) for the original data and the data with the permuted values of a particular variable. As we can see above, the most important variable for the model is the information about the guest’s country of origin. This confirms the intuitions obtained thanks to local explanations, for many observations this variable was the key aspect. In particular, the origin of Portugal (the country where the hotels are located) is the most significant for the prediction. The method indicated that the next principal variables for predictions are information on lead time and the number of special booking requests. Lead time is the number of days that elapsed between the entering date of the booking into the system and the arrival date. It is a factor that may inform about whether the stay was planned long before or it is spontaneous. Meanwhile, the number of special requests is related to additional interest in the booking, which may indicate that it is an important stay for the client. The next variables with a significant average impact on the model are the group of those concerning the booking method (agent and market segment) and the type of booking (customer type and adr associated with the booking cost). It should also be noted that some of the variables are of marginal importance. These are (from the bottom of the plot) the type of hotel (recall that the data relate to two hotels of different specificity), the number of adults, the number of previously not canceled bookings (this is probably also related to a small number of observations with a non-zero value of this feature - 3.1% in the entire dataset), the type of room reserved. These are the variables that should be considered to be excluded in order to simplify the model. On the other hand, it is quite surprising that the number of previously canceled reservations, which was often indicated by LIME as one of the most important factors for the predictions under consideration, is so insignificant (0.007 drop-out loss) according to the algorithm of the permutational variable importance. 1.5.5.2 Comparison with other models Plots similar to that above in Figure 1.84 are useful for comparison of a variables’ importance in different models. It may be helpful in generating new knowledge - identification of recurring key variables may lead to the discovery of new factors involved in a particular mechanism (Biecek and Burzykowski 2021) (in our case, cancellation of reservations). Thus, we decided to compare the importance of the variables in the models we had trained, described in the Dataset and models section. The plots below show the results for our main Random Forest model and 4 other models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one-hot encoding, purple - Logistic Regression, blue - Decision Tree, orange - XGBoost. Figure 1.85: The comparison of the importance of explanatory variables for selected models. The elements of a single plot were described above. Note the different starting locations for the bars, due to differences in the AUC score value obtained for the training dataset for different models. It can be seen that the importance of the variables is related to the type of algorithm used in a given model. For example, in a decision tree, each variable has a clearly noticeable effect on the predictions. In all the tree-based models explained, the most important variable overlaps - it is the aforementioned country of origin. In the logistic regression model, this feature is the third most important variable, but it is the model with the worst score overall. However, in general, the groups of the most important variables in each model are also similar. So our earlier conclusions regarding the key booking cancellation factors are confirmed. Likewise, in each of the tree-based models (even in a single decision tree model), the same variables were indicated as least important. An interesting fact is that in Random Forest with one-hot encoding the agent variable is even more important than in Random Forest with label encoding. It is a categorical variable, so we can see the influence of encoding here - trees could extract more information from this variable thanks to not creating unnatural numerical relationships as with label encoding. The second noticeable difference between these models is the importance of the market segment. 1.5.5.3 The global impact of variables Then we decided to check the influence of the most influential variables on the predictions in the context of the whole set. For this, we used the ALE method. The plots below show the results for our Random Forest and 3 other tree-based models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one-hot encoding, blue - Decision Tree, orange - XGBoost. Note that we chose not to generate plots for the logistic regression model because it performed too poorly. Figure 1.86: ALE plot for the country. The ALE plots work like Ceteris Paribus - they show how the variable affects a prediction but not only for one observation - for the entire training dataset. We have a greater probability of resignation for one country - this is Portugal. This confirms our hypothesis built on the basis of local explanatory methods. Generally, compatriots more often resign from booking. Figure 1.87: ALE plot for lead time. And again we have confirmation that the longer the lead time, the greater the chance that the customer will resign. The ALE plots are very similar to Ceteris Paribus but that is a great example that they are not the same. Look at the Ceteris Paribus profile for lead time in Figure 1.79. The probability of resignation increased to about 170 days, then decreased and remained at a constant level. Here we can see that it was the “local behavior.” Globally, the probability only grows, then it stabilizes (after about 350 days - a year). Therefore, you may suspect that it is not worth allowing reservations so far in advance. Figure 1.88: ALE plot for lead time shorter than 35 days. Moreover, one can spot the biggest change in predictions up to 30 days of lead time, which confirms the intuition coming from data exploration (see Figure 1.73). Thus, it is worth taking a closer look at how the ALE profiles change in this short period. In Figure 1.88, we can see that the increase in the average prediction is greater up to about 15 days, then it starts to stabilize. Figure 1.89: ALE plot for total of special requests and required car parking spaces. Additional actions taken by the client regarding booking reduce the likelihood of cancellation. The very first special request significantly reduces the probability of resignation. The influence of the next ones is not that clear. This also confirms the thesis we made earlier that the lack of special requests increases the probability of resignation. Reserving a parking space has an even greater impact on the predictions. We may think that in these hotels it is payable in advance, or that car travelers are less dependent on public transport, so their arrival is more certain. Figure 1.90: ALE plot for previous cancellations and previous bookings not canceled. Information about a given customer’s prior bookings is also very valuable for prediction. The fact of earlier cancellation of a reservation strongly influences the prediction of the next one, which seems natural. A non-zero number of prior non-canceled bookings works the opposite way, but the prediction values don’t fluctuate that much. After analyzing these examples, an important conclusion can be drawn about the XAI methods. ALE plots can be a great tool for analyzing the influence of variables on the prediction - they can be used to verify the hypotheses put forward at the stage of local explanations and introduce new ones. When comparing the results for different models, note that the profiles for Random Forests with both versions of the categorical variable encoding are almost identical (green and red lines in the graphs). Looking more broadly, the profiles are comparable for all models. The most significant differences can be seen in the variables relating to the previous reservations of a given customer - the number of canceled and non-canceled reservations. The XGBoost model favors non-zero values more - the prediction changes are bigger. In general, this model is the most sensitive to all variables, as can be seen from the shape of the profile curves. Moreover, we used the comparison of the PDP and ALE plots for our main model (see Figure 1.91). In the case of some variables, the profiles generated using both methods almost coincide. However, there are also variables where you can see differences in prediction values, but profiles are parallel to each other. This parallelism suggests and allows us to conclude that the used model is additive due to these explanatory variables (Biecek and Burzykowski 2021). Figure 1.91: Partial-dependence and accumulated-local profiles for the main Random Forest model and selected variables 1.5.6 Summary and conclusions The use of explainable artificial intelligence methods helped us understand the behavior of the models created. This is crucial before the implementation of any hotel management support systems based on machine learning models, in particular in the light of the current legal regulations (Goodman and Flaxman 2017) and the currently processed documents (Glauner 2021). Referring to the so-called Right to Explanation, it is especially important to provide the analysis of the examples of model prediction for single observations, i.e. a single customer and the booking made by them. The way the analysis presented by us can be considered an example of partial explanation. In addition, we have indicated the types of predictions that should be examined before putting the model into use - false positive and false negative, the most valid, and the closest to the decision boundary. Furthermore, we managed to discover that the country of origin of the booker is the most important factor determining the decisions of the created models. They are more likely to predict cancellations for people from the country where the hotels are located, i.e. Portugal. In business use, this could be considered an unfairly biased behavior, so the possible solution would be to use separate models for predicting cancellations for foreigners and compatriots. Moreover, the identification of the least significant variables makes it possible to eliminate them and simplify the model. Thanks to the conducted analysis, we also discovered the variables to which the model is notably sensitive. We indicated the ranges of the biggest changes in prediction, which in the case of designing the final system may be crucial for uncertain bookings, i.e. the predictions that are close to the model decision boundary. The xAI methods also allowed for the validation of the created model and an additional, not related to any metric, assessment of its behavior. By comparing several models, we gained confidence in the correctness of the main model analyzed in the article. The comparisons made also showed the impact of the coding of categorical variables on the predictions of the model. It turned out that this technical aspect did not significantly affect the interpretation of the features by models. In addition, it should be borne in mind that the discovered and described relationships serve not only to explain the operation of the model itself, but also constitute a newly generated knowledge about the phenomenon of hotel cancellation and confirm its causes, often intuitive, but not described in the scientific literature. This can be helpful for research on hotel cancellation policies. To conclude, the above article shows a real-life use case of using the methods of explanatory artificial intelligence. It is these methods that allow us to understand the operation of complex machine learning models, enabling us to validate them and generate new domain knowledge. References Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Antonio, N., de Almeida, A., &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., de Almeida, A., &amp; Nunes, L. (2019a). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Antonio, N., de Almeida, A., &amp; Nunes, L. (2019b). Hotel booking demand datasets. Data in Brief, 22, 41–49. https://doi.org/10.1016/j.dib.2018.11.126 Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Friedman, J. H. (2000a). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Glauner, P. (2021). An assessment of the AI regulation proposed by the european commission. https://arxiv.org/abs/2105.15133 Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2014). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Goodman, B., &amp; Flaxman, S. (2017). European union regulations on algorithmic decision-making and a “right to explanation.” AI Magazine, 38(3), 50–57. https://doi.org/10.1609/aimag.v38i3.2741 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011a). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Pekala, K., Woznica, K., &amp; Biecek, P. (2021). Triplot: Model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure. CoRR, abs/2104.03403. https://arxiv.org/abs/2104.03403 Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016a). \"Why should I trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, KDD san francisco, CA (pp. 1135–1144). New York, NY: Association for Computing Machinery. Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Smith, S. J., Parsa, H. G., Bujisic, M., &amp; van der Rest, J.-P. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 Staniak, M., &amp; Biecek, P. (2018a). Explanations of model predictions with live and breakDown packages. "],["explainable-artificial-intelligence-2.html", "Chapter 2 Explainable Artificial Intelligence 2", " Chapter 2 Explainable Artificial Intelligence 2 "],["xai2-phones.html", "2.1 Does brand has an impact on smartphone prices?", " 2.1 Does brand has an impact on smartphone prices? Authors: Agata Kaczmarek, Agata Makarewicz, Jacek Wiśniewski (Warsaw University of Technology) 2.1.1 Abstract Mobile phone became indispensable item in our daily life. From a simple device enabling contacting other people, it developed into a tool facilitating web browsing, gaming, creating/playing multimedia and much more. Therefore, the choice of phone is an important decision, but given the wide variety of models available nowadays, it is sometimes hard to choose the best one and also easy to overpay. In this thesis, we analyze the phone market to investigate whether the prices of the phones depend only on their technical parameters, or some of them have an artificially higher price regarding the possessed functionalities. Research is conducted on the phones dataset, provided by the lecturer, containing prices and features of different phones. As a regressor, Random Forest from package ranger was chosen. Given the type of the model (blackbox i.e. non-interpretable), we use Explainable Artificial Intelligence (XAI) methods, for both local and global explanations, to interpret its predictions and get to know which features influence them the most, and lower/raise the price of the phone. 2.1.2 Introduction and Motivation Mobile phones, since their coming onto the market, have gradually entered people everyday life. According to GSMA, almost 70% of the world’s population has one. This market has gained significant popularity in 2007, with the introduction of Apple’s iPhone. It revolutionized the industry by offering features such as a touch screen interface and a virtual keyboard. The smartphone market has been steadily developing and growing since then, both in size, as well as in models and suppliers. Smartphone, due to its mobility and computer abilities, has become a source of entertainment, a communication tool, a search engine and so much more. Suppliers constantly outdo each other introducing new improvements, better cameras or batteries, to attract customers. One could wonder which of them (or whether all of them) cause the price increase. Another question is, whether the price depends on the manufacturer - are there certain producers whose phones are more expensive, no matter the parameters? The task of determining a relationship between a smartphone’s features, brand, and the price is surely non-trivial. In such a problem, machine learning can be useful. Machine learning algorithms step into more and more areas of our life. We use them in risk analysis, medical diagnosis or credit approval, so why could they not be used in phones pricing? In general, we can distinguish two types of models: glass-box, which steps can be followed from inputs to outputs, and black-box, which do not have a readable way of determining predictions. In many cases, such as the one considered in this article, simple interpretable models are not capable of dealing with our problem satisfyingly, so we turn to more complex, non-transparent ones, which grant us higher accuracy, but lower understanding, and therefore, lower trust. Explainable Artificial Intelligence (XAI) addresses this problem. It is a set of tools to help you understand and interpret predictions made by your machine learning models. With it, one can debug and improve model performance, and help others understand models’ behaviour. In the article below, we deal with the problem of creating an explainable regressor for mobile phones prices. We build a black-box model and then use XAI methods to find out which features and brands contribute mostly to the final price. 2.1.3 Related work Ha Ngoc Anh (2-1-phones-smartphone?) shows changes in the mobile industry and competition between various smartphone companies and brands. There are also papers, which use Machine Learning tools to answer the question of predicting prices by various models and neural networks. Ibrahim M. Nasser et.al. (2-1-phones-ann?) proves the point in predicting smartphone prices with the use of neural networks. And Ritika Singh (2-1-phones-eda?) examines the importance of various features in predicting smartphone prices. In this article Explainable Artificial Intelligence methods were used to address the problem. They were described by Przemysław Biecek et.al. in “Explanatory Model Analysis” book (Biecek and Burzykowski 2021), with local methods as Break-down, Shapley and Ceteris-Paribus; and global as Partial-dependence profiles being explained there. Also, more explanations about XAI are to be found in articles about LIME (Ribeiro et al. 2016a) and ALE (D. W. Apley and Zhu 2020) plots. 2.1.4 Methodology Data description Research was carried out on the phones dataset provided by the lecturer i. e. data frame with prices and technical parameters of 414 phones. It contains ten explanatory variables and one target variable (price), therefore we deal with regression task. The sample of the data is presented below (2.1). Figure 2.1: Sample of the data Exploratory Data Analysis and data preprocessing At the beginning of our research we conducted Exploratory Data Analysis to get better understanding of the data we deal with. We mainly focused on the target variable and its distribution versus explanatory ones to identify potential influential features for our prediction. Below we present some results important for further work (2.2). Figure 2.2: 10 brands with the highest mean price of the phone Analyzing brands by the mean price of the phones produced, there can be identified a distinct leader, which is an Apple company. On average, a phone from it costs more than 3500 zlotys. In the top 10 brands, we can also see common ones such as Samsung or Huawei. Figure 2.3: Phone price distribution for 4 most common brands On the plot above (2.3) we can identify some outliers in terms of price, especially a phone made by Samsung company, which costs 9000 zlotys. Concerning the Xiaomi brand, we can observe that despite being a popular choice, the price of a single phone is relatively low - no phones are exceeding 4000 zlotys. As for the Apple products, conclusions from the plot above are confirmed. Figure 2.4: Correlation matrix Based on the partially presented EDA, we needed to conduct simple data preprocessing before modeling. Following steps were executed: handling missing values: Two features containing missing values were identified; both related to camera parameters (back_camera_mpix, front_camera_mpix). Those values turned out to be meaningful, as they mean that given mobile phone has no camera (back or front). Given that information, NAs were imputed with a constant value - 0. removing outliers: Based on features distribution, some extreme values were identified in the dataset’s explanatory variables (back_camera_mpix, front_camera_mpix, battery_mAh, flash_gb, price), which would weaken the model’s performance. Therefore they were removed. dealing with unimportant and correlated features: The variable name has been omitted, because it was practically unique in the dataset and naturally connected to the brand feature. Moreover, height_px and width_px were deleted due to their strong correlation with the diag feature (and with each other) (2.4); this feature was considered as a sufficient determinant of the phone’s dimensions. Models Next step after EDA was creating prediction models. To compare results and find the best model for mentioned data, there were created 3 models: ranger, xgboost and svm. Every model used cross validation during trainning. Svm and xgboost models cannot be trained using character variables so variable brand needed to be target encoded in these cases. There were used two measures to compare models results: root mean square error (rmse) and mean absolute error (mae). The results are presented in the table below ( 2.5). Figure 2.5: Correlation matrix The presented results point, that the ranger model is the best choice. This model was used in further analysis. 2.1.5 Results Ranger model results have been analyzed using Explainable Artificial Inteligence methods. Following paragraphs present local and global explainations. Local explainations In the first step of our XAI, the focus was on instance level explanations - analysis of single predictions and how each feature influences their values. Break down, SHAP, Lime and Ceteris Paribus profiles were used to show dependencies and draw conclusions. The drawback of those methods is that for each observation results can differ greatly. That is why they cannot be used to assume general ideas for a whole data set. Therefore below we present only the most interesting observations found during the research. They were grouped into pairs to show how identical parameters, but different brands can lead to totally different prices or vice versa. First example Figure 2.6: First example Figure 2.7: Breakdown profile for observation 20 Figure 2.8: Breakdown profile for observation 246 Shown above two observations (2.6) vary three features - ram_gb, brand and diag. It is visible on these two Breakdown charts (2.7, 2.8) that Samsung has a bigger diagonal, but less RAM GB and according to the model is more expensive by 200. However, this difference in reality is higher, Samsung is more expensive by 700. There are also differences in the impact of features - in first batter_mAh has a positive impact and in the second negative. In the first case for model the most important were ram_gb, diag and brand (in this order), in second ram_gb, brand and front_camera_px. The question is, whether in reality brand does not have bigger impact on price than shown here? Second example Figure 2.9: Second example Figure 2.10: A plot of results of LIME method for 40 observation Figure 2.11: A plot of results of LIME method for 319 observation On LIME plots (2.10, 2.11) two phones, which have similar values in many features (2.9), in two (battery_mAh and diag) second phone (2.11) has better values than first one (2.10). Even though the price of the first phone is three times higher according to our model. The only difference not mentioned above between them is a brand - the first one is iPhone. That seems to be a conclusion consistent with reality. Third example Figure 2.12: Third example Figure 2.13: Ceteris Paribus profile Ceteris Paribus profile (2.13) shows different influence of some features concerning two mobile phones (2.12). The biggest contrast we can observe in case of battery_mAh, which lowers the price significantly in case of OPPO phone, and increases when it comes to Apple one, leading to the same prediction for both if the value exceeds 5000 mAh. It is quite surprising because in case of the first one such battery parameters should lead to a bigger price. Another difference which can be observed in front_camera_mpix influence - whereas above ~ 15 Mpix we reach similar price, for smaller values it causes prediction’s increase for Apple, and steady value for OPPO (for both peaks around 10 Mpix value). Once more those impacts are unexpected because OPPO phone has a much better front camera. Global explainations In the second step of our XAI analysis, we focused on dataset level explanations - analysis of all predictions together and how each feature affects their average value. We use Feature Importance and Partial Dependence Profile to show dependencies and draw conclusions. The advantage of those methods, in comparison to local profiles, is the fact that those explanations are true in general for our data; whereas for each observation results can differ greatly. Figure 2.14: Feature importance Feature importance plot (2.14) presents which variable has the most significant impact on prediction result. According to this plot, the most important variables for the ranger model were ram_gb, flash_gb and brand. Two of them are memory parameters which are arguably the most important phone’s technical parameters. The third variable describes the phone’s brand name, which is not a technical parameter. Figure 2.15: Partial Dependence Profile (PDP) for numeric variables Partial Dependence Profile (2.15) presents global dependencies between variables and target. The plot confirms observation from the previous plot, presenting a strong dependency between memory parameters and price. On the front camera mpix plot, there is an unnatural behaviour near 10-12 mpx, suggesting that phones with these specific parameters are the most expensive. After some research, it appeared that phones with these parameters are mostly made by expensive brands like Samsung and Apple. This leads to the conclusion, that in this case, the brand name was the variable that impacted price, not front camera mpix. Figure 2.16: Partial Dependence Profile (PDP) for brand variable Partial Dependence Profile (2.16) looks slightly different for brand variable because it is a character variable. Surprisingly, this plot presents a weak brand name impact on price, unlike previous plots. Brands that increase the price are Apple, Archos, and CAT, but only one of those brands (Apple) is a big phone company. 2.1.6 Summary and conclusion To summarize, according to all explanations shown above, there are several conclusions, which can be drawn. The biggest impact on the predicted price for the model had brand, ram_gb and flash_gb. The most expensive brands as Samsung and Apple have biased prices, they are higher than predicted by the model. What is important to highlight is that these conclusions were made for this specific data set, which had only eleven features at the beginning. This may mean, that for bigger data set, with more features, the results could be slightly different. Such sets can be the subject of further research. References Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016a). \"Why should I trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, KDD san francisco, CA (pp. 1135–1144). New York, NY: Association for Computing Machinery. "],["xai1-explainable-german-credits.html", "2.2 Classifying people as good or bad credit risks", " 2.2 Classifying people as good or bad credit risks Authors: Paweł Fijałkowski, Paulina Jaszczuk, Jakub Szypuła (Warsaw University of Technology) 2.2.1 Introduction In 1980s first ever PC was introduced to the public. Since then, rapid technological progress in computer-science lead to enormous demand for implementing and developing previously theoretical studies to suit capable of remembering and performing extensive calculations machines. By combining academic effort with pragmatical, engineer approach, humanity succeeded in building complex infrastructure that is able to solve pretty much any problem we have ever faced. We found ourselves in the point, where machine-learning algorithms make sensitive and in many cases opinion-based decisions, based on numerical and statistical analysis. It may seem harmless on the first glance, but do we really “trust” these models when it comes to predicting lung cancer? Or granting us a credit? In many cases, we do not and we should not. This is where XAI or Explainable Artificial Intelligence kicks in. It is a branch of AI, which results can be clearly understand and interpreted by experts in a given field (in order to e.g. comply with regulations or improve model user experience). XAI, using multiple complementary tools, turns black-box (meaning, very hard to interpret and understand even by specialist) ML models into white-box predictable solutions. Why is it important? Mainly for two reasons. First one is, we feel safer knowing what is going under the hood. We can’t simply “believe” outputted answer without digging into specifics on what grounds it was constructed. It gives us sense of security and provides a way of detecting possible miscalculations. Second reason is, these models, due to their mathematical construction (often minimizing value of loss function) in many cases proved to tend towards biases. This cannot be accepted while building systems supporting banks or hospitals. In the article below, we are facing the problem of creating explainable classifier for German bank credit score. We will build black-box model and then employ XAI methods in order for us to understand which features and statistics tend to outbalance model’s decision towards good/bad borrower classification. 2.2.2 Dataset and models During entire analysis, we’ll be operating on publicly available data set containing information about German borrowers. Input was processed/collected by Professor Dr. Hans Hofmann from University of Hamburg and is available here. Data set provider, basing on academic research, have chosen certain variables to describe each borrower. That combination of numerical and qualitative variables proved to carry certain importance/influence on probability of paying back borrowed assets. Brief overview of statistics describing an observation (columns in data set): Statistic Explanation checking_status Status of existing checking account (qualitative) duration Credit duration in months (numerical) credit_history Credit history (qualitative) purpose Purpose (qualitative) credit_amount Credit amount (numerical) savings_status Savings account/bonds (qualitative) employment Present employment since (qualitative) installment_commitment Installment rate in percentage of disposable income (numerical) personal_status Personal status and sex (qualitative) other_parties Other debtors / guarantors (qualitative) residence_since Present residence since (numerical) property_magnitude Property ownership (qualitative) age Age in years (numerical) other_payment_plans Other installment plans (qualitative) housing Current housing deal (qualitative) existing_credits Number of existing credits (numerical) job Job type quality (qualitative) num_dependents Number of people being liable to provide maintenance for (numerical) own_telephone Ownership of telephone (qualitative) foreign_worker Worker from abroad (qualitative) Author also suggest using cost function that “punishes” model more for false-positives (classification of bad customer as a good one) than for false-negatives. For a model user (i.e bank-owner), it is much worse to classify bad customer as a good one, than the other way around. After careful deliberation and testing using multiple metrics the team decided to use random forest as a baseline model for further modeling and explanations. Random forest is a supervised machine learning algorithm, basing on a simple idea of decision tree. Each decision tree is making a classification prediction and then their votes are counted/merged. Only difference between standard forest voting and random forest is that in the latter case, only random subset of features is considered during selecting most important feature (with lowest gini index value). Model performed exceptionally good during cross-validation, even without fine parameter hyper-tuning. 2.2.3 Local explanations To better understand the behavior of Random Forest, we employed a set of machine learning explanation methods. In order to comprehend the reasoning behind model predictions and interactions between variables we began at the local level, i.e. single observation. This way, we could compare influence of each variable within a clearly defined context of every potential borrower. Our primary motivation in selecting observations to present in this paper was to show interesting cases and how variables interact with one another in our model in creating predictions. As a result we selected various observations. Methods employed by us were Break-down, SHAP, LIME and Ceteris Paribus. 2.2.3.1 Initial analysis We began by analyzing the output of the Break-down method on two random observations. Once compared side-by-side (see the figure) several key differences emerge. First of all, the importance of checking status variable (which will return in forthcoming sections). One can also notice the importance of the “age” variable, which seems intuitive. A further conclusion can be drawn from this - influence of a variable depends on its value. In fact, depending on interactions between variables in the data set even the same value can have opposite effects. An example (although miniscule) can be found by comparing observations 4 and 7 (both classified correctly). The same value of “job” variable (i.e. “skilled”) gives opposite influences (-1.7% “bad” probability and +1.4% respectively). Thus we should always take into account context in which each variable is evaluated. Regarding specific conjectures on the data set one can draw from this comparison, we can see that both observations share 7 out of the top 10 most influential variables. Among them are, for example age, checking status, which would be expected by a “common sense” intuition. Influences confirm that intuition. Young age contributes negatively towards credit rating and lower duration positively. Same for checking status - the higher the better. Figure 2.17: Comparison of plots of breakdown method for two observations - number 1 (left) and number 94 (right). Positive values indicate an increase in probability of being classified as a “good” borrower while negative values indicate the opposite. Figure 2.18: Comparison of plots of breakdown method for two observations - number 4 (left) and number 7 (right). The same value of job variable has a different effect on the prediction. 2.2.3.2 In-depth analysis Consequently, we used SHAP method to analyze the first observation and compare results with the Break-Down method. The results of this can be seen in the figure below. Duration and checking status remain the two most important variables, while age grows in importance. 8 of the top 10 variables remain the same, with around the same influences. The major difference being absence of information on existing credits and residence since, have been replaced with credit amount and purpose. This method offers new information, primarily about the variation of influence of each variable. One can notice that age is the most varied (since it can affect the prediction both positively and negatively), which again would correspond to the “common sense” intuition (as with duration, the credit amount and checking status). The opposite occurs with credit history, purpose, property magnitude and length of employment, where variation remains lower compared to influence. Changes, in order of importance are rather small, only the installment commitment moves by more than 3 places (from number 5 to number 10). The rest is swapping of checking status and duration, fall of property magnitude by 2 places, employment by 1 place and rise of age by 2 places. It should be noted that all these changes occur similar or very similar absolute influence values in break-down method. One can judge that importance of variables is a trait that can remain consistent between various explanation methods. Figure 2.19: A plot of results of SHAP method for 1st observation 2.2.3.3 The most certain predictions Our model classifies clients in binary terms as good and bad potential borrowers but the certainty of its predictions is not always the same. We decided to analyze the cases in which it has no doubts which group to assign a given client to. For this purpose, we used the Cateris Paribus method which checks how the prediction for a given observation would change for different values of one variable while the others remain constant. Figure 2.20: A plot of results of CP method for 21st observation Looking at the Cateris Paribus plot for an observation that the model classified as a good borrower with almost 97% certainty, we can draw several conclusions. First, the variables were divided into those important and less important for the model prediction. ‘Duration,’ ‘credit amount,’ and ‘age’ would clearly change the result if they had a different value - for example, a few thousand higher credit amount would significantly reduce the prediction, as would the duration value. On the other hand we see plots of ‘installment commitment,’ ‘residence since,’ ‘existing credits’ and ‘num dependents’ which are almost straight lines - changing their values would hardly affect the model’s predictions. This allows us to conclude that in this case it was the values of these three variables (listed first) that mainly determined the classification of the borrower. The explanations also once again confirm our intuition - the high certainty of the model as to the classification of the customer as a good borrower was motivated by logical arguments - a relatively low loan amount, short loan duration and the average age of the borrower. Figure 2.21: A plot of results of CP method for 64st observation Cateris Paribus charts for an observation confidently classified as a bad borrower are almost the opposite of the previous ones. This time the client is very young, he wants to take a high loan with a long repayment period. All these features have a negative effect on prediction. Interestingly, if only a few years older client applied for the same loan, the model’s prediction would be much higher. Once again, ‘installment commitment,’ ‘existing credits’ and ‘num dependents’ features do not seem to be important in the classification. However, the variable ‘residence since’ is different, as if it had assumed the value 1, it would significantly increase the prediction. 2.2.3.4 Incorrect predictions Although the accuracy of our model is at satisfactory level, sometimes it is providing incorrect predictions. We decided to take a closer look and analyze the situations when our model misclassifies. For this purpose we explained the false positives and false negatives predictions with the local methods LIME and SHAP. 2.2.3.4.1 False positive (#fig:2-2-lime_false_pos)A plot of results of LIME method for 56th observation which was missclasified by our model and predicted as false positive The model classified the observation positively mainly on the basis of the features duration (12 months) and savings status (no known savings). This is in line with our earlier conclusions - a short loan duration has a positive effect on prediction. To better understand the operation of the model in this case, we also made an explanation using the SHAP method. (#fig:2-2-shap_false_pos)A plot of results of SHAP method for 56th observation which was missclasified by our model and predicted as false positive The explanations agree on the influence of feature savings status but according to the SHAP method, duration has a marginal positive effect on prediction. Interestingly, both methods give checking status as the most influential feature. Its value (in the range from 0 to 200) negatively affects the prediction which is how the model should classify this observation but this is outweighed by the positive effects of other features. 2.2.3.4.2 False negative (#fig:2-2-lime_false_neg)A plot of results of LIME method for 945th observation which was missclasified by our model and predicted as false negative. The model’s prediction was not certain, but it indicated that the client is a bad borrower with a probability of 65%. The biggest negative contributions are related to checking status (in the range from 0 to 200), credit history (no credits/all paid) and duration (48 months). While the relatively long loan term may indeed have a negative effect on the prediction, the fact that there are no other loans seems to be contradictory. The LIME method did not answer all our questions. To dispel any doubts we also explained the observations using the SHAP method. (#fig:2-2-shap_false_neg)A plot of results of SHAP method for 945th observation which was missclasified by our model and predicted as false negative. SHAP’s explanations are very similar to those LIME’s ones. Again, features duration, checking status and credit hisotry have the greatest influence on negative prediction. Perhaps the strange contribution of the feature credit history is due to its interaction with other variables. 2.2.4 Global explanations A major shortcoming of local explanations is that they are just that - local. It is the ability to see “the bigger picture” that allows us to achieve a better understanding of the model. Thus, we resolved to using Global Explanation methods in order to properly grasp approach behind predictions in our machine learning model. 2.2.4.1 Feature Importance First of all, we began with Feature Importance method which gives information on the importance of features in the model. The results of applying this method on our model can be seen in the figure below. (#fig:2-2-feature_importance)A plot of Feature Importance values for each variable for our model. As one can notice, results are in line with local explanations. That is, the domination of such features as checking status, credit duration and credit amount. Then follows age, saving status and purpose. Yet again, our “common sense” intuition seems to be confirmed. Noteworthy, the plot resembles an exponential function, hinting that predictions are based mostly on the first few most important features, a trait not so obvious to observe in local explanations. It should be noted, however, that the low impact on the prediction of some variables may be dictated by a small number of observations - e.g. the number of non-foreign workers in our data set is just under 4%. Surprisingly, the explanation showed that some variables that had a significant influence on the prediction in local explanations, had a very low global impact on the model, e.g. installment commitment (installment rate in percentage of disposable income), which in both incorrectly classified observations was in the lead in terms of importance according to SHAP explanation but globally its impact was marginal. We can also notice the opposite situation - a globally important variable has almost no impact on the prediction of a specific observation - an example may be the age variable, which, according to feature importance, has the fourth largest impact on the prediction while in the analyzed by us incorrectly classified observations it is quite the opposite. If we assume that the model is correct, then we can also draw conclusions regarding the “real life” importance of features. Those results should still be compared with other Global Explanation methods. 2.2.4.2 Dependence of variables The feature importance method seems to be very helpful in exploring the model, but it also has some drawbacks. One is that it does not perform well on correlated variables - permutating only one dependent variable while the other remains constant is not the best solution. In the case of our data set this problem also occurs - for example, the duration of the loan is strongly correlated with its amount. For this reason, we decided to use the triplot package which enables the analysis of grouped variables. We divided the features of our data into 3 categories - those relating to the loan (e.g. credit amount and duration), the client’s account (e.g. checking and savings status) and the borrower himsefl (e.g. age and employment). Then we explained the contribution of the whole groups of variables. Figure 2.22: triplot for grouped variables. As we can see, although according to Feature Importance the most important variable for our model is checking status (rating group), the total prediction is influenced most by the characteristics of the loan itself while the variables connected with the current account have the most marginal effect on the model 2.2.4.3 Global impact of specific variables In order to better understand the influence of specific variables on the model prediction in relation to the global reference, we decided to perform the PDP explanations. This method works similarly to Cateris Paribus - it shows the preduction of the model for different values of the variable while maintaining other features of the model constants. We explained the four features that turned out to be the most significant for our model according to Feature Importance (checking status, duration, credit amount, age). 2.2.4.3.1 Checking status Figure 2.23: PDP plot for checking status variable. As we can see, a customer with a negative checking status has the lowest probability of being a good borrower according to the model and the best borrowers turn out to be those without a current account. It is somewhat surprising that the slightly higher prediction applies to customers with a lower positive checking status than those with a higher one. This may be due to the small number of clients with high checking status - just over 6% of all while other values are evenly matched. 2.2.4.3.2 Duration Figure 2.24: PDP plot for duration variable. The variable duration plot leads us to similar conclusions as for the local explanations. The longer the loan term, the greater the credit risk. For the initial values the prediction of a good borrower decreases almost linearly (the jagged graph is the result of using the random forest model), to a certain threshold of around 40 months when the loan duration is already so long that this variable does not have a significant impact on the prediction. 2.2.4.3.3 Credit amount Figure 2.25: PDP plot for credit amount variable. The plot of credit amount is somewhat similar to that of duration. As the loan amount increases, the probability that the customer will turn out to be a good borrower decreases until the loan is so high (around 12,000) that increasing it no longer has a significant impact on the prediction. The initial low value of the prediction and the increase for the loan amount from 0 to 4000 may be due to the fact that clients taking loans for small amounts may have problems with financial liquidity. 2.2.4.3.4 Age Figure 2.26: PDP plot for age variable. The age plot is consistent with our earlier assumptions and intuition - young borrowers who do not yet have a stable position on the labor market and their income is uncertain are the clients for whom credit risk is the highest. Subsequently, the prediction increases to peak for an approximately 38-year-old borrower. Statistically, people of this age have a stable job with high earnings. Thereafter, the prediction remains steadily high until around 55 years after which it begins to decline - this corresponds to the period when borrowers begin to retire which causes their incomes to decline frequently. 2.2.5 Summary and conclusions Applying XAI methods to our black-box model helped understand it, turning it, even if by a minimal margin, into more of a glass-box model. We found the most important features (that is checking status, duration, credit amount and age) and the rules governing their influence on predictions. We also confirmed the validity of a “common sense” intuition, e.g. the probability of being a “good” borrower being lower for very young and very old people. This allows us not only to validate the performance of our model, but also to better understand the rules governing this dataset. We can confidently say, that the main aim of our work has been achieved. The model has been explained and various explanations remained stable between methods used. Assuming our methodology was correct, it would mean that it is an adequate explanation that works not only within the model, but also within the data. With real-life data this can approach could prove useful in finding dependencies and interactions with phenomena, one would not connect with machine learning on first instinct. However, this would require greater research, far exceeding scope of this article. "],["how-to-predict-the-probability-of-subsequent-blood-donations.html", "2.3 How to predict the probability of subsequent blood donations?", " 2.3 How to predict the probability of subsequent blood donations? Authors: Maciej Chylak, Mateusz Grzyb, Dawid Janus (Warsaw University of Technology) 2.3.1 Abstract Blood donated for transfusions saves millions of lives every year. Reliable prediction of a patient’s blood donation intention is valuable information for the medical community. This work focuses on studying machine learning predictive models for such tasks through methods of explainable artificial intelligence (XAI). We consider a model based on the Random Forests algorithm and a dataset containing information about patients’ blood donation history. Through prudent data pre-processing, model preparation, and the use of global and local explanations of the model, we exhibit how strongly and in what way various features affect the model’s prediction. During our analysis, we also uncover meaningful patterns hidden in the data itself. Among other conclusions, we show that a short time since the patient’s last donation is crucial for positive prediction and that donation burnout is a probable effect affecting donors. We believe that the presented results are valuable for this matter of life and death subject. 2.3.2 Introduction and motivation Interest in explainable artificial intelligence (XAI) has increased significantly in recent years. XAI facilitates humans to understand artificial intelligence solutions (Barredo Arrieta et al. 2020a). It contrasts with a concept of the black box, where even its designers cannot explain why an AI model has arrived at a specific conclusion. The intense development of such methods has led to the wide choice of XAI tools that we have today (Maksymiuk et al. 2021). It includes the R package DALEX (Biecek 2018a), which is the foundation of this work. Our goal is to prepare and explain a model designed to predict the probability of subsequent blood donations based on a history of a patient’s previous offerings. Careful use of XAI tools allows us to verify model correctness and discover phenomena affecting blood donations that hide in the data. Obtained knowledge may have various implications, especially better prediction of future blood supply and an improvement of planning and advertising of blood donation campaigns. 2.3.3 Related work The prediction of an intention of a blood donation seems to be an important issue, as there are multiple articles available that try to tackle it while making use of machine learning methods and often the same dataset that is also used during this work. A few of them are presented shortly below. In (Darwiche et al. 2010), the authors test Multilayer Perceptrons and Support Vector Machines in combination with Principal Component Analysis as a solution to the mentioned issue. Their models achieve good results, and some ways of further improvement are proposed. In (Bahel et al. 2017), the authors use unsupervised machine learning techniques to cluster the data before building predictive models and check whether the performance is significantly improved when using such an approach. They found out that a Support Vector Machines model based on data clustered into four distinct groups with the K-Means algorithm achieved the best sensitivity of 98.4 %. In (M. Barhoom et al. 2019), the authors study models created in the JustNN environment. They propose a model with an outstanding test subset accuracy of 99.31 %. They identify the Recency, Time, and Frequency variables (described precisely in the following section of this article) to be the most important factors. In (Alkahtani and Jilani 2019), the authors use a different dataset but of a partly similar structure. They note that some religious periods like Fasting and Performing Hajj may negatively affect the number of donations. It is important to say that none of the articles we found include explainable artificial intelligence methods. Therefore, the presented approach is novel and may potentially lead to a perspective undiscovered before. 2.3.4 Data and model 2.3.4.1 Original dataset The data, which all the prepared models are based upon, comes from the Blood Transfusion Service Center Data Set, which is available through OpenML website (Vanschoren et al. 2013). The dataset consists of 748 observations (representing individual patients) described by 5 attributes: recency - months since last blood donation, frequency - total number of blood donations, monetary - total amount of donated blood in c.c., time - months since first blood donation, donated - a binary variable representing whether she/he donated blood in March 2007. 2.3.4.2 Data analysis Initial data analysis is a critical process allowing to discover patterns and check assumptions about the data. It is performed with the help of summary statistics and graphical data representations. The short data analysis below is based on two visualizations representing distributions and correlations of variables. Figure 2.27: Distributions of explanatory variables (histogram) Based on the above figure 2.27, an important insight can be made - distributions of Frequency and Monetary variables are identical (excluding support). It probably comes from the fact that during every donation the same amount of blood is drawn. The presence of both of these variables in the final model is pointless. Figure 2.28: Correlations of explanatory variables (correlation matrix) The above figure 2.28 represents correlations of explanatory variables measured using robust Spearman’s rank correlation coefficient. Apart from the already clear perfect correlation of Monetary and Frequency variables, a strong correlation of Time and Monetary/Frequency variables is visible. It probably comes from the fact that the minimal interval between subsequent donations is strictly controlled. Such dependence can negatively affect model performance and the accuracy of explanations. This potential problem is addressed during the pre-processing of used data. 2.3.4.3 Pre-processing A simple data pre-processing is conducted, mainly to reduce detected correlations of explanatory variables. Firstly, the variable Monetary is removed from the dataset. The information it carries duplicates information contained in the Frequency variable. Secondly, a derived variable is introduced instead of the Time variable. It is called Intensity and is calculated as follows: \\[\\textrm{Intensity} = \\frac{\\textrm{Frequency}}{\\textrm{Time}-\\textrm{Recency}}\\] The above equation results in values between around 0.031 and 1. The denominator can be interpreted as a time window bounding all the known donations of a given patient. Spearman’s rank correlation coefficient of the new variable Intensity and the old variable Frequency is -0.46, which is better compared to the previous 0.72 value for the Time and Frequency combination. 2.3.4.4 Final model According to the OpenML website, Ranger implementation of Random Forests (Wright and Ziegler 2017a) is among the best performing classifiers for the considered task. All the tested models utilize this machine learning algorithm. Performance of the models is assessed through three measures - basic classification accuracy and more complex area under the Receiver Operating Characteristic curve (ROC AUC) (Bradley 1997) (Fawcett 2006) and area under the Precision-Recall curve (PR AUC) (Raghavan et al. 1989) (Davis and Goadrich 2006). PR AUC is an especially adequate measure for unbalanced classification problems (Saito and Rehmsmeier 2015), which is the case here. Based on the described measures, the best model is chosen from the models trained on the following explanatory variables subsets: Recency, Time Recency, Frequency, Recency, Frequency, Time Recency, Frequency, Intensity The first two models, utilizing only two explanatory variables, perform significantly worse. Out of the last two models, the model utilizing the Time variable is slightly worse than the model utilizing the introduced Intensity variable. The accuracy of the last model is 0.85, other performance measures describing it are presented graphically below. Figure 2.29: ROC curve and corresponding AUC for the final model ROC curve visible in the above figure 2.29 represents good model performance and the AUC value of almost 0.92 is satisfactory. Figure 2.30: PR curve and corresponding AUC for the final model The baseline for the ROC AUC is always 0.5, but it is not the case for the PR AUC. Here, the baseline AUC is equal to the proportion of positive observations in the data. In our case it is \\(\\frac{178}{748}=0.238\\). Due to the above, the PR AUC value of around 0.81 visible in the figure 2.30 is also proof of high model precision. Summarizing the above model selection, the final model used for all the presented explanations is Ranger implementation of Random Forests utilizing Recency, Frequency, and Intensity variables. Its performance measures are at least good, so the prepared explanations have a chance to be accurate. 2.3.5 Global explanations Global explanations are used to discover how individual explanatory variables affect an average model prediction on the level of the whole dataset. Here, one can learn how strong is the influence of a given variable or how its particular values shape the overall model response. In the case of large numbers of observations, global explanations can be prepared using only a representative fraction of them to save calculations time. Dataset presented in this work can be considered of modest size, and therefore all observations are always used. 2.3.5.1 Permutation Feature Importance Permutation Feature Importance (A. Fisher et al. 2018) is a method of assessing how strongly a given variable affects the overall model prediction. The idea behind it is to measure a change in the model’s performance when it is not allowed to utilize a selected feature. Removal of variables from the model is realized through multiple random permutations of them. Figure 2.31: Permutation Feature Importance Profile Figure 2.31 shows, all explanatory variables have significant importance in the model under study. The most important feature is the Recency (highest loss after the permutations) followed by the derived variable Intensity and the Frequency in the last. The significance of the Recency variable is also well visible later on, especially when local explanations are considered. 2.3.5.2 Partial Dependence Profile Partial Dependence Profile (Friedman 2000b) (Greenwell 2017) allows users to see how the expected model prediction behaves as a function of a given explanatory variable. It is realized through averaging (basic arithmetic mean) of multiple Ceteris-Paribus Profiles, a method described shortly in the Local Explanations section. Figure 2.32: Partial Dependence Profile First of all, curves presented in the figure above 2.32 are visibly ragged. It is mostly caused by the selection of the machine learning algorithm - Random Forests contain many if-then rules that can change the final prediction value rapidly, even under small explanatory variable change. Therefore, it is best to look at the overall shape of yielded curves and ignore fine uneven details. Visible shapes are largely in line with the intuition - higher Frequency and Intensity values lead to higher average prediction (probability of subsequent blood donation to recall), whereas the impact of Recency is inverse - the lower the value, the higher the prediction. However, some parts of the curves need further investigation - the tails. For the Frequency variable, the profile tail simply flattens, whereas for the other two variables there is a significant change in the mean prediction value. The explanation for this in the case of the Frequency and Recency variables is simple - an insufficient amount of data. There are only eight observations with \\(\\textrm{Frequency} &gt; 30\\) and only nine observations with \\(\\textrm{Recency} &gt; 24\\). It is a worthy reminder that XAI methods can be only as good as the data and the model themselves. Interestingly enough, the case of the sudden drop in the mean prediction as a function of the Intensity variable is much different. It is a genuine pattern contained in the data itself, that the model has learned to take an advantage of. The following table aids greatly in interpreting these results: Observations with Intensity &gt; 0.55 (group A) Observations with Intensity &lt; 0.55 (group B) Total number of observations 241 507 Number of positive cases 42 136 In group A there is 17 % of positive observations, and in group B there is 27 %. Using Fisher’s exact test (R. A. Fisher 1922) it can be shown that the difference in proportions between mentioned groups is statistically significant (p-value 0.0028). A probable hypothesis is that above a certain Intensity threshold there is an increased chance of a patient’s burnout caused by too frequent donations. 2.3.5.3 Accumulated Local Effect Profile The method presented in the previous subsection - Partial Dependence Profile, can be misleading if explanatory variables are strongly correlated, which to some extent is still the case here, even after the pre-processing. Accumulated Local Effect Profile (D. Apley 2018) is another method used to explain a variable’s effect on the prediction, which is designed to address the mentioned issue. Figure 2.33: Accumulated Local Effect Profile Although this method addresses the very problem the used data has, no significant difference is visible in the figure 2.33 when compared to the previous figure 2.32. It only reassures that the conclusions presented in the previous subsection are correct. 2.3.6 Local explanations Local explanations are relevant on a level of a single observation. They aid in understanding how strongly and in what way explanatory variables contribute to the model’s prediction when considering a single instance of data. It is a visible contrast when compared to the previously presented global explanations. 2.3.6.1 Ceteris Paribus Profiles “Ceteris Paribus” is a Latin phrase that translates to “other things held constant” or “all else unchanged.” The idea behind this method is rather simple - for every variable, prediction is plotted as a function of its changing value while all other variables are held constant. Primary variables’ values come from a previously selected single observation and are marked with dots. This tool is also known as Individual Conditional Expectations (Goldstein et al. 2015). Figure 2.34: Ceteris Paribus Profile for observation number 342 In the figure above 2.34, Ceteris Paribus Profile for observation number 342 is presented. This patient is characterized by high Frequency and Intensity values (which has already been shown positive in terms of the model’s prediction), but also a high Recency value (which on the contrary is considered a negative trait). It is worth noting that the model’s prediction for this observation is correct (she/he did not donate). As the rightmost plot shows, the model’s prediction could be improved significantly only by lowering the Recency value. It is coincident with common sense - if someone was, at a time, an active donor but has not donated for almost two years, the chance of another donation seems low. Also, this local explanation complements the previously presented Permutation Feature Importance well - explanatory variable Recency is the most important one because it can negate the effect of positive values of both the other variables. Figure 2.35: Ceteris Paribus Profile for observation number 16 In the figure above 2.35, Ceteris Paribus Profile for observation number 16 is presented. This patient is characterized by a moderate Frequency value, a high Intensity value and a low Recency value. It is worth noting that the model’s prediction for this observation is correct (she/he did donate). What is interesting here is that the Intensity variable value is nearly perfect. Value even a little higher would raise the risk of the hypothetical donation burnout and lower the predicted donation probability. It is convincing proof that the described before patterns hidden in the data are expressed in the model as well and reveal themselves on a level of single observations. 2.3.6.2 Break Down Profiles The last method presented aims to answer probably the most elementary question that can be asked when trying to explain the model’s behavior - what is the contribution of particular variables to the prediction for a single observation? Break Down Profile (Staniak and Biecek 2018b) does it by evaluating changes in the mean model’s prediction when the values of consecutive variables are being fixed. Figure 2.36: Break Down Profile for observation number 4 In the figure above 2.36, Break Down Profile for observation number 4 is presented. The patient’s characteristics are good and well balanced - twenty completed donations, only two months since the last donation, and an Intensity value of around 0.47, which is high but, as it has been shown already, not too high. It is worth noting that the model’s prediction for this observation is correct (she/he did donate). The plot shows, the contributions of variables are all positive and well balanced as the values themselves. The result is not surprising but creates a good reference point for the following explanation. Figure 2.37: Break Down Profile for observation number 109. In the figure above 2.37, Break Down Profile for observation number 109 is presented. This patient is chosen deliberately because of the same low Recency value as the previous example but much lower Intensity and Frequency values. It is worth noting that the model’s prediction for this observation is correct (she/he did not donate). A Recency of two months still has a similar positive impact, but a mediocre Intensity value and a low Frequency change the prediction dramatically. When compared to the previous explanation, this plot is a perfect example of how important for positive prediction is that all variables’ values are well balanced. Previously (first Ceteris Paribus profile), it was visible that a high Recency value can negate the effect of positive values of the other two variables, but here it becomes clear that a low Recency value cannot make up for unfavorable values of the other two variables. 2.3.7 Conclusions and summary After data analysis and pre-processing, model preparation and selection, preparation and investigation of global and local model explanations, the short answer to the title question - “How to predict the probability of subsequent blood donations?” - is as follows: The most important factor is whether the patient under consideration is active - when her/his last donation took place more than six months ago, the subsequent donation probability falls drastically, and other factors lose their importance. But when the Recency is low, other factors become decisive. A bigger number of past donations is almost always considered positive for the prediction, and higher values of the proposed Intensity feature are favorable only until a certain threshold (around 0.55) when a phenomenon of burnout probably becomes relevant. We believe that the purpose of our work has been fulfilled. We took care to prepare a precise model based on meaningful data to limit the uncertainty of the obtained model explanations. The same conclusions came up multiple times during the analysis of different explanations, which reassures their correctness. Some of the discovered phenomena are visible not only in the explanations but also in the data itself. Over the years, XAI has become a powerful tool that can produce numerous meaningful insights about the considered model and the data itself. With that said, it is best used with caution, as any statistical tool can yield false conclusions when its assumptions are not carefully checked. References Alkahtani, A. S., &amp; Jilani, M. (2019). Predicting return donor and analyzing blood donation time series using data mining techniques. International Journal of Advanced Computer Science and Applications, 10(8). https://doi.org/10.14569/IJACSA.2019.0100816 Apley, D. (2018). ALEPlot: Accumulated Local Effects (ALE) Plots and Partial Dependence (PD) Plots. https://CRAN.R-project.org/package=ALEPlot Bahel, D., Ghosh, P., Sarkar, A., Lanham, M. A., &amp; Lafayette, W. (2017). Predicting blood donations using machine learning techniques. http://matthewalanham.com/Students/2017_MWDSI_Final_Bahel.pdf Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020a). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Biecek, P. (2018a). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. http://jmlr.org/papers/v19/18-416.html Bradley, A. P. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recogn., 30(7), 1145–1159. https://doi.org/10.1016/S0031-3203(96)00142-2 Darwiche, M., Feuilloy, M., Bousaleh, G., &amp; Schang, D. (2010). Prediction of blood transfusion donation, 51–56. https://doi.org/10.1109/RCIS.2010.5507363 Davis, J., &amp; Goadrich, M. (2006). The relationship between precision-recall and ROC curves. In Proceedings of the 23rd international conference on machine learning (pp. 233–240). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1143844.1143874 Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/https://doi.org/10.1016/j.patrec.2005.10.010 Fisher, A., Rudin, C., &amp; Dominici, F. (2018). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. arXiv. https://arxiv.org/abs/1801.01489 Fisher, R. A. (1922). On the interpretation of χ2 from contingency tables, and the calculation of p. Journal of the Royal Statistical Society, 85(1), 87–94. http://www.jstor.org/stable/2340521 Friedman, J. H. (2000b). Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics, 29, 1189–1232. https://doi.org/10.1214/aos/1013203451 Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2015). Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation. Journal of Computational and Graphical Statistics, 24(1), 44–65. https://doi.org/10.1080/10618600.2014.907095 Greenwell, B. M. (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421–436. http://doi.org/10.32614/RJ-2017-016 M. Barhoom, A., Abu-Naser, S., Abu-Nasser, B., Alajrami, E., Musleh, M., &amp; Khalil, A. (2019). Blood donation prediction using artificial neural network, 1–7. https://philarchive.org/archive/BARBDP-14 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2021). Landscape of r packages for eXplainable artificial intelligence. https://arxiv.org/abs/2009.13248 Raghavan, V., Bollmann, P., &amp; Jung, G. S. (1989). A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3), 205–229. https://doi.org/10.1145/65943.65945 Saito, T., &amp; Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. Plos One, 10(3). https://doi.org/10.1371/journal.pone.0118432 Staniak, M., &amp; Biecek, P. (2018b). Explanations of Model Predictions with live and breakDown Packages. The R Journal, 10(2), 395–409. https://doi.org/10.32614/RJ-2018-072 Vanschoren, J., Rijn, J. N. van, Bischl, B., &amp; Torgo, L. (2013). OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2), 49–60. https://doi.org/10.1145/2641190.2641198 Wright, M. N., &amp; Ziegler, A. (2017a). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 "],["explaining-diabetes-indicators.html", "2.4 Explaining diabetes indicators", " 2.4 Explaining diabetes indicators Authors: Martyna Majchrzak, Jakub Jung, Karol Niewiadomski (Warsaw University of Techcnology) 2.4.1 Abstract Machine Learning is nowadays widely used in many more or less scientific fields. Some interesting ones include economy, social networking and medicine. Sometimes however we not only need a well performing model but also an understable one. The Explainable Atrificial Intelligence (XAI) is still pretty new concept and has a lot of potential for its use. In this paper we harness XAI methods for a real world medicine problem. We take a look at the diabetic data, build a decent model based on it and try to understand what conclusions the model has reached. As a result of our experiment we will try to understand what factors contribute to higher chance of diabetes occurence. 2.4.2 Keywords Machine Learning, Explainable Artificial Intelligence, XAI, Classification, Diabetes 2.4.3 Introduction With the rise in popularity of Machine Learning algorithms people started questioning the solutions obtained by computers’ calculations. As an answer a whole new branch of AI has been formed - Explainable Artificial Intelligence (XAI). With its methods we analyzed the model made for the specific problem. Our task was to find correlation between different biomarkers and positive diagnosis of diabetes based on the data of over 700 women of indian ancestry (Pima tribe). In order to do that, we used XAI tools on our black box models to present the decision making process of algorithms in understandable for humans way from which we could later draw conclusions. 2.4.4 Methods 2.4.4.1 Dataset Our dataset consists of 768 observations of 8 features being different biomarkers and 1 target variable (class) saying if that woman was diagnosed with diabetes. preg : number of pregnancies plas : plasma glucose concentration (mg/dL) at 2 hours in an OGTT (oral glucose tolerance test) - a test in which subject is given glucose and blood samples are taken afterward to determine how quickly it is cleared from the blood pres : blood pressure (mm Hg) skin : triceps skinfold thickness (mm) measured at the back of the left arm. A measurement giving rough information about body fat percentage. insu : 2-hour serum insulin (mu U/ml) mass : BMI index (weight in kg/(height in meters)^2) pedi : diabetes pedigree function outcome, where DBF is a function that uses information from parents, grandparents, siblings, aunts and uncles, and first cousins and provides a measure of the expected genetic influence of affected and unaffected relatives on the subject’s eventual diabetes risk age : age (years) class (target): 1 if tested positive for diabetes, 0 otherwise Figure: Distributions of features. The dataset didn’t have any missing data, however after analysing the features and their meaning we found many 0 values that made no sense and weren’t physically possible (for example 0 value of mass which is BMI index - mass (in kilograms) divided by squared height (in meters)). We then went through the article for which this data was collected and found that it was possible that some tests had never been carried for some observations. This meant that the 0 values in our dataset (for features other than preg and class for which they absolutely make sense) were actually missing values. 2.4.4.2 Dataset versions During preprocessing and based on the observations of our fist models we created 4 different versions of dataset which we later used to train our models. First, the dataset was randomly divided into a training dataset (70% of data) and testing dataset(30% of data). Then the datasets were individually modificated in the following way: original - base dataset with no modifications naomit - base dataset excluding observations with any missing values mice - base dataset with all missing values imputed with mice (Multivariate Imputation by Chained Equations) package using PMM method. Mice is suitable for this dataset, since the values are missing at random. skip - base dataset excluding features skin, insu and pres with missing values in mass and plas imputed with mice package Dataset Rows Train Rows Test Columns original 537 231 9 naomit 372 160 9 mice 537 231 9 skip 537 231 6 2.4.4.3 Measures For the assessment of the model in our experiment we will use three commonly known measures: AUC - Area Under ROC Curve An ROC curve (receiver operating characteristic curve) is a graph that shows the performance of a classification model at all classification thresholds. This curve plots two parameters: TPR (True Positive Rate) and FPR (False Positive Rate), defined as: \\(TPR=\\frac{TP}{TP+FN}\\) \\(FPR=\\frac{FP}{FP+TN}\\) BAC - Balanced Accuracy This measure is a weighted accuracy, that is more suitable for datasets with imbalanced classes. FNR - False Negative Rate It corresponds to the number of cases where model incorrectly fails to indicate the presence of a condition when it is present, divided by the number of all. \\(FNR=\\frac{FN}{TP+FN}\\) It is particularly important for medical data models, because it indicates how many sick patients are going to be categorised as healthy, and therefor not get any treatment. 2.4.4.4 Models To identify the models, that could potentially be efficient classificators on this dataset, we measured the AUC, Balanced Accuracy and False Negative Rate measures on 6 different classification models from mlr package: Tree-based models: Random Forest - Random Forest Ranger - Random regression forest Boosting models: Ada - Adaptive Boosting GBM - Gradient Boosting Machine Other: Binomial - Binomial Regression Naive Bayes - Naive Bayes 2.4.5 Explanations In this section we will take time to investigate some model explanations that we will be using later. Feel free to use this chapter as reference for quick overview. For more in-depth knowledge please refer to paper Explanatory Model Analysis [1] by professor Biecek. //bookdown?? TODO All images in this section are also taken from this article. 2.4.5.1 Local Local explanations allow us to better understand model’s prediction from the level of single observations. This can be useful when we want to evaluate predictions for certain instances. For example, which values have the most importance for the particular observations, or how the change in variables would impact the result. Local explanations combined with professional expertise could sometimes hint towards potential problems with the model in case they contradict each other. 2.4.5.1.1 Break Down In this explanation all observations start from the same base that is the mean prediction for all data. Next steps consist of fixing consecutive variables on values from the observation in question and measuring the change in mean prediction that is now calculated with some of the values fixed. This shift in mean prediction is interpreted as the impact of this variable’s value on prediction for this observation. Figure TODO shows the corresponding steps. Figure: Panel A shows the distribution and mean prediction with values on the same level and above fixed on values from observation. Then change in mean is calculated and assigned to corresponding variables. A big drawback for this method is how the order of variable can influence the explanation outcome. This problem occurs most often as a result of existing interactions between variables as seen in OBRAZEK TODO Figure: Variables age and class interact with each other so the changing order results in different plots 2.4.5.1.2 Shap Shap is a direct response to the biggest problem of Break Down method that is the ordering chosen for explanation could alter the results as seen in figure TODO. Shap takes a number of permutation and calculates the average response of Break Down on these permutations. More permutations result in more stable explanations. Exemplary output is shown in figure TODO. Figure: Different results of Break Down for 10 random variable orders Figure: Shap output. Average impact calculated with Break Down for 10 permutations 2.4.5.1.3 Lime The essence of Lime is to locally approximate the black box model with a glass box one. Then we can use it for local explanations that should yield results appropriate for the black box. Firstly, we generate data “close” to our observation (also called Point of Interest) and make predictions with black box model. Based on those predictions we try to fit the glass box model that accurately predicts those observations. In result we receive a glass box model that should behave the same as original model as long as we remain close to the Point of interest. Figure TODO describes the idea. Figure: Colored areas correspond to different prediction for the black box model. Black cross is our Point of Interest and circles are generated data. The line represents our simpler model that approximates original model around Point of Interest This method is often used for datasets consisting of many variables. Methods like Break down and Shap come out short in these situations. 2.4.5.1.4 Ceteris Paribus Ceteris Paribus is a Latin phrase meaning “all else unchanged.” This accurately describes what it does. For the certain observation we take values that are interesting to us and observe how the prediction changes as we change values in those columns one at a time. This can yield interesting conclusions about change in values and its impact on predictions. Figures TODO and TODO show prediction change as response for variable change. Figure: Shape of lines resembles the nature of the model. Dot shows value and model’s prediction for the observation. Figure: Ceteris Paribus shows different importance in variables and their changes for different observations. Ceteris Paribus is very popular due to its simplicity and interpretability. The biggest drawback however is the possibility of unpredictable or misleading results. For example, with fixed variable age set to 18, predicting outcome for a significant number of pregnancies does not make sense. It is also hard to capture interactions when working with variables separately. 2.4.5.2 Global As the name implies in global explanations we look at variables from the perspective of whole dataset. We can check the importance of variable for the model or analyze average impact of certain values on predictions. 2.4.5.2.1 Feature Importance The main idea behind Feature Importance is measuring how important each variable is for model’s predictions. To do that we measure the change in AUC after permutating values of each variable. The bigger the change, the bigger importance of variable. To stabilize the results, we can measure average change for a number of permutations. In figure TODO we can see that variable gender is the most important for model’s prediction. Figure: Feature Importance output for 10 permutations. 2.4.5.2.2 Partial Dependence To put it short, Partial Dependence is the average of Ceteris Paribus for whole data. This results in average importance and impact of variable and its values. The similarities can be observed in figure TODO Figure: Multiple observations with Ceteris Paribus and their average as Partial Dependence. As we can see observations can return different shapes for Ceteris Paribus, hence averaging them could mean loss of information. To answer this problem Partial Dependence implements grouping and clustering that could show the difference as shown in figure TODO. Figure: Partial Dependence grouped by sex. As in Ceteris Paribus this method is very simple and understandable. However, it also carries over the same problems resulting from correlated variables. 2.4.5.2.3 Accumulated Dependence Also referred as Accumulated-Local Dependence. It is the direct answer for correlation issue in Partial Dependence. The construction of Accumulated Dependence is the same as in Ceteris Paribus. The only difference being how the observations are summarized. Partial Dependence uses marginal distribution while Accumulated Dependence uses conditional distribution. This means that when there are at most negligible correlations, Accumulated Dependence yields results very similar to Partial Dependence. 2.4.6 Results 2.4.6.1 Dataset versions comparison We compared the performance of the ranger model with default values on the testing datasets, using AUC, Balanced Accuracy and False Negative Rate measures. The naomit dataset has the best scores, however, using this dataset version may cause the loss of valuable information. Because of that, we are going to use original and skip versions for further analysis. 2.4.6.2 Model comparison The models were checked on both original and skip dataset version, with the following results: 2.4.6.2.1 Data Original The measure values between models are quite similar, however Ada has the best Balanced Accuracy and Binomial has the lowest False Negative Rate. 2.4.6.2.2 Data Skip On this dataset version Binomial still has the lowest FNR, but no model has a sighnificantly better results than the ranger model. Better results can be achieved with the use of parameter tuning. Based on the results, two models: Ranger and Ada were chosen for further experimenting. Hyperparameter tuning using random grid search was performed, resulting in classificators with the following values: Ranger num.trees=776, mtry=1, min.node.size=8 splitrule=“extratrees” Ada - loss=‘logistic’ - type=‘discrete,’ - iter=81, - max.iter=3, - minsplit=45, - minbucket=4, - maxdepth=1 2.4.7 Explanations results 2.4.7.1 Global 2.4.7.1.1 Variable Importance Figure TODO shows Feature Importance for both Ada and Ranger models. As we can see, both agree with each other as to what variables are most important to them. Their predictions are mostly based on the values of plas variable and then mass and age come second. Figure: Variable importance for Ada and Ranger based on AUC loss after permutations. 2.4.7.1.2 Partial Dependence Profile Partial Dependence profile seen in figure TODO give us perhaps the most interesting results. Firstly, they are adequate to Feature importance in Figure TODO. The more important variables return wider range of average prediction and less important give less varied predictions respectively. Secondly, we can observe the difference in model’s inner characteristics based on the shape of theirs PDP functions. Ada is more edgy and straight while ranger more smooth. Based on results yielded by PDP and according to these two models we can conclude that, generally speaking the bigger the values the higher the chance of positive Diabetes test result. Some other key observations include: Little to none impact by change of age after 35 years of living. A sudden increase in prediction with mass reaching 30 being the border of obesity in BMI model. Almost no change for ada prediction with change of pedi. A big increase in prediction with 7th pregnancy. Figure: Average predictions with PDP for ada and ranger. 2.4.7.1.3 Accumulated Dependence Profile Accumulated Dependence gave us similar results to Partial Dependence, thus we can conclude that our dataset does not contain any problematic correlations. Comparison between PD and AD is plotted on the Figure TODO for mass variable. Figure: Partial dependence and accumulated dependence on ranger’s mass variable. 2.4.7.2 Local Local explainations for Ada and ranger models were very similar so we focused on the one with a bit better metrics, that is Ranger. Ceteris Paribus shows us results adequate to Pratial Dependence Profile (See Figures TODO and TODO). Predictions are very consistent in variable plas. However in age and pedi cases we can spot different trends in prediction behaviour. This could hint towards potential interactions between variables. As for mass and preg they differ mostly in initial prediction value. Overall tendency is rising. Figure: Ceteris Paribus predictions for five selected observations Let us analyze observation 190 in more detail. Lime explanation (Figure TODO) tells us three biggest factors approximated by glass box model. Having plas lower than 100 in our case has significant negative impact on positive diagnosis of diabetes. On the other hand, having high mass index and being in 30’s adds to our probability of having diabetes. Figure: Lime output for observation 190 Breakdown and Shap (Figures TODO and TODO) as opposed to Lime give us prediction change for exact values, not intervals. Because of that the results may be different. So is the case with the age variable which behaves differently for Lime. According to Breakdown and Shap being 29 years of age slightly decreases prediction but Lime says otherwise. Not only it increases but also by a significant amount. Other variables seem to match their impact if just scaled down a bit for BD and Shap. Figure: Breakdown output for observation 190 Figure: Shap output for observation 190 The next observation we analyze is observation 214 for which the model has a prediction of about 44% for positive diagnosis of diabetes. Lime explanation (Figure TODO) tells us that three biggest factors for this prediction are pedi, plas and mass. This time the first one is pedi with high value of over 0.5. Despite high values of plas (over 115) and mass (over 33), the prediction is higher than average for this dataset, but still lower than 50%. Figure: Lime output for observation 214 Breakdown and Shap (Figures TODO and TODO) show similar results, just toned down. The biggest difference is the impact of pedi which in Lime was nearly twice as big as the impact of plas and mass. The only value making it less probable that the observation would have diabetes is age equal to 28. Figure: Breakdown output for observation 214 Figure: Shap output for observation 214 2.4.8 Discussion 2.4.8.1 Expert opinion There are two main types of diabetes: type 1 diabetes, where the body does not make insulin. It is mostly caused by genetic and environmental factors type 2 diabetes, where the body does not make or use insulin well. It is the most common kind of diabetes. It is caused by several factors, including lifestyle factors and genes Unfortunately, we do not have any data about the type of diabetes that was found among the patients in the investigated dataset. However, since type 2 diabetes is the most common type, and some of the factors causing the different types are common, we will focus mostly on the risk factors for type 2 diabetes. 2.4.8.1.1 Causes of type 2 diabetes According to the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK), type 2 diabetes can be caused by several factors: obesity and physical inactivity Extra weight sometimes causes insulin resistance and is common in people with type 2 diabetes. insulin resistance Type 2 diabetes usually begins with insulin resistance, a condition in which muscle, liver, and fat cells do not use insulin well. As a result, your body needs more insulin to help glucose enter cells. At first, the pancreas makes more insulin to keep up with the added demand. Over time, the pancreas cannot make enough insulin, and blood glucose levels rise. genes and family history As in type 1 diabetes, certain genes may make you more likely to develop type 2 diabetes. Genes also can increase the risk of type 2 diabetes by increasing a person’s tendency to become overweight or obese. ethnicity Diabetes occurs more often in these racial/ethnic groups: - African Americans - Alaska Natives - American Indians - Asian Americans - Hispanics/Latinos - Native Hawaiians - Pacific Islanders age Type 2 diabetes occurs most often in middle-aged and older adults, but it can also affect children. gestational diabetes Gestational diabetes is the type of diabets that develops during pregnancy and is caused by the hormonal changes of pregnancy along with genetic and lifestyle factors. Women with a history of gestational diabetes have a greater chance of developing type 2 diabetes later in life. 2.4.8.2 Comparison The causes of diabetes according to experts directly correspond with the columns is the explained dataset: plas - plasma glucose concentration, can indicate whether the patient suffers from a insulin resistance mass - Body Mass Index (BMI) can be one of the indicators whether patients weight puts him at risk for type 2 diabetes age- older people are generally more likely to develop diabetes pedi - predigree function is meant to be an indicator of patients’ risk for diabetes based on his genes and family history preg - women that have been pregnant multiple times are more likely to develop gestational diabetes, and therefore, type 2 diabetes later in life. Since all the women in the dataset have the same ethnicity, it is not a factor that can be considered valuable in this experiment. 2.4.9 Conclusion In this scenario the causes of diabetes indicated by the XAI methods match the causes presented by the medical professionals. The Explainable Artificial Intelligence is an extremely helpful tool. It can be used to gain better understanding of the classification models, especially the ones used for making medical diagnosis, where this understanding is crucial to patients health and safety. "],["how-the-price-of-the-house-is-influenced-by-neighborhood-xai-methods-for-interpretation-the-black-box-model.html", "2.5 How the price of the house is influenced by neighborhood? XAI methods for interpretation the black box model", " 2.5 How the price of the house is influenced by neighborhood? XAI methods for interpretation the black box model Authors: Klaudia Gruszkowska, Bartosz Jamroży, Bartosz Rożek (Warsaw University of Technology) 2.5.1 Abstract The value of a house is extremely important to all of us, no matter if we are buyers, sellers or homeowners. That is why correct price prediction is so important. With the help of machine learning we are able to predict this price but not every time we know what influenced such prediction result. For this reason, the interpretability of models, used in important areas of life, has recently become increasingly valuable. In this article we interpret and explain these unknowns using Explainable Artificial Intelligence (XAI) methods. 2.5.2 Introduction Being able to predict the potential price of a house is crucial for the real estate market. Therefore, over the years, many papers have been focused on improving prediction methods (Ge et al. 2021; Park and Bae 2015). The main question regarding forecasts is what data influences the price. In addition to physical factors of the house, such as size, number of rooms, the condition of the building, the price is also influenced by the area in which the house is located. The neighborhood affects the price of the house and the house affects the value of the surrounding buildings. Thus, house price prediction models based on neighborhood data were created (Can 1990; Heyman and Sommervoll 2019; Law 2017). With the development of artificial intelligence and machine learning, the use of this type of algorithm is gaining popularity in the field of forecasting house prices (Park and Bae 2015). In machine learning, we can distinguish two types of models: glass-box and black-box models. In the glass-box model human can follow the steps from inputs to outputs. Black-box models is also an important part of machine learning, however unlike glass-box models, they do not have a clear, human-readable way of determining predictions (Biecek and Burzykowski 2021). So why do we use them when we have glass-box models at our disposal? The complexity of black-box models is both an advantage and disadvantage at the same time. Due to it, we get better, more tailored predictions, but we also doom ourselves to the lack of precise information on how the given result was obtained. Using algorithms, we would like to know what influenced the prediction result and how. We are not inclined to trust such a difficult and complicated activity to algorithms whose decisions are unclear to human. Therefore, in this paper we use Explainable Artificial Intelligence (XAI) methods to analyze the output of the black-box model. Data The data that was used was the California Housing Data Set. This data was initially featured in the following paper:(Pace and Barry 1997).The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Data is cleaned, contains no empty information. Table 1 shows the explanation of each variable. Table 1: the meaning of the variables in the data set Column title Description longitude A measure of how far west a house is; a higher value is farther west latitude A measure of how far north a house is; a higher value is farther north housingMedianAge Median age of a house within a block; a lower number is a newer building totalRooms Total number of rooms within a block totalBedrooms Total number of bedrooms within a block population Total number of people residing within a block households Total number of households, a group of people residing within a home unit, for a block medianIncome Median income for households within a block of houses (measured in tens of thousands of US Dollars) medianHouseValue Median house value for households within a block (measured in US Dollars) oceanProximity Location relative to the ocean described as however in the category: “NEAR BAY,” “&lt;1H OCEAN,” “INLAND,” “NEAR OCEAN,” “ISLAND” Figure 2.38: Map of property distribution within the state of California. The brightness of the points reflects the price. The map 2.38 shows a density of bright blue dots in two locations, these are expensive properties located within two urban centers, Los Angeles, San Francisco, and San Diego. Also, more expensive properties are located along the Pacific coast. Figure 2.39: Map of property distribution within the state of California. Colors show the value of the oceanProximity column. Property map 2.39, packing the ocean_proximity feature. Properties with the ‘NEAR BAY’ category colored blue, appear in only one grouping next to San Francisco, NEAR BAY means San Francisco Bay. Green category, ISLAND collects 10 properties located on one island near Los Angeles. Machine learning model The machine learning model explained is from https://www.kaggle.com/camnugent/introduction-to-machine-learning-in-r-tutorial. A random forest model(Breiman 1999) from the randomForest (Liaw and Wiener 2002) library with parameters (ntree = 500, importance = TRUE) was used to predict property values. This model does not require scaling of data or decoding of categorical data. This makes it easier to analyze the results. The two columns total_bedrooms and total_rooms were averaged by dividing by the number of household members. The new columns are mean_bedrooms and mead_rooms. 2.5.3 Literature Over the years, many solutions have been developed for the task of house price prediction, and what particularly interests us, solutions using machine learning (Conway 2018; Fan et al. 2018; Park and Bae 2015). However, in our paper we will not focus on the process of creating good models but on the process of explaining models and their results using XAI methods. Why are these explanations so important? As we mentioned in the introduction, black box models do not give us insight into the reason for their decisions. As mentioned in the article (Barredo Arrieta et al. 2020b) that is one of the main barriers AI is facing nowadays. However, not everyone supports the development of XAI (Aivodji et al. 2019; Rudin 2019b ; Slack et al. 2020). They point to the insufficient credibility of the explanations and the possibility that the model conceal his unethical behavior. The second issue addressed in our paper is the relationship of house price to location. Several works have been created on this topic as well. For example, Stephen Law in his work (Law 2017) proposed defining a street as a local area unit and measuring its effect on house price. The results of his research of Metropolitan London showed significant local area effects on house prices. In our case the local area unit is block, which is a rectangular area marked by intersecting streets. Another example is the article written by Robin A. Dubin (Dubin 1998). He points out the importance of the correlations existing between the prices of neighboring houses, which is ignored in others method of predicting house values. 2.5.4 Local explanations Local explanations tells us, how model behaves in the area near given observation. Hence, it can not be used to explain how model works for whole set. On the other hand, it is useful to find out what can be done to change model’s prediction for observations we already have in our data set. We used observation 1 and 2000, because they are far from each other in the set so that they have different value.The location of the selected property is shown on the map 2.40 Figure 2.40: Choosen observations presented on a map. Red point - observation 1, green point - observation 2000 Break Down Break down provides a way to show the local importance of variables and the effect of individual values on predictions(Gosiewska and Biecek 2019). The basic idea is to calculate the contribution of variable in prediction of \\(f(x)\\) as changes in the expected model response given other variables. This means that we start with the mean expected model response of the model, successively adding variables to the conditioning. Of course, the order in which the variables are arranged also influences the contribution values. Figure 2.41: Break Down decomposition (L) observation 1, (R) observation 2000 Figure 2.41 shows the result of using Break Down method from DALEX (Baniecki et al. 2020a) package. The variables are ranked in terms of the importance of their impact on the predictions. The most important characteristic for both properties is median income. Median income is indicative of neighborhood status. Residents who earn more are likely to purchase more expensive properties. For both observations the influences are strong but opposite, high earnings for the first observation at $83,000 per year have a positive influence. In contrast, for property number 2000, earnings around $15 thousand per year are the reason why the model lowers the predicted price. Longitude is the second significant feature. Average number of rooms and location relative to the ocean was also significant. Interestingly, in observation number 2000, the inland location raises the price. From the map 2.38, it appears that it is the homes farthest from the ocean that are the cheapest. Lime Local Interpretable Model-agnostic Explanations (LIME) was firstly proposed by Ribeiro, Singh, and Guestrin (Ribeiro et al. 2016b). In LIME decomposition, our goal is to create an approximate glass-box model for a given observation. That model would be fully human-readable and can be easier analyzed. To do so, we create an artificial data set and teach the chosen glass-box model on it. The coefficients on the variables for the created model are the coefficients on the validity of the variables for our observation. Figure 2.42: Lime decomposition (L) observation 1, (R) observation 2000 Figure above 2.42 shows the result of using LIME method (lime?) from DALEX (Baniecki et al. 2020a) package. Common for both observations is that the median_income variable has the largest effect on prediction. However, the impact in the case 1 is large positive but in the case 2000 it is large negative. It is caused by the value of the variable. It is worth noting that the variables longitude and latitude have opposite signs in both cases. This is because these variables are negatively correlated. The second most valuable variable is the map location variable. It follows that for the observations, the key variables are median_income and some kind of location. This is consistent with our expectation and intuition that the higher the earnings of residents, the richer and more valued the neighborhood. Consequently, the houses themselves are also priced higher. Additionally, plots show us that impact of population,households or housing_median_age is not significant. What is interesting, in the case 2000 above 2.41 BreakDown as opposed to LIME found the location ocean_proximity = INLAND to be positive. Ceteris Paribus Ceteris Paribus is a phrase that comes from Latin. Word for word meaning is “other things equal,” but in XAI much more meaningful translation would be “other things held constant.” In fact, these few words describe this method deeply - we choose one variable from observation, change it’s value, while other variables held constant and calculate new model’s prediction. Output of this operation is a function that shows us how prediction changes. Figure 2.43: Ceteris Paribus: first observation Plot 2.43 above shows Ceteris Paribus calculated for first observation of data set. As we can see, in most cases it is straight line with change for small values. Households and population profiles are similar, which seems to be reasonable, because not many people look for apartment in a remote area. Latitude and longitude profiles are vertical lines with one decrease. Hence, a region on map can separated where median value would be the highest.Mean rooms is hard to explain in the opposition to mean bedrooms where we can see that the more bedrooms the higher the value is. It stops at certain value, above which valuation doesn’t change. Median income is almost linear function, which is understandable as rich people can afford expensive apartments. Figure 2.44: Ceteris Paribus: first and 2000th observation Next plot 2.44 shows Ceteris Paribus calculated for two different observations. Specially two observations with massive difference in median value where chosen to observe what would change. Clear differences can be observed. Especially latitude, mean bedrooms, mean rooms and population shows major changes. Rest of variables have similar profiles, but these four are great example why shouldn’t we draw conclusions about whole data set based on one ceteris paribus calculation. Local explanations conclusions As it could be observed, local explanations tells a lot about given observation. Unfortunately, juxtaposition of two different observations showed us, that they can have totally different explanations. Hence, local XAI, in this case, would allow us to increase already built house’s value rather than give an instruction how to build expensive property. To get this instruction global explanations should be used. 2.5.5 Global explanations In global explanations, we want to find out what influenced the model as a whole in the data, rather than looking at individual observations. This gives us an overall view of the predictions for the population. We can use this information to construct new observation with desirable value of target variable. Feature Importance Idea of permutation-based variable-importance(A. Fisher et al. 2019b): If a variable is important in a model, then after its permutation the model prediction should be less precise. The permutation importance of a variable \\(k\\) is the difference between model prediction for original data and prediction for data with permutation variable \\(k\\). Figure 2.45: Feature Importance Plot 2.45 show, Median income turns out to be the most important feature for the model under study. Removing the median income information by permutations, resulted in the largest decrease in the RMSE measure. This means that without this feature the model is much less precise. The geographic variables, longitude latitude and ocean proximity also turn out to be important for prediction. The average number of rooms, age, and the number of householders already have less impact on the prediction. The mean bedrooms category has the smallest and ambiguous impact. Whiskers extending beyond the left border of the bars indicate that there were draws where removing the variable improved the model results. The model may ignore this feature, having partial information about it in the form of number of rooms and household members. PDP PDP (Friedman 2000a) stands for partial dependence plot and it is connected with Ceteris Paribus. We take a random sample from data set, and calculate ceteris paribus for each one. After that, mean of all these functions is calculated, which leaves us with one profile for whole data set. In the plot below thick blue line is PDP and thin grey lines are Ceteris Paribus calculations for each observation. Figure 2.46: PPD As we can see above 2.46, households, mean bedrooms, mean rooms and population profiles are almost straight lines with fluctuations for small values. Housing median value is a interesting case, because small curvature can be observed at the end of line. This is probably due to the fact that new apartments are expensive, because they are modern and old ones are expensive because of historical background behind they. Latitude shows that most expensive houses are located on south, especially below \\(34 ^{\\circ} N\\), in California this is area near Los Angeles. Median house value is highest for longitude values lower than \\(118 ^{\\circ} E\\) (majority of a state except of San Diego and surrounding area). As it was in ceteris paribus, median income grows together with median house value to the level where income money simply doesn’t matter. Figure 2.47: PPD in groups PDP makes it possible to create separate profiles for different groups of observation. In our case, we used ocean proximity as it is a categorical variable. In the plot 2.47 above we can observe, that all profiles are similar to each other, the only significant difference is vertical shift. Conclusion drawn from this plot is the fact that houses 1h &lt;Ocean are most expensive, inland apartments are the cheapest, while near ocean and near bay behave similar to each other and have average prices. If we take a look at the plot from Data paragraph, conclusions look reasonable. 1h &lt;Ocean houses are the ones located in Los Angeles and in the suburbs of San Francisco, which are well known from expensive neighborhoods. Global explanations conclusions In contrast to local explanations, global explanations give a more general picture of the entire data set. With PDP profiles, we can see the relationships between the data. The future importance graph allows understanding what variables are relevant to the model. 2.5.6 Conclusion XAI methods can have multiple applications. They allow to study the behavior of Machine Learning Models. It gives an opportunity to understand the model, detect errors, for example, made during data preprocessing.With a good model matched to the data, XAI can be used to study the dependencies in the data. In this paper using XAI methods we made local and global explanations for the random forest model and 1990 California house data. This allowed us to bring the black box model closer to an interpretable as we learned the approximate effects of the variables on the prediction output. All the methods we used, Break Down, Lime, Ceteris Paribus for local explanations and Feature Importance, PDP for global explanations, indicated median income as the variable with the greatest influence on the prediction score. By combining the XAI methods with the results of the EDA analysis, we were able to observe and intuitively explain the findings. In the local explanations section we took a closer look at two very different observations and tested at the observation level how their variable values, affected the prediction outcome, and in the global explanations section we took a broader look at the performance of the model for the whole data set. In summary, our results provide insight into the value of using XAI methods to better understand black box models used for real-world problems such as, in our case, the real estate market and the impact of location-specific factors on house price. References Aivodji, U., Arai, H., Fortineau, O., Gambs, S., Hara, S., &amp; Tapp, A. (2019). Fairwashing: The risk of rationalization. In K. Chaudhuri &amp; R. Salakhutdinov (Eds.), Proceedings of the 36th international conference on machine learning (Vol. 97, pp. 161–170). PMLR. http://proceedings.mlr.press/v97/aivodji19a.html Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020b). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Breiman, L. (1999). Random forests. UC Berkeley TR567. Can, A. (1990). The measurement of neighborhood dynamics in urban house prices. Economic Geography, 66(3), 254–272. https://doi.org/10.2307/143400 Conway, J. (2018, January). Artificial Intelligence and Machine Learning : Current Applications in Real Estate (PhD thesis). Retrieved from https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf Dubin, R. A. (1998). Predicting house prices using multiple listings data. The Journal of Real Estate Finance and Economics. https://doi.org/10.1023/A:1007751112669 Fan, C., Cui, Z., &amp; Zhong, X. (2018). House prices prediction with machine learning algorithms. In Proceedings of the 2018 10th international conference on machine learning and computing (pp. 6–10). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3195106.3195133 Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Friedman, J. H. (2000a). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Ge, X., Runeson, G., &amp; Lam, K. C. (2021). Forecasting hong kong housing prices: An artificial neural network approach. Gosiewska, A., &amp; Biecek, P. (2019). Do Not Trust Additive Explanations. arXiv. https://arxiv.org/abs/1903.11420v3 Heyman, A., &amp; Sommervoll, D. (2019). House prices and relative location. Cities, 95, 102373. https://doi.org/10.1016/j.cities.2019.06.004 Law, S. (2017). Defining street-based local area and measuring its effect on house price using a hedonic price approach: The case study of metropolitan london. Cities, 60, 166–179. https://doi.org/10.1016/j.cities.2016.08.008 Liaw, A., &amp; Wiener, M. (2002). Classification and regression by randomForest. R News, 2(3), 18–22. https://CRAN.R-project.org/doc/Rnews/ Pace, R. K., &amp; Barry, R. (1997). Sparse spatial autoregressions. Statistics &amp; Probability Letters, 33(3), 291–297. Park, B., &amp; Bae, J. (2015). Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data. Expert Systems with Applications, 42. https://doi.org/10.1016/j.eswa.2014.11.040 Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016b). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, san francisco, CA, USA, august 13-17, 2016 (pp. 1135–1144). https://doi.org/10.18653/v1/n16-3020 Rudin, C. (2019b). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206–215. https://doi.org/10.1038/s42256-019-0048-x Slack, D., Hilgard, S., Jia, E., Singh, S., &amp; Lakkaraju, H. (2020). Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM conference on AI, ethics, and society (pp. 180–186). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3375627.3375830 "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 Author: Weronika Hryniewska Deep learning is one of the most rapidly developing field in artificial intelligence. Problems that previously required a lot of features engineering became easily solvable. New possibilities opened, and deep learning has started to adopt in various domains. One of the most demanding disciplines is medicine. As a result of the outbreak of the COVID-19 pandemic, many scientists became interested in the possibilities of deep learning application in radiology. Many solutions have been created for classification, segmentation and detection based on computed tomography and radiographs of the lungs. During classes, we explored deep learning methods for computer vision. If you would like to read more about them, please take a look at books: “Deep Learning with Python” (Chollet 2017) and “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems” (Géron 2017). We focused on results reproduction and/or further development of the available code of the following papers:: LungNet (Anthimopoulos et al. 2019) Adam Frej, Piotr Marciniak, Piotr Piątyszek BCDU-Net (Azad et al. 2019) (Asadi-Aghbolaghi et al. 2020) Maria Kałuska, Paweł Koźmiński, Mikołaj Spytek DeepCOVIDExplainer (Karim et al. 2020) Kacper Kurowski, Zuzanna Mróz, Aleksander Podsiad ERSCovid (S. Wang et al. 2020) Bartłomiej Eljasiak, Tomasz Krupiński, Dominik Pawlak COVID-Net (L. Wang et al. 2020a) Jakub Kozieł, Tomasz Nocoń, Kacper Staroń References Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In 2019 IEEE/CVF international conference on computer vision workshop (ICCVW) (pp. 406–415). https://doi.org/10.1109/ICCVW.2019.00052 Chollet, F. (2017). Deep learning with python. Manning. Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Wang, L., Lin, Z. Q., &amp; Wong, A. (2020a). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 "],["lungnet.html", "3.1 LungNet", " 3.1 LungNet Authors: Adam Frej, Piotr Marciniak, Piotr Piątyszek 3.1.1 Introduction Our goal is to recreate the results achieved in the article: Semantic Segmentation of Pathological Lung Tissue with Dilated Fully Convolutional Networks (Anthimopoulos et al. 2019). Authors developed a deep purely convolutional neural network for the semantic segmentation of interstitial lung diseases. The proposed CNN takes as input a lung CT image of arbitrary size and outputs the corresponding label map. We want to recreate that CNN and achieve as similar results as possible. Interstitial lung disease (ILD) is a group of more than 200 chronic lung disorders characterized by inflammation and scarring of the lung tissue that leads to respiratory failure. The diagnosis of ILD is mostly performed by radiologists and is usually based on the assessment of the different ILD pathologies in high resolution computed tomography (HRCT) thoracic scans. Early diagnosis is crucial for making treatment decisions, while misdiagnosis may lead to life-threatening complications. Pathological tissue is usually manifested as various textural patterns in the CT scan. The article proposes the use of a deep fully convolutional network for the problem of ILD pattern recognition that uses dilated convolutions and is trained in an end-to-end and semi-supervised manner. On the image is presented a typical HRCT scan with annotations. The white border line denotes the lung field segmentation, the blue denotes healthy tissue, the purple micronodules and the red the honeycombing pattern. Figure 3.1: Example of an annotated HRCT lung scan 3.1.2 Data In order to recreate the article we needed already annotated HRCT scans of lungs. 3.1.2.1 Original data The authors of the article used a dataset of 172 sparsely annotated HRCT scans, each corresponding to a unique ILD or healthy subject. The dataset contains 109 cases from multimedia database of interstitial lung diseases by the Geneva University Hospital (HUG), along with 63 cases from Bern University Hospital - “Inselspital” (INSEL), as collected by the authors. Two experienced radiologists from INSEL annotated or reannotated ILD typical pathological patterns, as well as healthy tissue in both databases. A lung field segmentation mask was also provided for each case. Article claims that the datasets are publicly available. However, we could not reach them. The HUG dataset required a copyright agreement signed by a person duly authorized by the institution (e.g., Department or Administrative Head or similar). We could not find any access to the INSEL dataset. In order to find datasets, which correspond to our theme, we looked for similar articles. One of them was an article Computer-Aided Diagnosis of Pulmonary Fibrosis Using Deep Learning and CT Images (Christe 2019), which is about pulmonary fibrosis that belongs to ILD. This article uses multiple datasets, but none of them is available online. 3.1.2.2 Our data Because of the issues with the original data we had to find new datasets. The first one, COVID19-DL contains lungs scans of COVID-19 patients. The dataset is designed for training deep learning models. It has 100 png images in grayscale, each in \\(256 \\times 256\\) resolution. The second one, The Lung Image Database Consortium (LIDC) consists of 1018 diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. The dataset is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Four experienced thoracic radiologists independently reviewed each CT scan and marked lesions. The images are in dicom format, grayscale and \\(512 \\times 512\\) resolution. The LIDC dataset is much better annotated and contains more cases. That is why we decided to use it in training process of our models. 3.1.3 Original model The original model has 13 convolutional layers. In particular, each of the first ten layers has 32 kernels of size \\(3 \\times 3\\) and dilation rates \\(1, 1, 2, 3, 5, 8, 13, 21, 34, 55\\). The authors chose not to increase the dilation rates exponentially. The outputs of these layers are concatenated, which are passed through a dropout layer with a rate of \\(0.5\\). The last three layers have \\(1 \\times 1\\) kernels with numbers of filters set to \\(128, 32, 6\\). The number of last- layer filters depended on number of classes. In our case we set them to \\(2\\) and \\(3\\) depending on dataset. 3.1.3.1 Preprocessing As authors did not mention any preprocessing techniques that had been used. We tried to improve results of their neural network by two techniques which we introduced by ourselves. One of them was a histogram equalization, which makes images more readable for humans. Below we have a comparison of two pictures with a histogram equalization and without. On the left side we have histograms of the scans. Figure 3.2: The comparision of scans Unfortunately, it did not improve results of our neural network. Another technique which we used was cutting images to smaller parts. We did it because a tumor is a small part of the whole image. By doing this, we increased the proportion of a tumor to the whole picture. Below we have a plot showing the comparison of preprocessing techniques. Figure 3.3: The comparision of scans 3.1.4 New models To improve results on our datasets we used another architectures of neural networks. 3.1.4.1 Smaller ones This architecture was a modification of the one proposed by the authors. We added a convolutional layer and a batch normalization to a convolution block and decreased the number of convolution blocks to 5. As a result, we did not achieve any noticeable changes in our metric. The model was not learning throughout epochs. Figure 3.4: The results of smaller architecture Figure 3.5: The results of smaller architecture 3.1.4.2 SegNet Another architecture which inspired us was the SegNet. It is a deep convolutional encoder-decoder architecture for image segmentation. Originally, it consisted of 10 convolutional blocks, where each block is made up of a convolutional layer, a batch normalization and as an activation relu is used. Depending if a block is in an encoder or a decoder, a pooling layer or a upsampling layer is added to the block. Figure 3.6: Original SegNet We used our modification of SegNet consisted of 8 convolutional blocks. We deleted 2 middle blocks. It performed much better than the original model proposed by authors of the article on both datasets. It achieved about \\(0.97\\) weighted accuracy over annotated area (the metric proposed by the authors of the paper, which we are trying to reproduce) on the LIDC dataset and \\(0.75\\) accuracy on the COVID dataset. Below we have two plots showing, how much our score on the validation set changed throughout epochs. Figure 3.7: The results of modified SegNet Figure 3.8: The results of modified SegNet This model we evaluated later. 3.1.4.2.1 Metrics We used several metrics to evaluate our model. The most important one is the confusion matrix. It shows how accurate is the model. We can notice that it improves a little over time but it is not significant. Most of predicted healthy area of the lungs is actually healthy. It is the main part of the scans. However, there are a lot of false positive cases. The model marks healthy parts of the lungs as lesion. It finds most of the actual lesions but marks it together with a lot of healthy part. Figure 3.9: Confusion matrix Accuracy, precision, recall and specificity are basic ones. Accuracy is not very meaningful because of the uneven distribution of the considered classes across the cases. Precision is very low that is connected with the confusion matrix. The model marks a lot of false positive cases. Recall is pretty high. The model finds most of the damaged part of the lungs. Specificity is extremely high that is also connected with the confusion matrix. The model finds almost all healthy lung tissue. In some metrics we can notice an improvement throughout epochs. Intersection over Union is an important metric in our case because we segment images. It says how accurately model marks lesions in lungs. It is calculated by dividing an area of the intersection of the actual and the predicted lesions by an area of the union of them. The metric is pretty high and improves over time. The reason why it is not close to 1 is the problem with false positives and low precision. 3.1.4.2.2 Our attempts to improve scores We tried to improve results of our best model in various ways. 3.1.4.2.2.1 Pretraining One of them was pretraining. We taught block after block to copy the input pictures. In this way we wanted our model to discover more patterns from our input picture. At the plots we can see the results of training each convolutional block. We achieved about 200 MEA for all blocks, but with more blocks we scored better result. Figure 3.10: Training of blocks After training of each layer we transferred weights to our modified SegNet. Then we frozen all layers of encoder and started training of decoder layers. Below we have a plot which shows the improvement of our model after training each layer of the encoder. We accomplished about \\(0.98\\) weighted accuracy. Figure 3.11: After transfer learning 3.1.4.2.2.2 Learning with an auxiliary task Our auxiliary task was generating a lung scan 3 millimeters further based on an input scan. We expected that our network would learn a better feature extraction for both tasks. Next we frozen first 11 layers, changed the last layer to softmax and we tried to learn our model on the proper task. Unfortunately, that model achieved worse performance and did not learn faster than the pure model. Figure 3.12: Performance of network with auxiliary task 3.1.5 Summary To sum up, our modified neural network achieved good scores. However, the whole project stopped to remind the original article. Article turned out to be irreproducible. Due to the fact that we had to change input data and model. References Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Christe, A. A. M. D., Andreas MD∗; Peters. (2019). Computer-aided diagnosis of pulmonary fibrosis using deep learning and CT images. Investigative Radiology, 54, 627–632. https://doi.org/10.1097/RLI.0000000000000574 "],["on-the-reproducibility-of-the-bcdu-net-model.html", "3.2 On the reproducibility of the BCDU-Net model", " 3.2 On the reproducibility of the BCDU-Net model Authors: Maria Kałuska, Paweł Koźmiński, Mikołaj Spytek (Warsaw University of Technology) 3.2.1 Abstract Reproducibility is a hot topic in present-day research. Especially in the field of data science and machine learning, the possibility of reusing the proposed solution by other scientists is essential as it proves its correctness and may become a starting point to further work. We introduce the article summarizing our attempts to reproduce BCDU-Net model used for lung segmentation from computer tomography images, proposed in (Asadi-Aghbolaghi et al. 2020). Moreover, we present our commitments to improve its performance, using techniques popular in the area of image processing. BCDU-Net turned out to be fully reproducible and highly optimized as only one of our attempts resulted in partly better performance. 3.2.2 Introduction Being able to reproduce results presented in published papers is a significant part of the scientific process. It is important mainly because it allows other scientists to verify that the method produces consistant results. The main goal of our work was to reproduce the results of the BCDU-Net deep neural network (Asadi-Aghbolaghi et al. 2020) and to check if the source code provided with the paper was of sufficient quality so as to add our own modifications to the network. We focused on working with the version of the network which performed a lung segmentation task. The architecture of the BCDU network is based on U-Net (Oktay et al. et al. 2018) which takes its name from the shape of the model. Both, the original and the improved models consist of max-pooling layers, convolutional layers and up-convolutional layers, but what makes BCDU-Net stand out, is the usage of BConvLSTM cells. They contribute to an easier flow of information between the first and last layers of the model, which in turn improves the performance. Original architecture of BCDU-Net 3.2.2.1 Dataset The dataset used by the authors of the article was downloaded from Kaggle and consists of eight files in NIfTI (Neuroimaging Informatics Technology Initiative) format. There are four 3D CT scans of human chest, the other four files are corresponding masks. Each photo consists of horizontal sections with the resolution 512x512. After unpacking those images, there were about 1200 horizontal images of lungs and corresponding masks. 3.2.2.1.1 Preprocessing The preprocessing applied to the lung images was to normalize the grayscale to the range \\([0, 255]\\) and to remove blood vessels and bones. In addition, horizontal sections consisting only of black pixels have been removed. In the case of masks, all pixels have been limited to the value set \\(\\{0, 1\\}\\). Moreover, masks around the lung area were generated, and binary erosion was also used in the photos prepared in this way. 3.2.2.2 Execution The code repository provided with the article contained all the necessary files to train the model and obtain results. At first we didn’t see if the authors attached information about the versions of packages they used, so we decided to try the following: keras==2.4.3, tensorflow==2.4.1, scikit-learn==0.24.1, numpy==1.19.5, matplotlib==3.3.4 and ran the code in python 3.8.7. Much later it turned out that the list of versions of packages was included in one of the pull requests in the Github repository, but we decided to stick with the ones chosen by us, as by then they worked and allowed us to obtain results. Even though the code was mostly functional, it still required some debugging. Some errors seemed as though they were simple omissions, whereas some might have been the result of us using slightly different versions of packages. In order to get the model to work we: added import os to the Prepare_data.py file, added import numpy as np to the models.py file, changed argument names from input, output to inputs, outputs in the definition of the model in the models.py file. After implementing these changes we successfully trained and evaluated the model. Even though at this moment the model was working, it was extremely slow. It was due to the fact that the calculations were being carried out using the CPU. To improve training speed we installed the cuDNN library, so that we were able to use the performance of our graphics cards. 3.2.3 Reproduction of the results Using the setup described in previous paragraphs we were able to train the model provided by the authors of the article and obtain results. Due to the technical limitations of our hardware we were forced to change some hyperparameters. We changed the batch size from 2 to 1, and the number of epochs from 50 to 40. Achieved results are presented in the table below. Accuracy Sensitivity Specificity ROC AUC Jaccard Score F1 Score claimed in the article 0.9972 0.9910 0.9982 0.9946 0.9972 0.9904 reproduced by us 0.9961 0.9894 0.9973 0.9933 0.9741 0.9869 The difference between these two models in most of these metrics is less than 0.1%. The only one with a bigger gap is Jaccard score. Our model produced results which show Jaccard score to be 2% lower than these claimed by the authors of the article. In our opinion the results are satisfactory and the difference can be blamed on the shorter learning time and changed batch size. The results from the article can be reproduced. 3.2.4 Further experiments In order to better understand the BCDU-Net network we decided to check out the possibility of improving its performance by implementing additional techniques, addressed to image processing. This way, we created a few modifications of original solution and could have compared the performances using well-known metrics applied to the problem of classification and, specifically, segmentation. 3.2.4.1 Modified architecture At first, we decided to modify the architecture of the model proposed by BCDU-Net’s authors. We developed and evaluated four new architectures which differed from the original one by layouts of layers, their size or activation functions. We have chosen two most interesting cases proving that slight changes may lead to different results. As the BCDU-Net consists of plenty convolutional layers, we decided to check what happens after adding another one. It was implemented in the beginning part of the model, before the first operation of max pooling. Moreover, the activation function of hidden layers was changed - we used hiperbolic tangent instead of reLU. The results of proposed version were marginally worse as they fell by less than 1%. First architecture modification The second chosen modification is more experimental. Inspired by another version of BCDU-Net, provided by authors in the source code, we removed one part of the network at the bottom of the schema. What is more, first layers after up-convolutional parts were quadrupled (each dimension was expanded by a factor of two). Parameters’ initialization method was changed from original He normal method to Glorot normal. Unfortunately, this modification turned out to be a flop. The value of Jaccard score was over three times lower than originally (0.29). Produced masks did not cover the whole surface of the lungs. 3.2.4.2 Regularization As one of the modifications to the BCDU network we tried adding regularization to the models’ layers. Including this penalizes the neural network for learning weights of high magnitude. Doing so makes the network a bit less complex, but can often lead to an improvment of the results. The two main methods of applying regularization are l1 and l2. In l1 regularization the penalty is proportional to the absolute value of the parameter, whereas in l2 regularization it is proportional to the sqare of of the weight. As such, l2 regularization is stronger when the values of parameters are higher, and weaker when they are lower. We tried applying both l1, and l2 regularization to the original model. The results of our testing have shown, that l2 regularization with the parameter \\(\\lambda=10^{-3}\\) scored the best. However the results are still somewhat lower than the results of the original network. The masks generated by the model with l2 regularization applied are shown on the figure below. Results of l2-regularized model 3.2.4.3 Additional preprocessing Even though the authors of the model already implemented some preprocessing features such as removing artefacts, we’ve decided to try some methods used to change the input pictures. They are often used as a way to improve the performance of a model. We tried two methods of altering the pictures: histogram stretching and histogram equalization. Contrast stretching is a technique, which allows the image to take up the entire brightness spectrum. The metod works by scaling the interval of the brightness values which appear in the image, to the whole available space. In traditional imaging that would be the interval \\([0,255]\\), but in our network \\([0,1]\\) as BCDU-Net uses values from the unit interval. In the case of our images, they were already scaled such that the lowest and highest values already appear in each picture. Because of that our implementation of histogram stretching didn’t have any effect on the pictures. The other method we’ve tried produced far greater results. Histogram equalization works by changing pixels’ brightness values such that the number of pixels of each brightness is approximately equal. There is however one modification we applied to this method. The value of zero contains valuable information in the case of our task. Because of that we left pixels of this value unchanged and applied the equalization only to positive values of brightness. The resulting images can be seen below. Histogram equalization We’ve trained the original architecture from the BCDU-Net article using data preprocessed with histogram equalization. The results were lower than these achieved by the original model. We have also tried to use non linear transforms. Such methods are often used in photograpy so as to apply filters to pictures. We didn’t however find any sources which would suggest using these methods in medical imaging. We’ve changed the value of each picture using the \\(\\tanh\\) function. The results shown below seemed to be just a brightened version of the pictures. Due to the lack of time, we decided that the images weren’t different enough from the original ones to train the model using them. Non-linear transform 3.2.4.4 Learning with an auxiliary task Transfer learning is a technique used in machine learning that focuses on benefits from storing knowledge gained during solving an additional, related problem. One of the most popular transfer learning methods is applying a new, auxiliary task for the model so we decided to add one to BCDU-Net. Details and inspirations on the subject of auxiliary tasks are exhaustively described in (Ruder 2017). Chosen auxiliary task was to predict the number of white pixels in the ground truth masks. New labels for the new task were acquired by ourselves from both traning and test datasets. In order to implement the new task, we were obliged to slightly modify the original structure of the net. After last up-convolutional layer, there used to be only one batch of three layers. The modification based on adding the new output of net, just after the last up-convolutional layer. We chose Root Mean Squared Error as a loss function. As its values were relatively small, when compared to the loss of original task, its weight was 2,5 times higher so it could have a significant influence on the process of fitting model to data. Proposed net modification resulted in high efficiency, nevertheless the values of metrics when evaluating it on test data, were not as high as these made by the original model. However, out of all modifications tested by us, this was one of the closest to the scores of original model. 3.2.4.5 GAN for CT images 3.2.4.5.1 CycleGAN The first idea was to use the available CycleGAN(Sandfort et al. 2019) to transfer non-contrast images to contrast images. This approach changes just the grayscale on image and does not change the shape of lungs, so we wouldn’t have to generate additional masks. However to use CycleGAN one needs to have both datasets: contrast and noncontrast. Only one data set was available to us. Therefore images we’ve prepared via histogram equalization were used as contrast images. Dilation with random kernel was applied to the generated images and corresponding masks in order to further diversify the initial dataset from the generated dataset. This action resulted in reduction of the lung area. CycleGAN results Later, a mix of both initial and generated dataset was used to train the BCDU-Net. Unfortunately, the training results were significantly worse. It might have happened due to the small differences between generated images’ ground truth masks and masks generated by us. 3.2.4.5.2 DCGAN The next approach was to generate images and masks from random noise with DCGAN by supplying to the network’s discriminator’s input both an image and a corresponding mask. This approach has been previously applied to chest X-ray images generation (Neff et al. 2017). The aim was to teach generator to generate images with corresponding masks simultaneously. The issue with this approach was that such a network requires hours of learning on a strong GPU. Therefore the code was implemented and run for too few epochs, which did not provide satisfying results. 3.2.4.6 Dual output model The goal was to help the person who interprets the photos locate them in the human body. Therefore, apart from the masks, we decided to generate labels informing about where a given lung section is located. There were two possible locations ‘upper’ and ‘lower.’ The division was based on the image below. The upper part is the part above human heart. The newly created network consisted of two branches. The first branch was the BCDU-Net network, while the second branch responsible for the classification of photos was the ResNet(3-2-resnet?) network. It has been slightly modified so that it could be used in a two-output model. Despite the small number of epochs = 2, the model results are satisfactory. Classification results Accuracy Sensitivity Precision F1 Score 0.9221 0.6066 1 0.7551 In the image below you can see lung images, masks and their labels produced by this modification to the network. 3.2.5 Other tools applied to the model 3.2.5.1 Tensorboard - supervising the training Tensorboard allows developers supervising the process of fitting and visualising the model. It is a visualization’s toolkit offered for models implemented in tensorflow. As BCDU-Net meets this criterion, we decided to use it during the process of fitting the original model so we could modify the hiperparameters in a pinch. Fortunately, checking the performance of the model on the validation dataset did not indicate overfitting or any other worrying phenomena. 3.2.5.2 XAI - explaining the predictions We’ve also worked on the aspect of explainability. Although deep neural networks are mostly seen as black boxes, in the recent years there’s been a lot of rapid development in methods which explain neural networks’ results, especially in the field of computer vision. However, we’ve encountered a problem, most of these methods are developed for classification tasks, and almost none of them work with segmentation. In the end we did find one, which worked - GradCAM. The method makes use of the gradients which are used to optimize the layers’ weights during training and can highlight the areas which are taken into consideration when producing the mask. In the example below we can see that the network focuses on the tissues which are inside of the human body but outside the lung area. We can deduce, that it recognizes the boundries of the mask in that way. GradCAM 3.2.6 Results and conclusions Accuracy Sensitivity Specificity ROC AUC Jaccard score F1 score claimed in the article* 0.9972 0.9910 0.9982 0.9946 0.9972 0.9904 reproduced original model 0.9961 0.9894 0.9973 0.9933 0.9741 0.9869 arbitrary modifications 0.8955 0.2908 0.9997 0.6453 0.2904 0.4501 l2 regularization 0.9867 0.9413 0.9974 0.9693 0.9311 0.9643 histogram equalization 0.9860 0.9920 0.9845 0.9883 0.9313 0.9644 auxiliary task 0.9933 0.9915 0.9937 0.9926 0.9660 0.9827 * authors of the article declared that training was conducted with hyperparameters impossible for us to use Our research based on (Asadi-Aghbolaghi et al. 2020) led up to a success. The solution appeared to be fully reproducible despite some minimal errors in provided source code. In the ‘Further experiments’ part we have described our contribution to the model development: proposed modifications of the architecture, adding regularization, conducting additional preprocessing of input images, learning with an auxiliary task, training with extended database using GAN and learning using dual output. Some of proposed modifications were evaluated using metrics dedicated to the problem of segmentation. The scores are presented in the table above. Unfortunately, modifications proposed by our team did not eventuate in improvement of the original performance, apart from the value of sensitivity after using histogram equalization, which proved high optimization of model proposed by Azad R. et al. However, we were limited by the resources we could have used for the project, so there is still a field to work continuation - trying to test and beat the model using better machines. References Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Neff, T., Payer, C., Stern, D., &amp; Urschler, M. (2017). Generative adversarial network based synthesis for supervised medical image segmentation. In Proc. OAGM and ARW joint workshop. Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., et al., et al. (2018). Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999. Ruder, S. (2017). An overview of multi-task learning in deep neural networks. https://arxiv.org/abs/1706.05098 Sandfort, V., Yan, K., Pickhardt, P. J., &amp; Summers, R. M. (2019). Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks. Scientific reports, 9(1), 1–9. "],["an-exploration-of-deepcovidexplainer-explainable-covid-19-diagnosis-from-chest-x-rays.html", "3.3 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays", " 3.3 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays Authors: Kurowski Kacper, Mróz Zuzanna, Podsiad Aleksander 3.3.1 Introduction and motivation To make no secret of it, the main motivation for our work was to pass the Research Workshop class. Our task was to try to reproduce the results of the DeepCovidExplainer project. It is a Deep Learning model based on deep convolutional neural networks for predicting disease (or lack thereof) from lung x-rays. In our opinion, however, the end goal isn’t the most important part of the journey. It is, as the saying goes, the process (and the friends we made along the way). Therefore, in this article we would like to focus on it. We will outline our adventure in trying to reproduce this model, including the problems and obstacles we encountered along the way. We will also try to describe what could be done to prevent such problems in your own projects. After all, reproducibility in research is necessary for two key reasons: to provide evidence of the correctness of a study’s results, and to provide transparency with your experiment and allow others to understand what was done. 3.3.2 Related work DeepCovidExplainer (Karim et al. 2020) is the work of 6 researchers. This project involved training several neural networks: VGG, ResNet, DenseNet, based on the COVIDx dataset to recognize whether a given image belongs to a healthy person, a COVID-19 patient, or a pneumonia patient. The COVIDx dataset, consists of six smaller publicly available datasets: Covid-chestxray-dataset (Cohen et al. 2020) COVID-chestxray-dataset: a public open dataset of chest X-ray and CT images of patients who are positive or suspected of COVID-19 or other viral and bacterial pneumonias (MERS, SARS, and ARDS.) Built to enhance models for COVID-19 detection (COVID-Net) and COVID-19 risk stratification (COVID-RiskNet): Actualmed COVID-19 Chest X-ray Dataset Initiative: also built to enhance models for COVID-19 detection (COVID-Net) and COVID-19 risk stratification (COVID-RiskNet) COVID-19 Radiography Database described in (Chowdhury et al. 2020) and in (Rahman et al. 2021), rsna-pneumonia-detection-challenge dataset from kaggle competition; Medical Imaging Data Resource Center (MIDRC) - RSNA International COVID-19 Open Radiology Database (RICORD) Release 1c - Chest x-ray Covid+ (MIDRC-RICORD-1c) (Clark et al. 2013). The above dataset is used in many other works on deep neural networks. We will name a couple of them. In the paper (L. Wang et al. 2020b), the authors use COVID-Net. They compare their results (more than 90% Accuracy) with the results they obtained on the VGG-19 and ResNet-50 networks. The paper differs from DeepCovidExplainer mainly in that in the latter the authors practically ensemble three networks, while in COVID-Net they use only the titular one. The authors of the paper (Ucar and Korkmaz 2020) use SqueezeNet to create the model, which is fine-tuned using Bayesian optimization. The fine-tuned model has over 90% Accuracy. The work differs in the network used and the method of selecting optimal parameters. Each of the above mentioned works deals with the application of deep neural networks in classifying the health status of a patient based on CXR images of his lungs. However, as our authors say, the final goal is to create a tool to assist radiologists in diagnosing the condition, not to replace them. 3.3.3 Our work 3.3.3.1 Getting started The first step in our journey was to create a virtual environment to run all the code. Unfortunately, this is where the problems start. The authors did not specify - either in their article or in the project repository - which libraries and their versions are needed, or even which version of Python the code is written for. This would have made our job very difficult if we had not thoroughly searched all the included code files. Fortunately, we were able to find a code snippet that listed the versions of some packages (numpy version 1.18.1, tensforflow version 1.14.0, keras version 2.3.1) and the runtime environment (Python 3.6.9). However, if this piece of code had not appeared in the previously generated .ipynb notebook it would probably have strongly affected the pace of our further progress, perhaps even prevented the completion of the project. Through trial and error (and repeating the process of running the code and installing missing libraries many times) we finally managed to figure out how the virtual environment should look like. Here are our results: Virtual environment Package Version Python 3.6 3.6.9 or newer numpy 1.18.1 tensorflow 1.14.0 tensorflow-gpu 1.14.0 (optional) keras 2.3.1 h5py 2.10.0 weightwatcher 0.2.7 matplotlib newest compatible scipy newest compatible scikit-learn newest compatible pandas newest compatible pydicom newest compatible ipython newest compatible jupyter newest compatible ipykernel newest compatible opencv newest compatible torch newest compatible PIL newest compatible xlrd newest compatible openpyxl newest compatible innvestigate newest compatible Standalone installation Program Version CUDA 10.0 (optional for gpu) cuDNN 7.4.2 (optional for gpu) 3.3.3.2 Finding data The main source of data for our project were the github repositories presented in the original article, which make up the COVIDx dataset. It contains chest X-ray (CXR) images of patients of different ages and health conditions. There are three prediction classes: the lungs of a Covid-19 patient, the lungs of a person with pneumonia, and the lungs of a healthy person. The images are mostly black and white and vary in size. Unfortunately, here we also encountered a major obstacle. Namely, the authors did not include the data on which they trained their model in their repository. We had to find the repositories from which the data were taken knowing the name of the dataset given by the authors (COVIDx). We then generated this dataset from various sources, although this was not without problems. Most of the datasets generated in this way had too few or too many images from the relevant classes relative to the description of the authors of the paper. We were able to obtain a dataset which mostly matched the description of the dataset used by our authors, but to this day we are unsure why the same dataset generation script kept giving different people different results. Hopefully the generated dataset matches the one used in DeepCovidExplainer but unfortunately we have no way of confirming that. 3.3.3.3 First look at the models After acquiring the dataset, it was time to attempt to train and test the preliminary models. In the article, the authors describe three separately trained model types: VGG (16, 19), ResNet (18, 34), and DenseNet (161, 201). All of these networks are convolutional networks; the number behind the name indicates the total number of layers (so, for example, VGG-16 has 16 and VGG-19 has 19). In VGG networks, the last three layers are dense layers, while the previous ones are convolutional layers, with a 3x3 filter, arranged in blocks of 2, 3 or 4. Each such convolutional block ends with max pooling. In all cases (except the last one, where the activation function is softmax,) the activation function used is ReLU. Figure 3.13: VGG19 architecture In the case of VGG, the convolutional blocks end in max pooling with stride=2, which makes the first two dimensions of the data accepted by the next layer twice as small. After a few convolutional blocks, we no longer have the ability to do another stride, making it impossible for the next block to operate on the reduced dimension. The consequence of this problem is that as the number of convolutional layers increases, the prediction error becomes larger. ResNet addresses this problem; it solves it by creating convolutional blocks with an additional connection between the data after the block and that before. In this way, further convolutional blocks learn small changes that can improve the prediction. This idea is somewhat developed in DenseNet networks. In them, instead of convolutional blocks, we have dense blocks that end in convolution with ReLU and max pooling. This time the results of the dense block are transferred as additional information to the result of each subsequent block. In this way, no information is lost and the network learns to improve the results slightly in each subsequent block. Figure 3.14: DenseNet161 architecture Of the six models mentioned in the paper, we decided to focus on the VGG-19 network, which we felt had the clearest architecture, was easily modifiable, and which we were able to run. However, here we also encountered a couple of problems. Firstly, the code from the repository set up by the authors created a model that distinguished 4 classes (covid-viral, non-covid-viral, bacterial, normal), while the article distinguished only 3 classes, which we wrote about earlier. We were therefore forced to modify the model to match the original idea from the article and change some of its parameters. Fortunately, this was not a hard problem to overcome, but its presence indicates that the code provided may be out of date with the article. 3.3.3.4 An attempt at preprocessing Our data for training the model had not been modified in any way at this point other than the resizing to 224x224x3 (RGB conversion) required to run the model. Therefore, we decided to do a full-fledged preprocessing as recommended by the authors of the paper. Unfortunately, the code for preprocessing posted on the repository did not work, so we had to actually write our own version of it, following the description from the article and fragments of unfinished code provided by the authors. Our preprocessing consisted of: resizing the image with anti-aliasing, histogram stretching (contrast enhancement), reducing noise with anisotropic diffusion, converting from RGB to greyscale format for better performance, creation of a mask using the threshold function from the OpenCV library (detection of the brightest spots in the photo), modifying (thickening) the mask using the dilate function from OpenCV (in order to be able to detect thin lines), removing annotations from the image using the inpaint function from OpenCV and using the already created mask. We have also dropped the part of the authors’ code where they introduce the division between the right and left side of the image, as they did not use this later in the code and it does not seem necessary. Figure 3.15: Before and after preprocessing 3.3.3.5 Modifications and other curiosities As we mentioned earlier, we focused on the VGG19 model, mainly because of the straightforward implementation and ease of modification of its architecture. We performed several experimental modifications and improvements that aimed to improve the performance of this model. Figure 3.16: Confusion matrix of the base model 3.3.3.5.1 Superficial changes The first changes we decided to make were small alterations to the VGG network. The reason for choosing VGG was the simplicity of making these changes combined with the ease of evaluating the results. Firstly, the value of filters was changed from 16 to 32 in the last (fifth) convolutional block. The second change was to add a new (sixth) convolutional block at the end of the network with the same values as the other blocks, except for the filters value which was set to 32. The last was to add another dense layer (256 neurons + Dropout 0.25 between the first and second dense layer) in the classifier block. Unfortunately, none of these changes resulted in improved prediction of the network. On the other hand, it’s somewhat interesting to note that these changes didn’t result in a significant worsening of the network prediction: the Accuracy still remains at 70-80%, the same as before the changes were made. Figure 3.17: Confusion matrices of models with small changes: first, second and third We also used 3 types of regularization: L1 regularization, L2 regularization, and mixed L1 and L2 regularization. In the case of mixed regularization, we obtained an improvement in the prediction of pneumonia, but this came at the expense of predicting both the absence of disease and the presence of COVID19. Unfortunately, it cannot be concluded that any of the regularizations improved network performance. Figure 3.18: Confusion matrices of models: with L1, with L2 and with both We also tested the performance of VGG19 without the so far present Dropout 0.5 mechanism between dense layers, and as you might guess we only got worse results. Figure 3.19: Confusion matrix of the model with no dropout 3.3.3.5.2 Multiple outputs and ensemble The idea behind creating the second output in our model was to see which dataset a particular image came from. To remind the reader, the dataset we are using is actually a mixture of several datasets. We tried adding an additional output three times. The first time, we decided to perform branching in the part of the VGG network composed of dense layers. The resulting output unfortunately turned out to be quite poor - first, the COVID-19 detection quality deteriorated, and second, too many images were misclassified as originating from the RSNA dataset. Figure 3.20: Confusion matrix of the model with branching in dense layers The lack of success with the first approach contributed to our second attempt - we separated the last three convolutional blocks and implemented the class weights mechanism. Unfortunately, this did not improve our results - this time all data were classified as coming from the sirm dataset. Figure 3.21: Confusion matrix of the model with earlier branching and class weights mechanism Finally, we decided to separate the network from the first convolutional block and dispensed with class weights. Similarly, this time all images were classified as coming from a single dataset. Figure 3.22: Confusion matrix of the model with the earliest branching and no class weights These results can be seen as a lack of success on the one hand, and somewhat positive news on the other. Despite the lack of improvement in prediction, we learned that images from different datasets are not that different. This is, of course, good news, so that we know that the model does not learn exemplary features based on potential special features of the sets. Let’s move on to our ensemble ideas. The first concept for the ensemble was to combine results from ResNet and VGG. Unfortunately, this idea did not give successful results, because ResNet classified all data as one class, which resulted in incorrect prediction of the whole ensemble. Figure 3.23: Ensemble of ResNet and VGG For this reason, we opted for an ensemble composed of most of the decently performing VGG networks generated during the previous steps. The results obtained were definitely better than those of the previous ensemble idea, but still did not give an improvement over the baseline. Figure 3.24: Ensemble of multiple VGGs 3.3.3.5.3 GAN and transfer learning - unsupervised pretraining and an auxiliary task To augment the training data, we decided to train an image generator using the generative adversarial network (GAN) method. With this model we could perform pretraining on a large number of randomly generated lung images without the risk of overtraining. The results are quite satisfactory however, due to hardware limitations the generated lung images are not of very high quality. Figure 3.25: GAN generator results We then tried to use pretraining on VGG19 using images from our dataset without labels. We chose not to use images generated from the GAN network because we were able to visually determine that the images were not of high enough quality for this task. We trained the convolutional layers using an unsupervised feature detection algorithm (autoencoder). After training the layers in this manner, we added an output layer and tuned the network using supervised learning (without unfreezing the convolutional layers due to the very high encoder accuracy - 99.99%). Unfortunately the results weren’t very satisfactory - the network placed far too much emphasis on the prediction of pneumonia and far too little on the prediction of normal lungs. Figure 3.26: Unsupervised pretraining results Let us focus next on the auxiliary task. We noticed that much of the dataset we use in our project comes from the RSNA dataset. It does not contain the lungs from COVID-19, but it does contain many more other types of ailments. This dataset also contains much more information about each image - such as the gender of the person in the image - and it was because of this information that we decided to create an auxiliary task. Our auxiliary task was to identify whether a photo was of a man or a woman. In order to accomplish this task we trained the VGG network, and then using the weights stored as training starters, we attempted to teach the network our initial task. Unfortunately, while the network’s results on the aux task were quite satisfactory, the network’s results on the output task were much worse - the network was unable to break out of the weights predicting two variables - moreover, it seems to even come close to predicting all values as one class. Figure 3.27: Auxilary task results 3.3.3.6 Final results Interestingly, all our models oscillated between 70-80% accuracy. In the end, it turned out that we got the best (or comparable) results after performing undersampling alone. This result is similar (or even a little better!) to the result obtained in the notebooks on the repository. Despite the fact that we tried to follow as closely as possible the process described by the authors, we could not achieve results similar to those in the article, where VGG19 had results ranging between 85%-95%. This is not surprising, given that we are not experts in Deep Learning practices and had access neither to the actual version of the code used by the authors nor are we even sure if we used exactly the same dataset as they did. Figure 3.28: Original authors’ results 3.3.4 Conclusions and summary As you can see, our road was long and sometimes arduous, but we must admit that we learned many valuable things during it. First of all, we gained a lot of practical knowledge about deep learning and became skilled in using related libraries. After the problems we encountered and the ones we heard about from our colleagues in other groups working on other deep learning projects, we drew some conclusions about what authors of a scientific paper can do to facilitate the reproducibility of their results. First, as authors we should make the code of our project available and also describe as precisely as possible the environment in which it was written, including our programming language’s version and the versions of all necessary libraries and additional software, if we use any. Without this it is very hard to verify our results and you can never be sure if any reproduction is completely accurate. The code we include should also be as up-to-date as possible, and it should be run at least once from start to finish. We should make sure it works as-is and doesn’t need to be corrected. If there are a lot of files, they should have clear and appropriate names, and we might even want to consider writing a short manual If we’re using open-source data, we should also provide access to it if possible, or at least describe its source in detail - it’s very useful to include links if they exist. By following these steps we will ensure reproducibility of our results and make it much easier to verify them, which as we know is crucial in any scientific project. References Chowdhury, M. E. H., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M. A., Mahbub, Z. B., et al. (2020). Can AI help in screening viral and COVID-19 pneumonia? IEEE Access, 8, 132665–132676. https://doi.org/10.1109/ACCESS.2020.3010287 Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., et al. (2013). The cancer imaging archive (TCIA): Maintaining and operating a public information repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7 Cohen, J. P., Morrison, P., Dao, L., Roth, K., Duong, T. Q., &amp; Ghassemi, M. (2020). COVID-19 image data collection: Prospective predictions are the future. arXiv 2006.11988. https://github.com/ieee8023/covid-chestxray-dataset Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Abul Kashem, S. B., et al. (2021). Exploring the effect of image enhancement techniques on COVID-19 detection using chest x-ray images. Computers in Biology and Medicine, 132, 104319. https://doi.org/https://doi.org/10.1016/j.compbiomed.2021.104319 Ucar, F., &amp; Korkmaz, D. (2020). COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from x-ray images. Medical Hypotheses, 140, 109761–109761. Wang, L., Lin, Z. Q., &amp; Wong, A. (2020b). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z "],["erscovid.html", "3.4 ERSCovid", " 3.4 ERSCovid Authors: Bartlomiej Eljasiak, Tomasz Krupinski, Dominik Pawlak 3.4.1 Introduction "],["covid-net.html", "3.5 COVID-Net", " 3.5 COVID-Net Authors: Jakub Kozieł, Tomek Nocoń, Kacper Staroń 3.5.1 Introduction "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 Author: Paulina Tomaszewska Artificial Intelligence (AI) especially Deep Learning (DL) is a rapidly emerging field. It is proved by the number of publications – every day some new paper is released. In the spirit of “open science” (Mendez et al. 2020) not only papers are published in journals but also are available previously as preprints. This helps in the fast exchange of knowledge between researchers. In order to fasten progress in the field even more, it is recommended to open source also the code as well as data used for analysis in the paper. In such a scenario, researchers inspired by someone’s papers will not have to implement the described solution independently but rather focus on adding improvements. Such a workflow, however, requires the reproducibility of the results shown in the paper. It means that by running the code given by the authors, the same results as described in the paper should be obtained. People started to verify the reproducibility of the papers also to check whether the results in the paper are reliable. It sometimes happens that the authors do “cherry-picking” of the results. The reproducibility of the papers is getting more and more attention. There is a web page (Yildiz et al. 2021) where the outcomes of the paper reproducibility studies are stored. In this chapters, students focused on the reproducibility of Deep Learning papers. It was motivated by two facts featuring models in Deep Learning: they are complex (often have an immense number of parameters) they have an inherent component of randomness (e.g. weight initialization, data augmentation) These two points show that the task of reproducibility in Deep Learning can be sometimes a challenge (Liu et al. 2020). References Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. "],["what-makes-an-article-reproducible-comparison-of-the-fer-paper-and-axondeepseg.html", "4.1 What makes an article reproducible? Comparison of the FER+ paper and AxonDeepSeg", " 4.1 What makes an article reproducible? Comparison of the FER+ paper and AxonDeepSeg Authors: Mikołaj Jakubowski, Patryk Tomaszewski, Mateusz Ziemła (Warsaw University of Technology) 4.1.1 Abstract Reproduction of the code presented in scientific papers tends to be a laborious, yet important process, as it gives the reproducers a better understanding of the methods proposed by the authors, and verifies the credibility of a given paper. While recreating an article, various problems, which sometimes can be hard to overcome, can appear. We decided to go through these problems and compare two papers with their corresponding code. As a result, we identify the most important characteristics of an article that factor into its reproducibility. 4.1.2 Introduction 4.1.2.1 Why is reproducibility important? Reproducibility is the ability to rerun an experiment described in a given article and obtain results similar to those presented. Ideally, we would like for the reproduced results to be identical to the ones in the article, however this can be almost impossible to achieve due to the influence of external factors, such as the internal structure of the GPU used. Unfortunately, in most cases even getting similar results may prove to be very difficult (Baker 2016). Many articles do not contain enough information for the reproduction process to be successful. Crucial steps of the experiment may be omitted or presented incorrectly. This is very damaging for the article itself and for the message it is trying to convey. Without the ability to rerun the experiment, the reader cannot be sure that no mistakes were made in the process or that the results were not purposefully misrepresented to fit the narrative. Because of that, the credibility of the article is significantly reduced, and it may prevent other researchers from using those findings in further works (Tatman et al. 2018). Moreover, being able to see each stage of the experiment on its own, tweak different values and observe the change in results can help the reader to better understand the methodology presented in the article and allow for them to apply it, or even improve it, in the future. All those reasons make reproducibility a crucial part in any scientific article, and yet it is being overlooked more often than it should. 4.1.2.2 Motivation The fundamental purpose of this article is to identify good and bad practices of an article’s reproducibility. We outline the methods used in AxonDeepSeg, so they may be used as a reference in future creation of articles to ensure their reproducibility. 4.1.2.3 Methods We decided to analyze two different papers, FERplus (Barsoum et al. 2016) which will be used for comparison, and AxonDeepSeg (Zaimi Aldo et al. 2018) which was our main focus. We analyzed the articles alongside the source code and other materials provided within them. For both papers, we evaluated their consistencies with the actual code and the ease of replication for anyone trying to verify their results. For the first paper, to evaluate the impact of omitted information and inconsistencies on the final results, we also created a secondary model in the Keras framework, since the original code is presented using the CNTK framework. 4.1.3 Analyzing the FERPlus paper What the authors of this paper want to demonstrate, is how to train a deep convolutional neural network (DCNN) from noisy labels. A great example of obtaining them is getting crowd-sourced labels of facial expressions. With the accuracy oscillating around 65%, there are more precise methods of tagging these photos, but they appear to be significantly more expensive and slower to produce. The authors want to compare 4 different approaches for training DCNNs, showing that including outlier-tagging information in models may have a positive impact on the results. Specifically for this research a new dataset of facial expressions, called FER+, has been created. Experiments have shown that, as the number of taggers increased, their agreement rate increased as well. They ended up with 10 taggers to label each face image, with one of eight emotions. This allowed for experimentation with multiple distribution schemes during training. Authors claim to have used a custom VGG13 model with 64x64 input, 10 convolution layers, interleaved with max pooling and dropout layers, and 8 emotion classes as output. Image 1. DCNN architecture used by the authors As mentioned earlier, various approaches for utilizing labels would be presented. In each example, probability distributions of emotion captured by the facial expression will be generated in a different way. Majority Voting (MV) is the simplest and most obvious method. Each observation gets assigned a label, with 1 for an emotion that was chosen most frequently and 0 for all other emotions. Multi-Label Learning (ML) approach assumes that it is natural for a picture to present more than one emotion. For each observation, whenever a certain emotion’s votes have exceeded a given threshold that emotion is labeled with 1, otherwise with 0. In Probabilistic Label Drawing (PLD), labels are dynamically assigned in each epoch of learning. Every observation gets a label with 1 for a chosen emotion, and 0 for all other emotions. The labels are randomly chosen with probability distribution, based on percentage of taggers’ votes for each of them. The Cross-entropy loss (CEL) method simply feeds the model with labels being equal to probability of an emotion appearing in observation. Exactly as before, probability distribution is based on percentage of taggers’ votes for each of emotions. Results of their work, which lead them to the conclusion that PLD and CEL are more effective in this scenario, were as follows: Scheme Accuracy MV \\(83.85\\%\\) ML \\(83.97\\%\\) PLD \\(\\pmb{84.99}\\%\\) CEL \\(84.72\\%\\) Table 1. Results of the evaluation of the models according to the article 4.1.4 Reproducibility analysis 4.1.4.1 Insufficiently explained data augmentation The original model had a 64x64 input, even though the original FER data was only 48x48. The input size was bigger because data augmentation was used. However, this process was only briefly mentioned. Due to the insufficient explanation of used methods, we chose to skip this process, and just repeat the other described steps, as we believed the data augmentation step not to be crucial to the hypothesis. The transformations applied in the original code were hard to reproduce exactly, as the exact specifics of data augmentation were split over multiple objects, variables, and methods. 4.1.4.2 Inconsistencies in the article As it turns out, the equations for the losses, which were a major part of the hypothesis, were written out incorrectly. The equations were in conflict with the words preceding them. For both Multi-Label Learning and Probabilistic Label Drawing, the losses were given in the form of: \\[\\mathcal{L} = - \\sum^{N}_{i=1}\\underset{k}{\\operatorname{argmax}}g(p)^{i}_{k}\\log q^{i}_{k}\\] where \\(g\\) was some form of an element-wise transformation. For ML, this was described as a new loss function, and for PLD it was described as standard cross entropy loss. The equation given is neither standard cross entropy loss, nor a valid loss function. \\(\\operatorname{argmax}\\) cannot be used as a gradient descent loss function, as it is not continuous, and its gradient is zero everywhere it is defined. After examining the source code, we realized the authors actually used standard cross entropy loss (extended by a transformation function on the target vector) losses except in the ML scenario, where the following loss was used: \\[\\mathcal{L}=- \\log\\sum^{N,\\#\\text{features}}_{i,k = 1}(p \\odot q)^{i}_{k}\\] where \\(\\odot\\) denotes element-wise multiplication. This loss was not present, and is not an equivalent of any loss function given in the article, both in writing or in the form of an equation. With these inconsistencies, a major part of the article is in direct conflict with the code. 4.1.4.3 Source code reproduction Multiple outdated dependencies were required, which forced us to downgrade our environment for it to work properly. Other than that, there were no issues with running the code. Both the setup process and usage was clearly described on the github page and worked as described. It is also worth noting that the original code is compatible with the CUDA architecture, making the training process significantly faster. After training each of the models once, we noticed that the accuracy we received was on average 2% lower than the values presented in the article. As no modifications were done to the source code, we are unsure why such differences appeared. However, the relation between the schemes which were highlighted in the article stayed the same - with majority voting and multi label being noticeably worse than probability or crossentropy. While this time the most accurate scheme turned out to be crossentropy, the gap between it and PLD is small in both the article and our results, and the difference can be attributed to variation. Scheme Accuracy MV \\(85.81\\%\\) ML \\(85.72\\%\\) PLD \\(86.58\\%\\) CEL \\(\\pmb{86.96}\\%\\) Table 2. Reproduced results of the evaluation of the models 4.1.4.4 The Keras model We tried to reproduce the original model in Keras1, trying to limit ourselves only to the information contained in the article, as long as it was possible. As mentioned before, we were forced to change the model to accept 48x48 inputs, and trained it as such, with appropriate losses equivalent to the losses in the original code. Using 48x48 inputs has caused a slight bottleneck in the network after the convolution and pooling layers, as the lower initial inputs reduce the dimensionality of the last convolution layer in the VGG architecture, but we assert that the bottleneck limits the capabilities of the model independently of the loss function. After training the 4 models once, we arrived at similar results as the original paper. The accuracies of the model were significantly lower (~\\(8\\%\\)), but this can be accounted for by a slight bottleneck in the middle of the model caused by the smaller input dimensions, the different gradient descent algorithm we used and untuned hyperparameters. Our training results were: Scheme Accuracy MV \\(77.42\\%\\) ML \\(76.52\\%\\) PLD \\(\\pmb{78.65\\%}\\) CEL \\(78.12\\%\\) Table 3. Results of the evaluation of the reproduced Keras models Our reproduction of the original model gave the same results - that PLD is the best method out of the four given in the article for predicting multilabel data. Sadly, it was not possible to recreate the article’s thesis using the article alone. 4.1.5 Analyzing the AxonDeepSeg paper The previous article has shown some of the issues that may appear when trying to reproduce an article. To further focus on methods of improving reproducibility, we analyze the AxonDeepSeg paper - an article that we have found to be very reproducible. Authors of this paper are introducing a new open-source software AxonDeepSeg, created to ease the process of axon and myelin segmentation from microscopy images of the nervous system. This can be especially useful in research of magnetic resonance imaging, as a validation method for new techniques. Said software has three features: - two ready to use CNN models trained from scanning electron microscopy (SEM) and transmission electron microscopy (TEM). - a CNN architecture, suited for axon and myelin segmentation problem - a training procedure, which can be used to generate new models based on manually-labelled data As the dataset for such a problem is not easily accessible, it had to be created for the purpose of this project. Microscopy images were marked and cross-checked by at least two researchers. Sheets were manually segmented using GIMP. The final ground truth dataset consists of a single png image with the corresponding values: background = 0 (black), myelin = 127 (red), axon = 255 (blue). Some of the pictures required small manual corrections to avoid false positive outcomes. Since this dataset is not large, an augmentation strategy was used in order to reduce overfitting and improve generalization. This strategy includes random shifting, rotation, rescaling, flipping, blurring and elastic deformation. The architecture of the CNN was designed for both SEM and TEM images. The image below explains the architecture design. Dashed lines signify that the segment is viable only for the SEM model, since the TEM model is smaller. Image 2. CNN architectures used in AxonDeepSeg models To assess the quality of the segmentation authors used Dice coefficient, given by \\(Dice = \\frac{2(|A \\cup B|)}{|A| + |B|}\\) where \\(2(|A \\cup B|)\\) is the number of pixels that well predicted by the model in both images, \\(|A|\\) is the number of well predicted pixels in image A and accordingly \\(|B|\\) for image B. To assess the performance of myelinated fiber detection, they used sensitivity (true positive rate) and precision (positive predictive value) measures. Both of them are based on the number of detected axons, calculated using the positions of their centroids. \\[TPR = TP/(TP+FN)\\] \\[PPV = TP/(TP+FP)\\] Authors evaluated their trained model with the metrics given above, which produced the results as follows: Modality Test sample(s) Axon Dice similarity Myelin Dice similarity Pixel-wise accuracy Sensitivity Precision SEM Rat 1 0.9089 0.8193 0.8510 0.9699 0.8468 SEM Rat 2 0.9244 0.8389 0.8822 0.9876 0.7987 SEM Human 0.8089 0.7629 0.8114 0.9300 0.7306 TEM Mice 0.9493 0.8552 0.9451 0.9597 0.9647 TEM Macaque 0.9069 0.7519 0.8438 0.9429 0.8129 Table 4. Results of the evaluation of the models according to the article 4.1.6 Reproducibility analysis 4.1.6.1 Installation instructions The installation was well documented, with all dependencies listed as an anaconda dependency file. Because of that, the process of installation of necessary libraries was streamlined to executing commands listed on the project’s page. The project also provides a video describing the installation process. 4.1.6.2 Usage instructions with examples Since the paper lead to the development of a scientific tool, the code ended up well-documented. A website containing detailed instructions regarding the usage of the tool is provided with the project. For ease of use, the tool also comes with several python notebooks with fully functional code snippets displaying its potential use cases and capabilities, such as data preparation, model training, processing images and usage of various evaluation metrics. 4.1.6.3 Accessible prepared data The source code comes with two example datasets, one for the SEM model and one for the TEM model, which are the same as the ones used in the article. Both of those datasets are available in a format already prepared for work with this tool. Additionally, the documentation contains instructions on how to create a compatible dataset from your own images, as well as a reference to the source of images used in the examples. 4.1.6.4 Documentation of differences between the code and the article The project itself is still being improved after the release of the article. Because of that, there do exist differences between the code and the original paper, but they are documented inside of the repository, and each difference is given a reason for the change. 4.1.6.5 Accessible pretrained models The aforementioned notebooks contained functions capable of downloading pretrained models for the tool from the internet, making the models accessible with a single line of code. As such, for most use cases there is no need of dedicating hours of processing time to train a different model. Evaluating those models yielded similar, if not slightly better results to the ones described in the article. The difference can be attributed to the constant improvement of the source code since the release of the article. Modality Test sample(s) Axon Dice similarity Myelin Dice similarity Pixel-wise accuracy Sensitivity Precision SEM Rat 1 0.9256 0.8366 0.9574 0.9178 0.9336 SEM Rat 2 0.9458 0.8278 0.9666 0.9337 0.9581 TEM Mice 0.9439 0.8661 0.9701 0.9179 0.9738 Table 5. Reproduced results of the evaluation of the models 4.1.6.6 Working training code The notebooks also contained example well commented code allowing easy model training, either with the use of training data used in the article or any other data conforming to the format specified in the documentation. We ensured that the code was working by successfully training a model with a single epoch. 4.1.7 Conclusion While the source code of the first article can be compiled without any difficulties, and the results it generates are similar to the ones described in the article, the code itself is not consistent with the article. The differences in loss methods, which are the focus of the paper, and the lack of a proper description of used augmentation methods makes this article poorly reproducible without access to the code. Even paired with the code, the paper is not a proper description of the code’s actions, and as such, the thesis of the article is not the thesis which the code is evidence for. In contrast, the second article provides a sufficient description of the technical knowledge required to reproduce the source code. No details relevant to the paper’s thesis were omitted. As the code changed over time, all inconsistencies with the article are labeled, and the reason for each one of them is given. The authors went to great lengths to ensure that the usage of the tool is as easy as possible, providing detailed instructions for common use cases, as well as video recordings and premade code snippets. The project also contains example datasets and pretrained models for each of the network types, further simplifying working with the tool. We posit that there are two main components of reproducibility of an article: the consistency of the method used with the method described in the paper, and a proper technical description of all variables relevant to the thesis, so that with enough effort, the method can be repeated again from the paper alone. What’s more, the inclusion of additional material, such as code snippets, is also beneficial for the ease of reproducibility. References Baker, M. (2016). Reproducibility crisis. Nature, 533(26), 353–66. Barsoum, E., Zhang, C., Ferrer, C. C., &amp; Zhang, Z. (2016). Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM international conference on multimodal interaction (pp. 279–283). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2993148.2993165 Tatman, R., VanderPlas, J., &amp; Dane, S. (2018). A practical taxonomy of reproducibility for machine learning research. Zaimi Aldo, W. M., Herman, V., Antonsanti, P.-L., Perone, C. S., &amp; Cohen-Adad, J. (2018). AxonDeepSeg: Automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. https://doi.org/10.1038/s41598-018-22181-4 https://gist.github.com/mtizim/364d25f4bdc2ee00cb2a97d270d6aef2↩︎ "],["can-you-classify-histopathological-data-at-home-reproducing-the-ara-cnn-models-data-and-performance-.html", "4.2 Can you classify histopathological data at home? Reproducing the ARA-CNN model’s data and performance.", " 4.2 Can you classify histopathological data at home? Reproducing the ARA-CNN model’s data and performance. Authors: Jan Gąska, Jakub Lis, Wojciech Szczypek 4.2.1 Abstract The idea of reproducibility of scientific researches is crucial especially in the area of data science. It has become more important along with the development of methods and algorithms used in machine learning as they are more and more complex and complicated. This issue concerns users of all types: students, scientists, developers. Moreover, attaching code used in a paper helps readers to focus on the real content rather than sophisticated explanations and descriptions included in the article. It is also valuable because the users can use the code as examples of using the package. When it comes to Deep Learning the problem of reproducibility is much more disturbing than anywhere else. There are so many factors that are random-dependent in DL algorithms, that it is crucial to provide some kind of tool, which would enable us to deal with differences between results presented in the article and our own chunks of code. This being said, it can be easily observed that reproducibility could be beneficial for both authors of the scientific articles and other researchers who would like to retrace the article’s workflow. 4.2.2 Introduction Recent developments in the field of Machine Learning, and especially, Deep Learning have provided many specialists with a powerful tool regarding their occupation, research, and, in general, development. However, the medical field has been affected, positively, to the largest extent among these professions, with the deep learning algorithms, statistics, and machine learning, supporting the medical field and contributing to its automatization. Nonetheless, due to its delicate and crucial nature, which is the better good of the human life, precision, thoroughness are required, thus it necessitates a precise model, able to differentiate between alike types of recognition problem and telling the uncertainty of its conclusions; in sum, reliability and almost perfect accuracy are critical metrics if comes to developing our model, overwise an error may cause fatal consequences. In our case, our described model ARA(Łukasz Rączkowski 2019) implements classification methods to histopathological cancer images. Model’s functionality was almost entirely based on Convolutional Neural Networks (CNN’s), using the Bayesian approach, as it is common in classifying images, as a form of implementing the medical vision and classifying eight different types of cancer. The model is set to counter bias, high variance in data answers, and low accuracy on new, unseen data by the model, which is affected by the difficulties in training the data, thus several methods had to be implemented to balance out the data set’s non-balanced structure, as well as to prevent overfitting that often causes problems in alike models. The used data set for training contains 5000 images with ascribed labels of types of colorectal cancer, photos were cut in order to focus on the tissue itself, rather than on the neighboring tissues. Namely, the data set used is called Kather et al. With authors claiming almost over 90% of accuracy to their model, we have tried on recreating the steps and authors recommendations in their procedures, nonetheless throughout our effort we have remained unbiased by the results. 4.2.3 Definition Reproducibility as a problem has been addressed by scientists of various fields of studies. The exact definition also differs among areas of studies. For instance, Patrick Vandewalle in 2009 suggested a definition of reproducible research work: “A research work is called reproducible if all information relevant to the work, including, but not limited to, text, data, and code, is made available, such that an independent researcher can reproduce the results” (Vandewalle et al. 2009). On the other hand, Association for Computing Machinery (Computing Machinery 2018) divides the problem into three tasks as follows: Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation. Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts. Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts that they develop completely independently. 4.2.4 Methodology We considered two scientific papers: RMDL: Random Multimodel Deep Learning for Classification and ARA: accurate, reliable, and active histopathological image classification framework with Bayesian deep learning plenty of papers from many issues of both of the articles were supplied with GitHub repository, thus we were given supplementary material to check whether the issues raised in the articles are reproducible. As we faced the problem of measuring reproducibility we discussed many ways of grading its level. One of the ideas was to create a unified measure of value that would calculate the ratio of plots we managed to reproduce. However, we quickly noticed that this approach is not appropriate as sometimes it is not fair to mark articles by the same fixed criteria. There are some additional plots, and reproducing them is not as important in the terms of reproducing the whole article as if it was with more crucial plots. These are the reasons why we did not decide to use a binary (1 - reproducible article, 0 - not reproducible article) as a final mark for the articles. When we were investigating the articles in terms of reproducibility we noticed that the problems we were facing could be grouped into a few categories. Thus we proposed 6 different issue categories which our research team faced during the attempt to reproducing the results presented in a scientific paper: Requirements The code used in the article requires additional libraries and packages to be installed. We assessed whether a list of the requirements was available (e.g. it was provided in the GitHub repository). Moreover, we have taken into account the versions of the packages and whether the dependencies issues were resolved well. Resources DL algorithms are time and resource-consuming. Sometimes it is impossible to reproduce results on an ordinary personal computer. We found this issue quite crucial for people who would like to reproduce the results in the article. Randomness problems Some functionalities are based on randomness. Sometimes even changing the seed may change the results and make it difficult to draw correct conclusions. No access to source codes Some results shown in an article could not be reproduced because the codes had not been available on the Github repository or it was not explained explicitly how to use them properly. Results Last but not least our aim was to reproduce the results provided in the article. Through series of experiments, we wanted to achieve identical or at least similar results as those presented in the article. Such classification helped us highlight to what extent were the articles reproducible in the terms of each category. 4.2.5 Results It is high time we commenced forth to the main parts of our analysis, the results. In them we will have an insight into the previously mentioned categories, using them as our metric for the final verdict of the reproducibility problem, describing our procedures while progressing through the steps. Further in the document, we will summarize the results on a perspicuous chart. 4.2.5.1 Requirements The requirements were officially provided under this link by the authors on their project’s github. The first potential problem that explicitly caught our attention were the long outdated versions of packages, e.g pandas on version 0.23.1, or tensorflow on 1.12.0, nonetheless it was understandable due to the rapid nature of packages development in the computer science environment, especially in the case of Python. Unfortunately, with given requirements, the provided code faces critical issues, not allowing it to properly execute, forcing us into the inquiry, which has resulted in reconsideration of few packages. For code to execute, we changed the tensorflow version from 1.12.0 to 1.13.1. This simple update allowed the code to properly function. Unfortunately there was little information provided by the authors, nor on the github page, nor in the article, about potential issues regarding the version, and only could we resolve the version incompatibility by examining the error structure and peeking deeper into the code, which consumed a part of our research. 4.2.5.2 Resources In general case of resources, they can be divided into the computational cost and time cost. Because nor the article, nor the github provided the information about neither of aforementioned costs, there was little to compere, nonetheless (to be precise, the article mentions in general terms the lower costs of time of their model, but gives no benchmark nor any reference). So we had to come up with our own metric determining the reproducibility in that case, in our own subjective terms. We concluded that the algorithm had to be able to be run by an average user with an average computational power within an reasonable time (in our case maximum 8 hours). In the terms of the aforementioned terms, having used the google Collaboratory with on the gpu, the total amount of time necessary to reflect the authors indications was about 2 hours, hence very reasonable time as for advanced CNN network for having run 100 epochs for every label in the learning procedure. So in this case, there were little problems, and as the authors claimed, their model is superb if comes to time cost, hence it is easy to retrieve results. 4.2.5.3 Randomness problems In general, model had little important parameters that would change its functionality, so if it comes to randomness they were only based only on the concept of neural networks and its randomly initialized principal values. Besides this fact, the model was highly deterministic, user could not turn in the parameter that would change its functionality. Also the instructions provided by the github, precisely, step by step, explained how the authors retrieved the results, used the code. In the terms of randomness, the model is unambiguous, clear. 4.2.5.4 No access to source codes Without deepening too much into the detail, all the source code was provided in the main repository and there were no problems of this kind of nature. However, the returned results were in a form of a long string of data, in its raw form. The authors didn’t supply with method of visualizing the data, that was used in the article in form of graphs. The ambiguous answer eventually was found here in the issues on the github, nonetheless we had to still come up with a way of mimicking the original visualization for the clarity and unifying the layout. Eventually the graphs that we will present, will be crafted from our made code. 4.2.5.5 Results The main part and the most crucial of our whole investigation. Having successfully run the program and retrieved the results came the time to compare our results with the articles’. The article provided its readers with few visualizations of the data and a comparison table. In this section we will have a closer look at each of them, comparing each one and providing a brief verdict. ROC score Our results analysis will start with the AUC curve and performance for the multiclass classification. Before any conclusions, let’s see the original results: And our visualization from the data we retrieved: In the case of acquiring the AUC data, we were almost fully successful in reproducing the article’s data. Our results are slightly worse than the article’s, nevertheless, the overall score is shown in the parentheses mark almost full success if comes to the score. Even the largest error is lesser than 2% of the relative value. Having said that, we are sure to confirm the results in this case. The average AUC for the original case was 0.995, and in our 0.990, hence the relative error is around 0.5%. Precision-recall curve As in the previous example, first, we will supply you with the original graph: And now with our results: In this case, there were more issues than in the ROC curve. The overall scores were more grievously worse than in the original, nonetheless, the trends in the outputs stayed the same; here for the tumor types of Stroma, Complex and Debris the score was significantly worse in both original and the reproduction, indicating the model keeps its trends, functionality. In this given case the average AUC score for the original data was 0.972 and in our case 0.852. Despite the more noticeable discrepancies, the overall output remained on a decent level, hence making it reproducible. Lastly, we present the accuracy table. First the data from the article. Method Method Type Problem Type Max. reported 10-fold ACC Max. reported 5-fold ACC Max. reported 2-fold ACC 10-fold AUC 5-fold AUC ARA-CNN CNN Binary 99.11 ± 0.97% 98.88 ± 0.52% 98.88 0.998 0.999 ARA-CNN CNN Multiclass 92.44 ± 0.81% 92.24 ± 0.82% 88.92 ± 1.95% 0.995 0.995 Compared with our results: Method Method Type Problem Type Max. reported 10-fold ACC Max. reported 5-fold ACC Max. reported 2-fold ACC 10-fold AUC 5-fold AUC ARA-CNN CNN Binary 0.90 ± 0.67% 89.52 ± 0.25% 0.87 0.92 0.95 ARA-CNN CNN Multiclass 0.8616 ± 0.81% 84.15 ± 0.42% 83.23 ± 2.55% 0.90 0.92 It is visible that they are not perfectly matched, nonetheless were close, and kept the same pattern of scores as in the original one, hence it is safe to assume success in this part of results reproduction. 4.2.6 Summary and conclusions With the results having been presented, we are able to give a final verdict on whether the article was reproducible. All mentioned issues were not insurmountable, the article in each separate category can be rated high or at least satisfactory. After examining reproducibility, we have identified the most important aspects that the article we worked on should have contained. Primarily, we faced the lack of code used to create the plots in the article. Authors should ensure easy access to all user codes. Secondly, a random seed should be provided for each randomized task. In our case, because of the similarity of the achieved results, it was not a big issue, but with many problems, it can be crucial. Moreover, another limitation was the hardware and time. We believe, that good practice should be use datasets and parameters that can be used by the average person without special hardware. However, it is obvious that in most cases this cannot be provided, but then such articles may be very difficult to reproduce. Coming back to the article we worked with, we are satisfied with the results obtained and we hope that all authors of future articles will care for reproducibility at least as much as the authors of the mentioned article. References Computing Machinery, A. for. (2018). Artifact review and badging. https://www.acm.org/publications/policies/artifact-review badging Łukasz Rączkowski, J. Z., Marcin Możejko. (2019). ARA: Accurate, reliable and active histopathological image classification framework with bayesian deep learning. Springer Nature, 14, 1–11. https://doi.org/10.1038/s41598-019-50587-1 Vandewalle, P., Kovacevic, J., &amp; Vetterli, M. (2009). Reproducible research in signal processing. IEEE Signal Processing Magazine, 26(3), 37–47. "],["rethinking-the-u-net-architecture-for-multimodal-biomedical-image-segmentation.html", "4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation", " 4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation Grudzień Adrianna, Łukaszyk Marcin, Piasecki Michał 4.3.1 Abstrac In our work we wanted to explore what reproducibility is, and how it applies to deep learning. The articles we had choesen are An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL) and MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation. In both papers we faced simmilar issues and limitation ,yet we have concluded our work with diffrent results for each article. First paper is unreproducible and second one is mostly reproducible. 4.3.2 What Reproducibility Is? Reproducibility of an experiment plays crucial role in scientific community and serves as a proof for validity of both experiment and its consequences. It constitutes a basis of scientific endeavours and enable us to use science as a method to comprehend reality around us. This essential term means ability to repeat particular experiment and obtain similar results, while having the same conditions, data, tools and means of measurement. Unfortunately, it is not always possible to repeat experiments and conditions required, them might prevent larger community to conduct them. To show these difficulties, we would like to provide two examples, which clearly illustrate, that some experiments might be almost impossible to repeat. Firstly, let’s consider repeating Galileo experiments of dropping objects from fixed height to establish, whether objects fall with the same velocity. Such simple test is available for everyone, who is eager to explore experimentally basic Newtonian principles. On the other hand, if we would like to switch to quantum physics and prove experimentally quantum nonlocality, we are more than likely to be unable to conduct experiments proving this theory. The infrastructure required to conduct such experiments makes reproducibility an exclusive idea limited to only handful of people. It is hard to underestimate the significance of reproducibility in science. We listed below main reasons for its importance: 1 Proof for correctness of experiment Having obtained same results of an experiment conducted many times by the independent researchers, we are ensured that results are not coincidence. As a result, we can start treating new results as a proper description of phenomena around us. 2 Different views on experiment After repeating an experiment and acquiring same results, other researchers may draw different, interesting conclusions. It enables to use conducted experiment to full extent, squeezing out as many conclusions about reality as it is possible. 3 Possibility of improving experiment Other researchers may figure out how to improve experiment to obtain results in more efficient or precise way. 4.3.3 First article (An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL) ) 4.3.3.1 Summary of Article The main purpose of the article is creating combinations of different neural networks and finding the best model. In order to achieve these goal, authors use various optimalization methods , dropout layers and different models for prediction. Authors describe methods used in article: TF-IDF, Word2Vec for Feature Extraction, NaïveBayesClassifier, SVM and S2GD for classification and types of neural networks : Deep Neural Network, Convolutional Neural Network and Recurrent Neural Network. They explain all algorithms and ideas used in their work. Consequently, they combine different architectures to obtain one model – RMDL. Having constructed the model, authors decide to evaluate their masterpiece on different datasets. They use it both on text data (WOS, Reuters IMDB, 20newsgroup) and image data (MNIST, Cifar-10). Authors evaluate model with metrics like : Precision, Recall F1 and Score. The all model is build with usage of Cuda, tensorflow and keras libraries. At the end, authors discuss how its model might come useful in classification task. 4.3.3.2 Our Work with Reproducibility of First Article We had run all our experiments in google Collaboratory. One of the datasets was not available due to unavailability of server that article was referring to. Rest of datasets were freely accessible. Most of our issues were due to hardware limitations. The predominant ones were RAM lack of free memory errors and memory leaks. Collaboratory doesn’t allow to run program more than 12 hours straight so we couldn’t train the biggest models. Our main goal was to explore reproducibility of given paper, and main issue that we combined were lack of definitions of terms that authors used. One of them was error rate that wasn’t defined. They didn’t specify what depth of RMDL is and a lot of our work was to guess based of their results what is the proper definition. 4.3.3.3 Reproducibility of this Article We cannot acknowledge it as reproducible. The results that were achieved by authors are not properly documented and defined in article making it hard to check if we had similar results. Some of our struggle results from the hardware limitations where we cannot train similar sized models or in extreme situations cannot train at all. 4.3.4 Second article (MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation) 4.3.4.1 Introduction Throughout XX century we have developed multiple methods for imaging human bodies. Radiography, functional magnetic resonance and fluoroscopia are just mere examples of methods we invented. It is hard to underestimate its benefits, they enable us to understand processes inside our bodies to much greater extent. Thanks to them, it is possible to detect pathologies and monitor our health at scale not imaginable before. Having been exposed to such vast amount of medical images, computer scientists started thinking about creating software, which could analyse them automatically. Since late 60’s researchers have been attempting to build models segmenting medical images and conducting diagnosis. Were we to create such software, we would relieve doctors from segmenting images manually and support them with independent diagnosis from computer. As a result, it would make our healthcare system less overwhelmed and more people could seek help in predicament. 4.3.4.2 Summary There were multiple ways of creating such models. Firstly, ‘rule-based’ approach was commonplace, with scientists explicitly setting rules for model evaluation. Poor generalization resulted in different approaches based on geometrical analysis or fuzzy logic. Nonetheless, recently we can notice general shift towards Deep Learning as a way of segmenting and diagnosing images. Convolutional neural networks have been obtaining outstanding results, with U-Net architecture standing out of the crowd. We would like to review article “MultiResUNet Rethinking the U-Net Architecture for biomedical image segmentation.” In this article, modification of U-Net – MultiResUNet is proposed. Authors are convinced that small changes in well know U-Net architecture lead to better model performance. Authors list situations, in which U-Net architecture do not get sufficient results. 4.3.4.3 What is U-Net? U-net is a deep learning architecture that was proposed in 2015. It’s a modification of simple neural network with odd number layers of convolution layers and a max-pooling or up-convolutions layers, which is used to for segmentation of images. This modification is a series of connections between each simmilar distanced layer from middle one. This “bends” our network making it U-shaped, thus the name. Each level of network consists of series of convolution layers and then max-pool or up-convolution layer that depends on which side of and U shape are we in. Figure 4.1: Example of U-net In order to improve U-Net , authors decide to introduce following changes in model architecture. They factorize the bigger and more expensive 5 × 5 and 7 × 7 filters as a succession of 3 × 3 filters. This saves computations as we need calculate 3 3x3 convolutions layers to have similar results as calculating 3x3, 5x5 and 7x7 layers. They concatenate all of the estimated layers with one made from 1x1 convolution layers. This allows to better retrieve spatial features from different scales. Figure 4.2: Example of ResBlock The excellence of U-Net architecture stems mainly from the introduction of connection between layers, which enables to save spatial information while going deeper into U-Net architecture. However, authors speculate that combination of a simple copy of an output of previous convolutional layer and information received after all transformations conducted in model results in discrepancy in information carried by both of them. To minimize this discrepancy authors suggest “ResPath” - series of consecutive convolutional layers. They decrease number of convolutional layers while going deeper into the architecture, due to smaller difference in information in “depth” of the model. Figure 4.3: Example of ResPath In the end we get simmilar looking neural network but with improvements in most places. Authors states that their structure has simmilar computational complexity but gives better results. Figure 4.4: MultiResUNet Authors test their improved architecture on 5 different datasets and compare results with “basic” U-Net. They introduce new metric: Jacard Index to measure performance of both models. For mask A and model’s marked area B it calculates the intersection of both sets divided by their union. Authors show that their model obtain better results, learns faster and is less vulnerable to perturbations. 4.3.4.4 Our Work with Reproducibility of Second Article We decided to repeat experiments in Google Colab using Python Unfortunately,we were not able to obtain all datasets used by authors. After long research, we managed to download datasets: Fluorescence microscopy images Murphy Lab, Electron microscopy images, Dermoscopy images IC-2017dataset Having downloaded datasets, we encountered many problems. There were multiple problems with transforming datasets into desired format used by authors. Problems with conversion, partition of datasets and ambiguous files led to frustration. Moreover, Google Colab, environment in which we tried to reproduce results, had not enough RAM to smoothly generate results. At the end, we managed to obtain some results, which were similar to results from the article. [Wykresy dermoskopi i endoskopi] (#fig:Original Rsults)Orginal Plots (#fig:Our Rsults)Our Plots 4.3.4.5 Reproducibility of this article Article is reproducible. In article authors try to improve U-Net architecture. Providing simple explanations and empirical data, they explain motivation behind their improvements. Unfortunately, few of datasets used by authors are no longer available, which may make it harder to conduct same experiments. However, rest of the datasets is easily accessible, so it is still possible to verify results obtained by authors. When it comes to authors argumentation behind new architecture, we are not completely satisfied. As students, who have little expertise in using Machine Learning in medical areas, we are not satisfied with mathematical explanations provided by the authors. No mathematical proofs and small number of datasets make us feel unease, if authors’ architecture outperforms basic U-Net architecture globally. 4.3.5 Conclusion In both articles we coudn’t simply state if article are reproducible. We had to have non-binary measure for each. First article is mostly unreproducible due to lack of proper deffinitions thath dosn’t allow us to be sure thath our experiments are conducted in the same environment. Second article is reproducible. Our tries of recreating simmilar results were mostly positive and most of our issues were with hardwere limitations. The biggest truble is some missing data stes thath are not longer available from refer to sources. Some of our worry is in how authors explained their reasoning as it’s mostly based on assumption and results. "],["dl2-rmdl-unet.html", "4.4 The reproducibility analysis of articles covering RMDL and UNet++ architectures churns", " 4.4 The reproducibility analysis of articles covering RMDL and UNet++ architectures churns Authors: Marceli Korbin, Szymon Szmajdziński, Paweł Wojciechowski (Warsaw University of Techcnology) 4.4.1 Background The main subjects of our report are reproducibility of scientific papers and a brief analysis of papers titled RMDL: Random multimodel deep learning for classification and U-Net++: A Nested U-Net Architecture for Medical Image Segmentation. Nowadays we are facing a reproducibility crisis. A vast part of scientific papers are hard, if possible, to reproduce. Therefore we are convinced it is important to talk about this issue, especially today. Reproducibility is an ability to be recreated or copied. In other words, the main goal of reproducibility is to obtain as similar results as possible to those in a paper, by using a method described in this paper. As it turns out, it’s not that easy to achieve. Reproducibility is an essential aspect of a scientific paper for several reasons. The first, and in our opinion the most important reason, is the insurance of correctness of the results. By just reading a paper we cannot be sure about accuracy of the results. There is always a chance of a mistake made by researcher. Moreover, machine learning models are usually at least partly random (dividing data into training and test sets, choosing default parameters). It is possible then that promising results are in some point a consequence of a coincidence. By reproducing the paper we increase the reliability of its results. The second advantage of reproducibility is transparency. There is a chance, albeit small, of manipulating data by a researcher to achieve better results. Reproducting a paper can ensure us that such an unethical incident has not taken place. Lastly, reproducing a paper can definitely help us better understand a subject. Through running a code by oneself, one can find some lines which are unclear to them and try to understand them. 4.4.2 Random Multimodel Deep Learning for Classification The first article we discuss is RMDL: Random multimodel deep learning for classification (4-1-rmdl?), which expounds RMDL – a voting ensemble technique which uses randomly generated deep neural networks with various architectures. The idea of such approach is to benefit from the advantages of each neural network architecture used. Therefore RMDL is versatile and suitable for various types of classification problems such as text, video or image classification. In the article, the model’s text and image classification performance is presented on the examples of popular data sets: MNIST, CIFAR-10, WOS, Reuters, IMDB and 20newsgrou. RMDL generates multiple neural network models based on three architectures: Multi-Layer Perceptron, Convolutional Neural Network and Recurrent Neural Network (more precisely, RNN with LSTM and GRU units). The number of generated Random Deep Learning models (RDL) is explicitly defined with a parameter. The number of layers and nodes for all of these RDL models are generated randomly and all the models are trained in parallel. In text feature extractions, RMDL uses word embedding, TF-IDF and n-grams, while in text classification examples, word vectorization techniques (GloVe) are used. Every RDL has an output layer that uses softmax to determine the prediction of class. The final prediction from all of the RDL models is determined through the use of majority vote. During the optimization, RMDL uses stochastic gradient optimizers – in examples present in the paper, RMSProp and Adam optimizer were used. Each generated RDL model can use different optimizers. Therefore, if several generated models do not provide a good fit, they can be ignored for the time of voting. Each of the used architectures, feature engineering techniques or optimizers are described briefly. Presented results show that RMDL performed better than baseline models for each exemplary data set. On Google Scholar this paper has 81 citations, which indicates mediocre relevance. Such a solution is neither commonly used nor quoted. 4.4.2.1 Results Each of us conducted experiments to check if we would be able to obtain the same results as the ones present in the paper. First thing we noticed was that the process of training those models consumed a lot of time, therefore we decided to focus on some of the datasets. We chose IMBD, 20NewsGroup, two Web of Science datasets (WOS-5736, WOS-11967) and MNIST. Tables below show our results. Model size IMDB 20NewsGroup Paper’s result Reproduction’s result Paper’s result Reproduction’s result 3 RDLs 89.91 89.39 86.73 82.46 9 RDLs 90.13 89.32 87.62 85.77 15 RDLs 90.79 89.54 87.91 85.94 Table 1: Accuracy for text datasets. Model size WOS-5736 WOS-11967 Paper’s result Reproduction’s result Paper’s result Reproduction’s result 3 RDLs 90.86 87.10 87.39 78.15 9 RDLs 92.60 91.02 90.65 84.88 15 RDLs 92.66 89.98 - - Table 2: Accuracy for Web of Science text datasets. Model size Papers’ result Reproduction’s result 3 RDLs 0.51 1.37 9 RDLs 0.41 0.67 Table 3: Loss for MNIST dataset. As we can see, in all experiments we obtained worse results than those in the paper. In most cases the difference is not substantial, although for 3RDLs for WOS-11967 it achieves 9.23 p.p. As the reasons thereof we assume randomness and, less definitely, differences in hardware. Nevertheless, accuracy of our models was still relatively high. Some problems occurred during our experiments. The first one and the hardest was the insufficient amount of processing power. At first RNN models were considerably difficult to train, since default parameters didn’t allow for the use of GPU. In turn each epoch took several minutes for the model to be train. We managed to fix that issue by modifying parameters of RNNs models and eventually we could to train RNNs models in a reasonable amount of time. While we were trying to conduct experiments on Web of Science datasets, it turned out that the website, which was supposed to contain data available to download, did not exist anymore. We managed to find those data using other sources, download it and load manually. Another difficulty was insufficient RAM size, which prevented us from checking WOS-46985 results. The data was too large and the only output of our code covering this part consisted of error messages. 4.4.3 A Nested U-Net Architecture for Medical Image Segmentation The second article to be analyzed is UNet++: A Nested U-Net Architecture for Medical Image Segmentation (Z. Zhou et al. 2019), which presents an architecture based on encoder-decoder networks and skip pathways between them, meant to reduce the distance between the components. Such a solution is claimed to simplify learning tasks in case of semantically similar feature maps of the decoder and encoder networks. The architecture is evaluated in medical image segmentation tasks, involving segmentation of lung nodule, cell nuclei, liver and colon polyp in images. UNet++ consists of an encoder and decoder, connected through dense convolutional blocks and skip pathways between them with additional layers. After data enters the encoder, a feature map is processed in a block, whose number of layers is dependent on the network level. The skip pathways in blocks are meant to shorten the distance between semantic levels of the feature maps in the encoder and decoder, improve gradient flow and make the overall optimization process easier. Another important feature of UNet++ is optional deep supervision, which works in either of two modes: accurate mode and fast mode. It serves to enable model pruning and is based on building full resolution feature maps at chosen semantic levels of the network. All the elements are illustrated on figures and described as means to an improvement of previously implemented U-Net and wide U-Net architectures. The three models are then compared in a series of experiments, which prove an outperformance of UNet++ over the other two architectures on each dataset. 4.4.3.1 Problems The second experiment brought us some problems as well; the first of them was the downloading of necessary data. Authors of the original article shared some links to datasets, however not all of them were public. In several cases a special permission was needed – we sent some requests, but did not receive all the permissions. Eventually, we used three datasets in our experiements, which contain pictures of: Lungs, Nuclei and Polyp. Our next problem to overcome was data preprocessing. There were issues on the GitHub profile concerning preprocessing, on which the authors did not give any instructions. This made us decide to improvise. The Nuclei and Polyp datasets were the only ones we were able to preprocess, while the Lung dataset turned out too complicated for preparation. The problem with this dataset was its size, which prevented us from loading the whole set at once. Moreover, the photos therein were three-dimentional. After we have managed to preprecess data, we started to work on the code, which was not simple either. The authors were using the older versions of several libraries with many errors. At first we tried to modify authors’ code to work on the newer versions of libraries. The procedure failed due to complexity of that task, yet we managed to overcome it by exchanging the newer versions in place of the older ones. We also had to figure out how to make use of GPU, which demanded us to install the correct versions of libraries. Despite GPU being enabled, the learning process still consumed a considerably high amount of time. 4.4.3.2 Results We were eventually ready to train our models and try to reconstruct the authors’ results. The authors sugested two metrics to evaluate effectiveness of a model: - Dice coefficient - overlap area multiply by two and divided by the total number of pixels in both photos; - Intersection over Union (IoU) - area of overlap divided by area of union. Metric Nuclei Polyp Paper’s result Reproduction’s result Paper’s result Reproduction’s result Iou 94.00 85.74 33.45 24.59 Dice 95.80 79.88 - 34.27 Table 4: Dice and IoU measures for Nuclei and Polyp datasets. Both metrics describe how similar the two pictures are. We conducted two experiements on the Nuclei and Polyp datasets. Results turned out to be fairly similar, in spite of all of the reproducted scores being lower. The highest difference occurred between dice coefficients on the Nuclei dataset. According to us, the source of these differences is an insufficient amount of the processing power. We were unable to set the same batch size or epochs number the authors did. In our opinion, optimizing those parameters would have improved our results. Despite the differences, we consider the results of the article to be reproducible. 4.4.4 Conclusions Reproduction may be considered as simply using a premade code, so it bears no difficulties. We assumed the same, however the whole process turned out to be much more complicated. During this procedure there is a high probability for several troubles to occur. These did happen to us with the datasets of both articles we worked with. We consider sharing datasets with code by authors as a good solution to the problem. Likewise, data preprocessing is problematic. In our opinion it is crucial to describe this process; otherwise the reproduction is more difficult, if possible at all. Another problem to have occured in both articles is processing power. Although it is not authors’ fault, it remains very important during the reproduction of an article. Contacting an author might solve at least a part of these problems. The ability to ask about a specific method or to consult a line of code is a simple way of clarifying things. That said, authors seldom do that; there were many issues on Github concerning these articles, of which few were answered. References Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., &amp; Liang, J. (2019). UNet++: Redesigning skip connections to exploit multiscale features in image segmentation. IEEE Transactions on Medical Imaging. "],["can-you-trust-science-on-reproduction-in-deep-learning-.html", "4.5 Can you trust science? On Reproduction in Deep Learning.", " 4.5 Can you trust science? On Reproduction in Deep Learning. Authors: Filip Chrzuszcz, Szymon Rećko, Mateusz Sperkowski (Warsaw University of Technology) 4.5.1 Abstract One of the most important aspects of machine learning scientific articles is reproducibility. Obtaining results similar to those from the paper, using the same codes and data sets, allows for verification and evaluation of the reliability of the research. In our work, we try to reproduce the results of two deep learning articles using the code and information provided by their authors. We will present the problems with reproduction that we encountered, their reasons and the action we had to take to solve them. The results of our experiments show that despite the lack of information or computational resources, it is possible to obtain sufficiently similar results to those presented in the articles. 4.5.2 Introduction To increase the credibility of published scientific literature, researchers should increase the credibility and effectiveness of their research (Casadevall and Fang 2010). Reproducibility in the field of deep learning is the feature of scientific research, which allows for independent repetition of the experiment and obtaining similar results using the code of the original creators of the experiment (Pineau et al. 2020). To increase it, articles should contain as much information as possible about datasets, models and their parameters that will help others to recreate them. This includes more researchers who will be able to continue their work in a given field. Unfortunately, often even basic reproduction is not possible. The purpose of this article is to recreate two articles on Random Multimodel Deep Learning (Kowsari et al. 2018) and Adversarial attacks against medical deep learning systems (Finlayson et al. 2018), respectively, and to identify reproducibility problems occurring in them. 4.5.3 Methodology To study the reproducibility of the original publications, we used the author codes publicly available on GitHub. Each of the following subsections will address further important aspects of reproduction and, depending on the article, the numerous problems we encountered. 4.5.3.1 Models As Deep Learning evolves and extensive scientific research happens, many new architectures are constantly introduced such as published in 2017 Transformer Neural Network (Vaswani et al. 2017), which revolutionized the Natural Language Processing (NLP) field of machine learning. Most common architectures include Deep Neural Networks, Recurrent Neural Networks and Convolutional Neural Networks (further as DNN, RNN, CNN respectively) (Du et al. 2016),(Suzuki 2017). 4.5.3.1.1 RMDL This paper introduces a new approach to deep learning models. Authors propose an ensemble, named Random Multimodel Deep Learning (RMDL). It consists of many randomized deep learning models, referred to as Random Deep Learning (RDL). Only the extent of randomization is controlled by the hyperparameters. The resulting ensemble consists of units with different architecture, which according to the authors leads to increased robustness of the model. Our reproduction covers the experiments on the classification tasks of images and text. The same or similar model can be expanded to many more fields of machine learning. 4.5.3.1.2 Adversarial medicine This research paper addresses the topic of influencing the performance of a machine learning model. Unlike the first RMDL project, where the topics the authors dealt with were quite broad, here we have a thorough consideration of one particular aspect. Namely, the authors talk about specific types of attacks on an already trained architecture for predicting whether a particular disease is present in a given medical image. Our reproduction dealt with training the models proposed by the authors and then attacking them with prepared scripts whose task was to flip the labels predicted by the model. Additionally, this had to be done in a way that was completely indistinguishable to the human eye, i.e., the images had to remain as similar to the original photo as possible. 4.5.3.2 Datasets The most important part of supervised machine learning is the dataset. Data scientists spend the majority of development time on data cleaning (Chu et al. 2016). The basis for the reproduction of deep learning models is that the sets for training and testing come from the same dataset used by the models in the article. Even small differences in data sets can affect the results and the reliability of reproduction. 4.5.3.2.1 RMDL (Kowsari et al. 2018) in the original paper is used as a classification model. For comparison with the original results, this reproduction covers the same datasets as (Kowsari et al. 2018) did. All of the datasets used are well known pre-cleaned data, so the majority of preprocessing is already done. Reproduction attempts on 6 different text classification datasets were made. More specifically: WOS-5736, WOS-11967, WOS-46985, which are datasets published (Kowsari et al. 2017) by authors of the original paper and they cover classification of scientific abstracts (Web Of Science). The number in the dataset name refers to the number of documents in the corpora. The next NLP dataset is Reuters-21578 containing 10 788 documents on topics of business/economics from Reuters newswire service. The next two datasets were IMDB, for classification of movie reviews, and 20NewsGroup for topic classification of newsgroups posts. Respectively they contain 50 000 reviews and 19 997 news posts. Newsgroups can be interpreted as discussion groups. For image classifications, two datasets were used. The first one is MNIST containing 70 000 handwritten greyscale digits in a 28x28x1 pixels format. The second one is CIFAR-10, which consist of 60 000 images of 10 classes with common objects such as aeroplane, automobile, cat. This dataset images are in format 32x32x3, which implies RGB photos. 4.5.3.2.2 Adversarial medicine The authors add 3 types of medical images to their models. These are lung, eyeball, and skin photos. These photos do or do not include pneumothorax, melanoma, and diabetic retinopathy depending on the class. A total of about 20,000 images are available to analyze the results provided. To train the model, additional data must be downloaded from open-source sources to then use a ready-made script to train the model. The data for testing has been prepared by the authors and is available in a conveniently downloadable form as tables in .npy format. The images are 224 by 224 pixels in colour. It is worth noting that even though these images have been provided there are some problems with reproducing their display in the code, as there are often no images with the indices designated by the authors, which is a bit of a problem. Additionally, the scripts include relative paths, to the authors’ computers, which obviously cannot be easily reproduced, requiring additional work. 4.5.3.3 Hyperparameters Hyperparameters can have a significant impact on the training and performance of a model, therefore they are an important component of the reproduction process of any article in which they occur. 4.5.3.3.1 RMDL The original papers introduce randomization of hyperparameters in the elements of the ensemble, as a feature to increase the robustness of the ensemble. In the code/library provided by authors on the GitHub linked to the paper, hyperparameters differ from regular neural networks. The most important factors are the numbers of RDL’s used in the model. When referring to RDL’s in terms of numeric order, interpret them in increasing order, starting from 1 first DNNs, then RNNs, and finally CNNs. Barely any information on the original paper’s experiment is available. Therefore in the reproduction experiments, whenever possible, the used hyperparameters were the same as in the original author’s code. Although it is unsure whether the code provided is the one used to generate the article results, as such basic things as plots differ in the library provided by them. In the paper, randomization of the optimizer used in an RDL is proposed to improve robustness. Authors argue that if one optimizer doesn’t perform well on a certain dataset, other RDL’s will have different ones which may score higher. Despite that argument, this option is not used by the authors and therefore not used in reproduction. 4.5.3.3.2 Adversarial medicine The scripts provided for training the models as well as for the attacks contain rigidly set hyperparameters, and the articles and code comments themselves lack any comments about changing them or adjusting them to fit the data. For the prediction of image labels, the well-known and widely available Resnet50 and InceptionResNet v2 models are used. As for the scripts intended for attacks on the model infrastructure, the number of comments contained therein allows only a guess as to what they can improve or worsen individual parameters. The situation is slightly better in the case of parameters responsible for training the network itself. There are available and quite well-described parameters such as the rate of learning, or the size of the batch, on which the network is to learn. However, the article itself does not mention in any way to change the values of these parameters, so during our reproduction, they remained at the same level. 4.5.3.4 Evaluation Choosing proper evaluation metrics for the problem is a crucial point in classification tasks. Many different metrics were introduced to fit specific situations. For example, in testing for a contagious disease, Recall (1) might be more important than Accuracy (2). \\(Recall\\) = \\(\\frac{Number\\;of\\;Predicted\\; Positives}{Number\\;of\\; True\\; Positives}\\) \\((1)\\) 4.5.3.4.1 RMDL In reproduction experiments, the same metrics as in the original paper were used. This way, the results achieved can be compared. For NLP tasks accuracy score (2) is used. For image classification tasks error rate (3) (Z.-H. Zhou and Feng 2017) is used. \\(Accuracy\\) = \\(\\frac{Number\\;of\\;Correct\\; Predictions}{Number\\;of\\;Total\\; Predictions}\\) \\((2)\\) \\(Error\\) \\(Rate\\) = \\(1-Accuracy\\) \\((3)\\) 4.5.3.4.2 Adversarial medicine On the reproduction of the second article, it is hard to talk about the metrics used by the authors, as most of the attacks took place on single images, where the probability of belonging to a particular class was analyzed. The only measure other than probability was the AUC value. This is the area under the ROC curve that results from comparing different True Positive Rate and False Negative Rate values over different thresholds. This measure is used to analyze the performance of the network on the authors’ basic unaltered images. 4.5.3.5 Computational environment Because of this, how computationally exhaustive this task is, most of the research on deep learning is done with incomparably more resources than we could afford. (Thompson et al. 2020) Due to these constraints, we had to make concessions to get as many results as possible. 4.5.3.5.1 RMDL In the case of RMDL, Google Colaboratory (Bisong 2019) was used for all experiments. Code was written in Python, using the library RMDL version 1.0.8, provided by the authors of the original paper. Our experiments resemble the examples for the datasets, provided by the authors, with some changes. Corrections in code to repair errors, manual download of data for WOS datasets were introduced to run the code. Additionally, due to limited resources in the Collaboratory (RAM and processing time), limits on some of the experiments were set, described further. Unfortunately on the IMDB and 20NewsGroup datasets, no significant results were achieved due to computational limits, even after setting rigorous limits on hyperparameters to decrease computational resources. 4.5.3.5.2 Adversarial medicine In terms of the equipment required, it was quite a burden to have to load all of the provided boards and photos into the computer memory. Due to their weight, there were some problems with the stability of the code. The whole code can be run using the local Python environment. The attacks themselves were not too computationally complicated, although the training of the network was quite a complicated task due to the high computational requirements of this training. Additionally, small corrections in the code were necessary, although they were not too significant and usually resulted from different versions of packages, rather than the carelessness of the authors. 4.5.4 Result 4.5.4.1 Reproduced Results While achieving slightly lower scores than the papers proposed, the reproductions support the main claims of both original papers. The original results in comparison to the reproduced results can be found in the Tables below. In the case of (Kowsari et al. 2018), tables show competing or higher accuracy scores than ‘state of the art’ baseline models in classification tasks on a variety of datasets, as proposed by the paper. In the case of (Finlayson et al. 2018), the table shows that the attacks correctly change the classification of the model while being unrecognizable by the untrained in this task, the human eye. 4.5.4.2 Random Multimodel Deep Learning for Classification The main result of the paper is achieved, as the networks score results either similar to the best of the baselines or higher than any of them. Comparison between these values isn’t straightforward, as the models consist of different architectures. Randomization of hyperparameters in the units of the ensemble is a crucial point, which according to the original paper, improves the robustness of the ensemble. In the table below scores on the left (Paper) are taken directly from the original paper, while the scores on the right (Repr.) represent successful attempts at reproducing the corresponding RMDL ensemble. Lacking scores (marked as three dashes) are discussed in section Methods. Only in WOS-5736 3 RDLs model, the same architecture as in the original paper is shown, consisting of 1 DNN, 1 RNN and 1 CNN. In every other reproduction model, half of RDLs were DNNs and half were CNNs, caused by computational limits. Only in Reuters 3 RDLs model, the additional RDL was assigned to CNN, in every other model it was assigned to DNN. .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset WOS-5736 WOS-11967 WOS-46985 Reuters-21578 Score Source Paper Repr. Paper Repr. Paper Repr. Paper Repr. RMDL 3 RDLs 90.86 89.37 87.39 84.25 78.39 — 89.10 87.64 9 RDLs 92.60 89.28 90.65 — 81.92 — 90.36 89.83 15 RDLs 92.66 — 91.01 — 81.86 — 89.91 — 30 RDLs 93.57 — 91.59 — 82.42 — 90.69 — Table 1: RMDL reproduction accuracy scores in comparison to the paper, on four NLP datasets. Figure 1 shows the comparison of accuracy score through epochs on the Reuters-21578 dataset. Taking into account the different scales on test plots, the reproduction model closely resembles the original model change through epochs. In the ensemble architecture, low scoring models (in this case RDL 3 and 5 in reproduction) still increase overall accuracy. Paper’s Plots Our Reproduction Figure 1: Accuracy scores on the train and test sets for Reuters-21578 dataset. In the plots of the paper, the model consisted of 3 DNNs, 3 RNNs and 3 CNNs. In the reproduction, 5 DNNs and 4 CNNs. As this table below shows, the results achieved during reproduction attempts on the MNIST dataset are slightly worse than those presented in the article. Both the 3 RDLs and 9 RDLs models on this dataset have the same architecture as those presented in the article. The difference in the results may be since during reproduction the models have trained 100 epochs, which is 20 less than in the article. The results of the reproduction of the CIFAR-10 dataset presented in the table are significantly worse than the results presented by the authors. The reason for this is not the difference in architectures. Rather, we should look for it in the fact that due to the previously mentioned problems, the training of models on this dataset has been reduced from 200 epochs to 100 epochs. However, as we’ll see in Figure 2, that is not the point. .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset MNIST CIFAR-10 Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 0.51 0.55 9.89 38.23 9 RDLs 0.41 0.65 9.1 36.91 15 RDLs 0.21 — 8.74 — 30 RDLs 0.18 — 8.79 — Table 2: RMDL reproduction error rate in comparison to the papers result, on two image classfication datasets. Figure 2 shows the comparison of loss through epochs on the CIFAR-10 dataset. Looking at the growing loss on the test set, while at the same time the loss on the training set was decreasing, it appears that most of the RDLs overfitted, which resulted in a deterioration of the final score. Paper’s Plots Our Reproduction Figure 2: Error rate on the train and test sets for CIFAR-10 dataset. In the plots of the paper, the model consisted of 3 DNNs, 3 RNNs and 3 CNNs. In the reproduction, 4 DNNs, 3 RNNs and 2 CNNs. 4.5.4.3 Adversarial medicine Referring to the results achieved is not a simple matter. As we mentioned earlier, almost all available results both in the authors’ code and ours are in the form of probabilities of qualifying a particular image to a given class. So as far as we can talk about success in the case of analyzing one particular photo, it is hard to talk about reproducing this result when using other photos. It is worth saying, that the model trained by us on basic data obtained similarly good results as the one used by authors. Paper’s Plots Our Reproduction Figure 3: ROC curves on training sets. As seen in Figure 3, we were able to achieve very similar results in training the initial networks as the authors. This shows that the script responsible for this part of the work was well prepared, and indeed, this part gave us relatively the least problems, compared to the other reproduction issues of this paper. .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Pneumothorax Melanoma Retinopathy Paper Repr. Paper Repr. Paper Repr. Before Attack 0.997 0.981 0.99 0.994 0.994 0.98 After Attack 8e-11 2e-11 1e-11 2e-11 3e-12 2e-10 Table 3: Adversarial Attack on lung photo The table above lists the probabilities of adequately evaluating a photograph containing a pneumothorax. The results vary minimally, however, a very important fact, namely the reversal of the prediction of the data by the model was achieved by us, which is a great success. Similar results were achieved for the other two diseases, which indicates that these attacks are relatively well performed. Figure 3: Images of pneumothorax and noise which distracts model. In the images shown above, you can see exactly the idea of the whole article. The first and second images are not different to the human eye, and the algorithm considers them completely different and gives different predictions for them on whether the lungs have pneumothorax or not. In the third image, you can see the noise that causes this prediction swap. The authors prepared two types of attacks on the developed predictions. The first one named PGD attack takes advantage of the possibility to access the model weights and performs, so to speak, the reverse process to model learning, i.e., it tries to increase the error as much as possible. With this part of the project, we had no problem, because the code was prepared without any problem. Attacks were firing without any problems, and all the images were displaying correctly. The second approach named Patch attack was to introduce noise into the learning process by changing the weights using the process of maximizing the probability of returning an incorrect label provided the noise was introduced to a particular pixel. In the case of these attacks, the problem was that the repository was inadequately prepared by the authors, as some images and labels were missing, making it very difficult to replicate exactly what was described in the paper. Some images were able to be displayed correctly, but some were not. .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Accuracy Patch Attack Paper Repr. 0.00 0.125 Table 4: Accuracy of Second Attack on 8 samples of lung photos .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} AUC Patch Attack Paper Repr. 0.00 0.00 Table 5: AUC of Second Attack on 8 samples of lung photos As can be seen in the tables above, despite the problems previously described, we were able to reproduce attacks on the label predicate infrastructure. The AUC and accuracy metrics are at low levels, the same as the case presented in the authors’ code. 4.5.5 Discussion 4.5.5.1 What was easy? To summarize our work, we can certainly say that it was easy to reproduce simple models, especially if they were correctly prepared by the authors. In both the RMDL and Adversarial Medicine projects, there happened to be networks or attacks that did not need any changes in the code and worked practically immediately. In the case of both articles, communication with the authors was not necessary for the code to work, however, based on the fact that the authors of both projects regularly respond to issues on Github, we can conclude that it is not a problem. 4.5.5.2 What was hard? Unfortunately, things were not as easy as we would have liked. In the case of the first article, a significant problem was the lack of adequate computer resources, which at extreme moments made it impossible to work with the entire project, and thus verify the results provided by the authors. In the second project, it was also not possible to completely verify the thesis of the authors, however in this case it was connected with not completely well prepared repository, where all the files were not available. However, the code itself worked without problems, and there were no computer problems here either. References Bisong, E. (2019). Google colaboratory. In Building machine learning and deep learning models on google cloud platform (pp. 59–64). Springer. Casadevall, A., &amp; Fang, F. C. (2010). Reproducible science. Infection and Immunity, 78(12), 4972–4975. https://doi.org/10.1128/IAI.00908-10 Chu, X., Ilyas, I. F., Krishnan, S., &amp; Wang, J. (2016). Data cleaning: Overview and emerging challenges. In Proceedings of the 2016 international conference on management of data (pp. 2201–2206). Du, X., Cai, Y., Wang, S., &amp; Zhang, L. (2016). Overview of deep learning. In 2016 31st youth academic annual conference of chinese association of automation (YAC) (pp. 159–164). IEEE. Finlayson, S. G., Chung, H. W., Kohane, I. S., &amp; Beam, A. L. (2018). Adversarial attacks against medical deep learning systems. arXiv preprint arXiv:1804.05296. Kowsari, K., Brown, D. E., Heidarysafa, M., Jafari Meimandi, K., Gerber, M. S. and, &amp; Barnes, L. E. (2017). HDLTex: Hierarchical deep learning for text classification. In Machine learning and applications (ICMLA), 2017 16th IEEE international conference on. IEEE. Kowsari, K., Heidarysafa, M., Brown, D. E., Meimandi, K. J., &amp; Barnes, L. E. (2018). Rmdl: Random multimodel deep learning for classification. In Proceedings of the 2nd international conference on information system and data mining (pp. 19–28). Pineau, J., Vincent-Lamarre, P., Sinha, K., Larivière, V., Beygelzimer, A., d’Alché-Buc, F., et al. (2020). Improving reproducibility in machine learning research (A report from the NeurIPS 2019 reproducibility program). CoRR, abs/2003.12206. https://arxiv.org/abs/2003.12206 Suzuki, K. (2017). Overview of deep learning in medical imaging. Radiological physics and technology, 10(3), 257–273. Thompson, N. C., Greenewald, K., Lee, K., &amp; Manso, G. F. (2020). The computational limits of deep learning. arXiv preprint arXiv:2007.05558. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762. Zhou, Z.-H., &amp; Feng, J. (2017). Deep forest: Towards an alternative to deep neural networks. CoRR, abs/1702.08835. http://arxiv.org/abs/1702.08835 "],["machine-learning-1.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following short papers introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: Validation and comparison of COVID-19 mortatility prediction models on multi-source data. Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier One model to fit them all: COVID-19 survival prediction using multinational data. Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases. Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness. Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk "],["validation-and-comparison-of-covid-19-mortality-prediction-models-on-multi-source-data.html", "5.1 Validation and comparison of COVID-19 mortality prediction models on multi-source data", " 5.1 Validation and comparison of COVID-19 mortality prediction models on multi-source data Authors: Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier 5.1.1 Abstract The work of (yan_et_al_2020?) from the first months of the COVID-19 pandemic laid the foundations for further research in the area of machine learning models that predict patients’ chances of survival. It was done by introducing a simple decision tree that in the opinion of the inventors could potentially support the cause. Since that time a few papers have emerged that touch upon the same case in which other researchers tested this decision tree on their datasets. Their findings that the original model is not suitable for patients from other countries than China appeared interesting to us and hence in the following paper we present results of our work which aim was to build models on each of the considered datasets as well as on all of them combined in order to find an universal approach for classification of patients from various countries. After testing various models such as XGBoost, Logistic Regression, SVM and Tabnet we came up with the conclusion that there is no one model for all of the datasets that includes only at most 4 crucial variables. 5.1.2 Introduction At the end of 2019, the novel coronavirus disease 2019 (COVID-19) pandemic broke out. In the next few months it quickly spread around the world. Daily cases were increasing exponentially. Although most of the cases involved mild symptoms, the healthcare system in many countries still became overloaded. Therefore, the physicians were in an urgent need of a quick system to predict how severe the state of a patient could get, especially if there is a risk of death or a need to be put at the ICU. A certain solution for this problem was using machine learning to create a mortality prediction model based on easily available biomarkers. One of the first models proposed in may 2020 by (yan_et_al_2020?) could predict mortality rates of the patients 10 days in advance with great accuracy. The article is considered the state-of-the-art in the COVID-19 machine learning. Some issues with this model appeared when it was used on data from different hospitals. When used on biomarkers from patients in France, Netherlands and the United States the model’s accuracy significantly dropped. In our article, we describe how the datasets differ, we analyze how (yan_et_al_2020?) model compares to newer model (Zheng et al. 2020a) which evaluates hospitalization priority for COVID-19 patients by validating it on different datasets and propose new model which could yield great results on both datasets. 5.1.3 Data description We have begun our work from analyzing article (yan_et_al_2020?), authors have created a model based on blood samples collected form 375 patients from the Wuhan region in the first quarter of 2020. Data from another 110 patients were treated as an additional test set. Dataset contains 81 variables, including 74 describing the blood tests results, but not all tests were performed on each patient. Scientists finally created a model based on three variables: lactate dehydrogenase, lymphocytes and high-sensitivity C-reactive protein, these are going to be crucial also in our work. The article (yan_et_al_2020?) received several replies, scientists from the USA, France and the Netherlands validated the model on datasets from hospitals from their home countries. The results pointed to the problem of rashly predicting death for patients who eventually survived Covid-19 infection. They also claimed that the (yan_et_al_2020?) model learning from most recently performed measurements was not an appropriate tool to prioritize ICU admissions and should therefore use results from first tests. The datasets from corresponding articles contained 3 previously highlighted features, in datasets from (Dupuis et al. 2021) and (Barish et al. 2021) there are multiple test results and other useful features such as age, but information about other blood components is not included. The data from (Barish et al. 2021) was especially helpful, the data was shared to us after consultation with one of the article’s authors. The data came from Northwell Health, New York State’s largest hospital network serving 11 million patients and contained 1038 records. Table 1: contains all most important information about each of datasets and shows which of them contains features needed to create models. Article Records Variables Lactate Dehyd. C-protein Lymphocytes Neutrophil Age (yan_et_al_2020?) 485 81 Yes Yes Yes Yes Yes (Barish et al. 2021) 1038 14 Yes Yes Yes No No (Quanjel, Holten, Gunst-van der Vliet, et al. 2021) 305 15 Yes Yes Yes No Yes (Dupuis et al. 2021) 178 43 Yes Yes Yes No Yes (Zheng et al. 2020a) 214 33 Yes Yes Yes Yes Yes Careful inspection of distributions of these key variables from various articles helps to understand problem with applying model form [china] on dataset from corresponding articles. Lactate dehydrogenase was the most important feature in model from (yan_et_al_2020?), relying only on its value patients were supposed to be directed to the ICU. The chart shows that indeed distribution of sever cases is significantly shifted towards higer values in comparsion with non severe cases, but such a large difference does not occur in other datasets, patients requiring additional care usually have an increased lactate dehydrogenase, but not as much as patients from China. An interesting dependence ocuurs in the dataset from (Zheng et al. 2020a), a large part of the data also comes from China, there is a great distinction between the severely and slightly ill, but the overall lactate dehydrogenase levels are much lower. Additionally, slightly ill patients from Europe and the US have higher LDH levels than severely ill patients from China. Figure 5.1: Comparison of lactate dehydrogenase amongst datasets A similar situation occurs when it comes to high-sensitivity C-reactive protein (CRP), in the dataset from (yan_et_al_2020?) the distributions are clearly shifted among themselves, but similar dependency occurs only in the data from [new]. Patients suffering from the virus slightly and severely from other countries with have less varied level of CRP. Figure 5.2: Comparison of high reactive C protein amongst datasets The same dependence occurs with lymphocytes, one additional fact about these distributions is that French do not have lower level of these cells as do patients from rest of the world. Figure 5.3: Comparison of lymphocytes amongst datasets Neutrofil was not used in [china] model, but it separates sever and non-severe cases in both datasets very well. Figure 5.4: Comparison of neutrophil amongst datasets Age is also crucial feature in predicting virus severity, the young people get sick more mildly than the old ones. Figure 5.5: Comparison of patients age amongst datasets 5.1.4 Comparison of the models We have decided to validate two models proposed by (yan_et_al_2020?) and (Zheng et al. 2020a). The first one is a decision tree with 3 nodes and the second one is an XGboost model with 4 explanatory variables. To verify if the model proposed by Yan et al. is effective in predicting COVID-19 mortality amongst patients around the world, validation on external datasets was made. The decision tree was tested on combined data from American(Barish et al. 2021), French(Dupuis et al. 2021) and Chinese(yan_et_al_2020?), (Zheng et al. 2020a) hospitals. The merged dataset has 1842 observations and 3 explanatory variables: lymphocyte, lactate dehydrogenase (denoted by LDH), and C-reactive protein (CRP). All these medical indicators have been previously studied as key factors in severity and mortality prediction(Cao et al. 2020), (Q. Zhao et al. 2020). To compare Yan et al. decision tree with exemplary models, we created Logistic Regression, Support Vector Machine (SVM) and Tabnet model. Tabnet is a deep learning model for tabular data (Arik and Pfister 2021). It allows creating high performance and explainable classifier on numeric data. The merged dataset was splitted into a training table (80% of observations) and a test table (20%). Then, newly created models were trained on the training table and validated on the test table. Yan’s algorithm was tested on the same part of a merged dataset as other models. We have selected accuracy, precision, recall, ROC AUC and AUPRC as final score metrics. Table 1: Model scores on test table from merged dataset. Model Accuracy Recall Precision ROC-AUC AUPRC Tree 0.674 0.908 0.474 - - Logistic Regression 0.832 0.606 0.776 0.861 0.746 Tabnet 0.840 0.569 0.838 0.913 0.824 SVM 0.777 0.596 0.631 0.843 0.699 Although the decision tree achieved high recall score (0.908), its precision is unsatisfactory. Models trained on multinational data have distinctly higher accuracy and precision thus they are more suitable for medical triage. SVM Tabnet proved to be the most efficient algorithm, having AUC at 0.91 and AUPRC at 0.824. The SVM was the worst model out of the new ones, with AUC at level 0.843 and AUPRC at 0.699. Figure 5.6: AUPRC results on test table from merged dataset. Tabnet scored significantly better than Logistic Regression and SVM. Six months after the Yan et al’s paper had been published, another group of Chinese scientists released their article about machine learning models for COVID-19 patients (Zheng et al. 2020a). They examined several algorithms from which XGBoost performed most efficiently. Besides variables that had been used in the previous article, XGBoost proposed by Yichao Zheng et al. also needed information about the level of Neutrophil in each patient’s blood sample. Importantly, this model was originally designed to predict severity therefore it was expected to have high recall and low precision in mortality prediction task. Model validation was performed on the dataset from Yan et al’s article. To compare XGBoost performance, another Tabnet model was created. It was fitted to the same data as XGBoost and then validated on Yan et al’s dataset. Additionally, we decided to create two versions of each model. The first one with the Neutrophil variable (the same explanatory variables as proposed in the article) and the second one with Age instead of Neutrophil. Table 2: XGBoost and Tabnet scores on Yan et al’s dataset Model Accuracy Recall Precision ROC-AUC AUPRC XGBoost with N 0.573 0.994 0.515 0.865 0.791 XGBoost with Age 0.746 0.975 0.646 0.942 0.918 Tabnet with N 0.538 0.976 0.432 0.868 0.787 Tabnet with Age 0.747 0.984 0.588 0.941 0.891 As expected, all models achieved very high recall. However, XGBoost from the article (with Neutrophil) had significantly worse performance than XGBoost with Age variable. A similar situation occurred with Tabnet models. Overall, XGBoost with the hyperparameters proposed by Zichao Zheng et al. but trained on data with Age column in the place of Neutrophil had best results with 0.942 AUC and 0.918 AUPRC. Figure 5.7: AUPRC results on Yan et al dataset. Models with Age variable are more effective in mortality prediction. 5.1.5 Conclusions Summing up, the first conclusion is that the original decision tree is not an algorithm that can be universally used to assess chances of survival of any patient around the world. The fact that the blood characteristics vary with ethnic groups is significant and therefore usage of some additional variables can improve the models’ predictive capabilities. This is not what the authors of (yan_et_al_2020?) expected but adding Age or Neutrophil variables to the set indeed boosted the performance. Hence, the intuitive windup would be that the best model is the XGBoost with Age variable since it gets high results according to multiple metrics but it is not that simple. For example its precision is significantly lower in comparison to the Tabnet model prepared on the merged datasets. Therefore, what should be learned from this paper is that none of the proposed models that were validated and compared by us should be used on other datasets than those on which they were trained. References Arik, S. O., &amp; Pfister, T. (2021). TabNet: Attentive Interpretable Tabular Learning. AAAI Conference on Artificial Intelligence (AAAI). https://arxiv.org/abs/1908.07442 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Cao, Y., Liu, X., Xiong, L., &amp; Cai, K. (2020). Imaging and clinical features of patients with 2019 novel coronavirus SARS-CoV-2: A systematic review and meta-analysis. Journal of Medical Virology, 92(9), 1449–1459. https://doi.org/10.1002/jmv.25822 Dupuis, C., De Montmollin, E., Neuville, M., Mourvillier, B., Ruckly, S., &amp; Timsit, J. F. (2021). Limited applicability of a COVID-19 specific mortality prediction rule to the intensive care setting. Nature Machine Intelligence, 3(1), 20–22. https://doi.org/10.1038/s42256-020-00252-4 Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Zhao, Q., Meng, M., Kumar, R., Wu, Y., Huang, J., Deng, Y., et al. (2020). Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysis. International Journal of Infectious Diseases, 96, 131–135. https://doi.org/10.1016/j.ijid.2020.04.086 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020a). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(6), 100092. https://doi.org/10.1016/j.patter.2020.100092 "],["one-model-to-fit-them-all-covid-19-survival-prediction-using-multinational-data.html", "5.2 One model to fit them all: COVID-19 survival prediction using multinational data", " 5.2 One model to fit them all: COVID-19 survival prediction using multinational data Authors: Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska 5.2.1 Abstract During the outbreak of SARS-CoV-2 machine learning practitioners tried to build a model that was able to predict the survival or death of patients based on available medical data. Yan et al. (2020a) were among the first researchers to introduce their model based on blood data (lactic dehydrogenase (LDH), lymphocyte percentage, and high-sensitivity C-reactive protein (hs-CRP)) with 0.90 accuracy, however, recreations of this model trained on other countries’ data - the US, Netherlands, and France were not so successful. In this article, we explore the possibility of building an international model for predicting COVID-19 survival. We focused on exploring the models, their variable importance, analyzed the bias they introduced, and concluded with guidelines for future researchers working on this topic. 5.2.2 Introduction Machine learning models are becoming popular in medicine because of the various opportunities they create. Such algorithms may be useful in performing early diagnosis, assessing disease severity, or personalizing treatment. During the COVID-19 pandemic, there were numerous possibilities associated with machine learning models. For example, an algorithm could predict which patients should be qualified for the Intensive Care Unit or who should be treated under a respirator. Additionally, due to the worldwide character of COVID-19 pandemic, it was easier to gather data about symptoms and various blood measures from thousands of patients. In this work, we analyze the article “An interpretable mortality prediction model for COVID-19 patients” by Yan et al. (2020a). The paper presents a decision tree, which predicts whether a patient will die or survive the disease based on the level of lactic dehydrogenase, C-reactive protein, and lymphocytes in blood samples. The presented model obtains high accuracy and ROC AUC scores on data from Yan et al., but has poor scores on datasets from the Netherlands and the US. According to the article ‘A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions’ Wiens et al. (2014) increasing the number of dataset sources for machine learning models may lead to poor performance. With a large number of datasets, controlling the extent to which data from each hospital contributes to the final model is complicated. As a result, procedures for identifying an optimal setting for hyperparameters can quickly become inefficient. In our research we are focusing on answering the question if creation of the model with satisfactory behavior and performance independent of the data origin is possible. This approach can lead us to create an international model, which can help in recognizing the severity of COVID-19 cases. Contrastingly, creating such an algorithm can be impossible for many reasons, for example conducting different examinations in hospitals or different medical standards. 5.2.3 Data sources To create an international model, we used data from three different sources: China, New York, and the Netherlands. Dataset from China is added to the article by Yan et al. (2020a). It contains 375 observations of LDH, CRP, and Lymphocytes percentage, along with the outcome of the COVID-19 disease. Dataset from the Netherlands is attached to the article by Quanjel, Holten, Gunst-van der Vliet, et al. (2021). It contains 306 observations. Apart from information about blood samples, the age and gender of patients are also provided. The New York dataset is not attached to the article by Barish et al. (2021), as it contains confidential data. However, due to the civility of the authors, we were provided with the dataset. The New York dataset contains 1000 observations of blood samples, which is more than in both other datasets combined. On the dataset combined from the sources above we will check if the model trained on such data will be useful in a medical setting. We will check both the utility of a model and it’s fairness. 5.2.4 Model building First, a model performing very well on training data can perform poorly on new samples coming from a different location. Such observations can be classified as out-of-distribution and models can become unpredictable in this setting. It can become really dangerous especially for patients. In many different areas of research there are efforts to detect such samples for example in (5-2-ood?). There are also some fairness concerns. As authors of Vayena E (2018) mentioned, machine learning practitioners in medicine should be committed to fairness. Data sources should reflect the population and it is necessary that they contain enough members of all demographics. The source of bias in machine learning models can be traced to dozens of factors ((5-2-bias-sources?)). In our case, we found that the model presented by Yan et al. (2020a) is not portable and does perform poorly on data from New York and the Netherlands. As we can see in Figure 5.8 models trained on such a dataset tend to include the source of data as an important feature. That means the model is biased for some or all sources of data to give better predictions. Simultaneously it fails to achieve scores as good as models dedicated for each country. Unfortunately, such models have low scores in different performance metrics and are worse than a model created specifically for a given source of data. As an example, we used lazypredict by Pandala (2019) twice to compare scores of various models trained on data from all 3 sources: the first run was on data without the column containing the source of data and a second run was on data containing all columns. The top 3 models from both runs are presented in the tables below (results are sorted by ROC AUC). Table 1: Scores from the lazypredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources excluding the column containing information about the source of data Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken AdaBoostClassifier 0.753 0.714 0.714 0.751 0.191 NearestCentroid 0.700 0.702 0.702 0.708 0.023 KNeighborsClassifier 0.726 0.689 0.689 0.725 0.038 Table 2: Scores from the lazypredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources including the column containing information about the source of data. Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken RandomForestClassifier 0.784 0.740 0.740 0.779 0.326 XGBClassifier 0.770 0.735 0.735 0.768 0.223 LabelPropagation 0.767 0.729 0.729 0.765 0.114 When the source of data is excluded from the training dataset, the results look less promising. This is expected as the origin of data proved to be a useful feature. Next, we tuned the parameters of the top models from each table with grid search and checked their scores using the dalex (Baniecki et al. 2020b) package. RandomForestClassifier and AdaBoostClassifier scored 0.83 and 0.76 ROC AUC respectively which proves the hypothesis about the influence of the origin. Our next step was to explore the ways to measure the effect of training on the data from certain countries. As we have proved, the origin is important in the modeling. Whether it is the effect of the healthcare system, biological differences between people, or the hospitals they were in, it may influence the model in ways that may not be clearly predicted. To determine how important the place of origin is, we trained RandomForestClassifier on data with place of origin. In Figure 5.8 we can see that the two most important features are blood-related. However, the third most important feature is the information whether the patient was from China or not. Figure 5.8: Variable importance for the RandomForestClassifier model trained on a dataset that contains information about the origin of data. Notice that the drop-out loss for the “from_china” variable is rather high when compared with others. Here, the variable importance is measured with perturbations (A. Fisher et al. 2019c). The way it works is that we first measure the performance on the entire model. Next, we reorder elements of a column (variable), train model, and then measure performance again. The difference in performances is called drop-out loss and it depicts the importance of each variable. This information was not surprising, however, we also made a similar test that conveniently simplifies the model. We created a surrogate decision tree based on the RandomForestClassifier. The surrogate model is trained to approximate the predictions of a black box predictor (Molnar 2019). Figure 5.9: Surrogate model for RandomForestClassifier trained on a dataset with information about origin of data. We can observe that the third division of the dataset is made on a condition ‘origin from China.’ As a conclusion, origin of data is an important feature. As expected the variable indicating the source of the data was among the three most important splits. We also had concerns over the bias introduced by the source variable. The bias (or fairness) of the classifier is discrimination in decisions made by the model. The kind of fairness that we focus on is called group fairness and it concerns the difference in outcomes between groups of people. There are many ways to measure this bias with so-called fairness metrics. They all can be derived from confusion matrices for different subgroups. We focus on five of them that are used in Fairness check which are Full Name Confusion Matrix Equivalent Citation Equal Opportunity TPR Hardt et al. (2016) Predictive Parity PPV Chouldechova (2016) Predictive Equality FPR Corbett-Davies et al. (2017) Accuracy Equality ACC Berk et al. (2017) Statistical Parity STP Dwork et al. (2012) Fairness check detects bias in metrics via the four-fifths rule (Code of Federal Regulations 1978). It simply looks at metrics for the privileged subgroup (in this case whether data comes from China) and for unprivileged subgroups and calculates their ratio. If this ratio is within (0.8, 1.25) then we assume that there is no bias. To investigate this claim we trained two machine learning models. The first one was XGBoost with the same parameters as in Yan et al. (2020a). The results were quite surprising as the model introduced bias in 4 metrics. Figure 5.10: Fairness check of the XGBoost model presented in Yan et al. (2020a). Here, each horizontal plot represents a different metric for this model and in each plot there are 2 bars. Each bar represents a ratio between the model’s score in a given metric for data from New York or the Netherlands and China. The desired result is when those ratios are between 0.8 and 1.25. Here, in four metrics these ratios are not in this range meaning the model is biased against different sources of data. To make sure that the model did not overfit the data and gave steady predictions we also checked the fairness of the Histogram-based Gradient Boosting Classification Tree from the scikit-learn package (F. Pedregosa et al. 2011b). The bias was indeed lower but still significant. Therefore we also decided to use bias mitigation strategies. To do this, we firstly merged some subgroups for the algorithms to work better. We tried to make “fair classifiers” with two Python packages fairtorch and fairlearn (Bird et al. 2020). They are related to each other as the fairtorch implements the solutions from fairlearn. Using those in-processing algorithms (these are the kind of mitigation approaches that reduce the bias during model training) we obtained 2 additional models. One of them was a neural network with two hidden layers 128 and 64 neurons respectively, and ReLu activation function. The other was the Histogram-based Gradient Boosting Classification Tree that was trained using the reductions approach. The amount of bias reduced by the neural net from fairtorch was not satisfying enough and therefore will not be shown here. However, the results from fairlearn were quite good. Figure 5.11: Fairness check of the models before and after the reductions. Models checked are Histogram-based Gradient Boosting Classification Tree but with different parameters. Bar’s length is a ratio between a model’s scores in a given metric for data not from China (other) and from China. The desired result is that the green bars are shorter than the blue ones and are close to 1 (more precisely between 0.8 and 1.25) meaning that the new model is less biased against the origin of data. As we can see, despite the fact that the reduced model does not fit within the green field we decided that the bias was in fact reduced. The last thing to check was the performance of this model. Such reductions in the amount of bias may result in a significant drop in performance. In this case, it was the same. The ROC AUC metric dropped from 0.71 to 0.59 which for the medical applications is not enough. Therefore we concluded that in the case of this data the models were biased towards different origins. 5.2.5 Discussion Machine learning can be very useful when applied to medical data. Over the last few years, the increased amount of health-related information created many possibilities to aid medical specialists, for example in Decision Support Systems. Such data can be successfully applied to predicting COVID-19 as proven by Yan et al. (2020a). However, creating a multinational model isn’t trivial for a few reasons. Even in a single country, forms of data collection vary from hospital to hospital. It’s impossible to enforce a unified system and data format across continents. Moreover, some measurements may depend on the time of the day taken - for example, blood pressure will be different in the morning and in the evening. Another problem was observed by Kaushal et al. (2020): Whether by race, gender or geography, medical AI has a data diversity problem: researchers can’t easily obtain large, diverse medical data sets—and that can lead to biased algorithms. The case of having a model that doesn’t take into account the origin of the sample will lead to simplification of such a model since cut-off values were different for data from China, NY, and the Netherlands. This leads to lower metric scores and this excludes it from using in a medical environment, where precision is crucial. 5.2.6 Summary We were working on a dataset merged from data coming from different countries and in this setting, machine learning models tend to be biased towards different nationalities. Our attempts at reducing this discrimination were not successful. We believe that with a bigger or more balanced dataset we would have a slightly better chance at meeting our goal. Making models on country levels or geographical regions to achieve maximum fairness is more reasonable. It will also allow scientists to achieve the best results in predicting COVID-19 survival and ultimately we defeat COVID-19 and live happily ever after, till the end of our days, as said by short man(HOBBIT) and acclaimed author Bilbo Baggins. References Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020b). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 Fisher, A., Rudin, C., &amp; Dominici, F. (2019c). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Pandala, S. R. (2019). Lazy Predict. Python package. https://github.com/shankarpandala/lazypredict Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011b). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Vayena E, C. I., Blasimme A. (2018). Machine learning in medicine: Addressing ethical challenges. PLOT Medicine, 15(11), 1–4. https://doi.org/10.1371/journal.pmed.1002689 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 "],["transparent-machine-learning-to-support-predicting-covid-19-infection-risk-based-on-chronic-diseases.html", "5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases", " 5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases Authors: Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik 5.3.1 Abstract The moment the COVID-19 outbreak in 2019 occurred, people started using machine learning in order to help in managing the situation. Our work is a build up to research described in (Yan et al. 2020b) and in (Bello-Chavolla et al. 2020) articles. We improve and create new models predicting COVID-19 mortality based on blood parameters and analyze original models presented in the articles. We train a prediction model calculating risk of getting infected with the virus depending only on person’s sex, age and chronic diseases and created an application which calculates the COVID-19 mortality risk based on input data and explains the model with visualizations. Considering accuracy of the model, calculations based only on chronic diseases may not achieve best results but we propose a transparent machine learning approach which doesn’t need any additional medical information, is explainable and easy to understand and requires only data that everyone has access to. Our application is available at URL: https://hubertr21.shinyapps.io/Explainable_COVID19_Mortality_Predictor/ 5.3.2 Introduction The COVID-19 pandemic is currently one of the world’s biggest problem, affecting not only everyone’s private lives, but also other areas of human activity, including research. In the scientific field, this is a specific issue that scientists are still able to approach this problem in completely different ways, because no distinctive and right path has yet been established. It is in the combination of these three aspects that we see an opportunity to deepen the current understanding of the pandemic and create a useful tool for predicting mortality from the disease. All available models for predicting coronavirus mortality have numerous disadvantages, which include predictions based on small data sets, the use of medical information unavailable to the average person, unexplained models or their low effectiveness. Besides using data from mentioned articles ((Yan et al. 2020b), (Bello-Chavolla et al. 2020)) we also explore data used in creation of an article (Barish et al. 2020). In order to provide additional knowledge for each of us, we present an application that eliminates all these drawbacks, to a greater or lesser extent. Our solution is the most beneficial for various XAI stakeholders, as described in article (Barredo Arrieta et al. 2020c). Explanations such as SHAP (SHapley Additive exPlanations) or break down profiles are used. The proposed application bases its mortality prediction on the information about chronic diseases that the patient suffers from. In addition to the predicted mortality, it also provides a break down plot as an explanation of the mortality, SHAP visualization, to show impact of the most important parameters and also analysis of the mortality risk depending on age. By using the random forest model, created using ranger R package ((Wright and Ziegler 2017b)), we also ensure the explainability of the entire process. Thanks to this solution, our application is useful for both ordinary people and doctors. 5.3.3 Flaws As a first step, we started analyzing one of the first articles (Yan et al. 2020b) on the development of predictive models to predict COVID-19 mortality in depth. The main disadvantages of the model presented in it were the very poor prediction of mortality in the case of testing on external data sets (Quanjel, Holten, Vliet, et al. 2021), (Barish et al. 2020) and the related allegation of not testing one’s solution on external data (Barish et al. 2020). In addition, our team noticed a very large bias present in the original data, thanks to which even the simplest models achieved extremely high efficiency of over 90%. 5.3.4 Improvements Our first approach to the problem was to try to improve the work of the articles authors in question through very different methods. Firstly, we started from creating a correlation between variables heatmap to see which features were the most important. In the next step we created two models using Gradient Boosting and Ada Boosting which both achieved better results than the model presented in (Yan et al. 2020b). Moreover, thanks to (Barish et al. 2020) we were given access to additional data set with more records and were able to test the models. Next, we used Principal Component Analysis to try to split the data using reducing dimensions of the data receiving a surprisingly good result - we were able to separate the two classes with one straight line. Finally, took a look at the data distribution - it is visibly skewed left. We fixed it by applying proper transformation which resulted in improving the model. While evaluating the models we mainly focused on precision score which defines what percentage of patients predicted to be in chosen class (survival and death in this case) really represents that class. If the precision score is low it is either dangerous for people’s life or leads to overcrowding the hospitals. Prediction using original parameters on external data First, we decided to use the broader pandemic knowledge that the authors of the original article did not have to improve the model on its default data. Inspired by a newer article (Barda et al. 2020), we decided to test how the selection of variables improved according to the latest knowledge will affect the predictions of the predictive models. For the suggested parameters: age, C-reactive protein, chloride, albumin, lymphocyte count and LDH, we created a correlation map to select the most important of them. Figure 5.12: Heatmap showing correlation between the variables in the original dataset. The more intensive the color is, the more variables are correlated Figure 5.12 shows that among the most important features there are age, albumin, LDH and C protein, which we used in our models. After training and testing the GradientBoostingClassifier and AdaBoostClassifier models on slightly reduced data (the original test set had to be replaced by the test and validation set), we obtain cross-validation precision at the level of 0.979 and 0.958 and the following reports(Figure 5.13 and Figure 5.14). Moreover we present confusion matrix from original model (Figure 5.15). Figure 5.13: Confusion matrix showing accuracy, precision, recall and other scores for evaluating a classification for the GradientBoosting model Figure 5.14: Confusion matrix showing accuracy, precision, recall and other scores for evaluating a classification for the AdaBoosting model Figure 5.15: Confusion matrix showing accuracy, precision, recall and other scores for evaluating a classification for the original model The above results confirm the improvement in the quality of the original model (Figure 5.15), whose precision score for death is only 0.81 and cross-validation stands around 0.97. Thanks to the authors of the paper (Barish et al. 2020), we were given access to additional data with features coincident with those we already had. Data set contained over 1000 observations and only fourteen features that were selected by owners to fit as good model as possible. In order to test previous model (AdaBoost), we used it to predict the outcome for those thousand patients, receiving following results (Figure 5.16). Figure 5.16: Confusion matrix showing accuracy, precision, recall and other scores for evaluating a classification for the original model tested on external data Model performance has dropped significantly, the outcome is far from desired. PCA In order to analyze data further, we performed Principal Component Analysis. For visualization’s simplification we considered only first two, most substantial components. Obtained two-dimensional plot is shown on Figure 5.19. Explained variance ratios were: 22.6% (for the first component) and 0.063 for the second component), which results in total explained variance ratio of 28.9% for both components together. Taking absolute values of the scores of each feature from the first component might be also used for feature selection. Those with the highest magnitudes were: Prothrombin activity, Lactate dehydrogenase, albumin, Urea, neutrophils(%), (%)lymphocyte. Some of the features are those, that we have already known are important, such as albumin, (%)lymphocyte or Lactate dehydrogenase, but age was around the middle, not among the top ones. The most noticeable fact is that our two classes are almost separable with just a single line. Even without any sort of complex machine learning or other algorithms, it’s possible and not complicated to fit a line that divides cases ended in death from cases followed by patient’s recovery. As an example, same visualization with additional function \\(y=2.5x+2.5\\), created without any sort of optimization techniques, is presented on Figure5.18. Figure 5.17: Vizulized results of Principal Component Analysis used to reduce dimensions of the original dataset. Red colored points represent people who survived after being infected with COVID-19, green colored points represent people who passed away because of the virus. Figure 5.18: Vizulized results of Principal Component Analysis with line separator used to reduce dimensions of the original dataset and to divide observations into classes. Red colored points represent people who survived after being infected with COVID-19, green colored points represent people who passed away because of the virus. Such division achieves (train) accuracy of 0.94, which is almost as good as results received by machine learning algorithms described in the paper, what might encourage to consider given data not authoritative. Data distribution analysis We analyzed the distribution of percentage of lymphocytes, lactate dehydrogenase and high sensitivity C-reactive protein by creating histograms for the original and the new data. Figure 5.19: Distribution of percentage of lymphocytes, lactate dehydrogenase and high sensitivity C-reactive protein for the original dataset Figure 5.20: Distribution of percentage of lymphocytes, lactate dehydrogenase and high sensitivity C-reactive protein for the new dataset We noticed that all variables are strongly left skewed which is unfavorable for the model because more reliable predictions are made if the predictors and the target variable are normally distributed. Trying to make our model better, we applied square root transformation. Then we trained a new model on them (also using Ada Boost Classifier) and we tested it on the new data: Figure 5.21: Confusion matrix showing accuracy, precision, recall and other scores for evaluating a classification for the AdaBoostClassfier model tested and trained on original (highly lef skewed) data afer applying square root transformation. From Figure 5.21 we can see that the model generally improved (the accuracy is higher) and it performs slightly better. General drawbacks In addition to the aforementioned bias, our models can still be accused of learning on very small data sets not exceeding even 1000 observations, which still affects the uncertainty related to the effectiveness of the presented proposals. Moreover, the most promising model, developed on an external data set, unfortunately did not show sufficiently high efficiency to be useful in medical applications. In addition, the models that have been proposed by us, are, unfortunately, not explainable models, which makes them less desirable by doctors. The last, and perhaps the least obvious disadvantage is that the data used for prediction alone is unattainable for single entities, as blood tests can only be performed by highly qualified medical personnel. This aspect makes solutions based on these models incomprehensible to the average person, which significantly limits their usefulness. Summary To sum up, the most desirable effect of our work turns out to be an explainable model with high-quality predictions, based on a large database. In addition, it should be based on easy-to-obtain information about a person’s health and be understandable to both ordinary people and physicians. In search of research that would help us explore this branch of machine learning, we manage to find an article (Bello-Chavolla et al. 2020), which provided us with both a comprehensive set of data, understandable to everyone, and a very rich information background. 5.3.5 Transparent Machine Learning While working in machine learning one can often encounter an issue called the black-box problem (Rai 2020). It occurs when a model is complex, unexplainable and not transparent. Explainability solves this problem by “unpacking the black-box” which is essential in building trust in the model. That is why we want to create an explainable model which could be useful to every person which doesn’t need any additional medical information and easy to understand and requires only data that everyone has access to. We used explainers from (Biecek 2018b), package for explainable Machine Learning, to create visualizations, that allow user to understand where the results come from and because of that, they are more transparent and clear. Model To create the model we use random forest from (Wright and Ziegler 2017b) package constructing a multitude of decision trees which are one of the most transparent and easy to explain models, even for people not familiar with machine learning concepts. Moreover, we tuned and tested XGBoost ((Wright and Ziegler 2016)) and Support Vector Machines from (Meyer et al. 2021) package to maximize AUC measure. Ranger performs best with AUC of 0.92, XGboost turns out to be slightly worse with AUC of 0.87. Support Vector Machines achieve the worst results - AUC around 0.78 (Figure 5.22). Figure 5.22: Area Under (AUC) the Receiver Operating Characteristics (ROC) curve for considered models. It serves the purpose of comparing classificators - the bigger the AUC, the better the model. Data The data set used in the application is the same one that was used in the article (Bello-Chavolla et al. 2020) and it is an open source data published by General Directorate of Epidemiology in Mexico. It consists of 150,000 records from Mexican hospitals, of which over 50,000 are patients with confirmed coronavirus. The most important data for the project are information of chronic diseases, age and date of death. 5.3.6 Application In order to achieve our goal we create an easy application, in which one can choose his or hers chronic diseases, sex, age and it calculates the COVID-19 mortality risk for particular infected person. Additionally, in the bookmarks there are presented plots about the model. The first one (Figure 5.23) is a break down plot which shows how the contributions attributed to individual explanatory variables change the mean models prediction. Despite printing out the risk it also enables its user an easy option to understand the outcome. Another one (Figure 5.24) is Ceteris Paribus profile which examines the influence of an explanatory variable (which is age in this case) by assuming that the values of all other variables do not change. This visualization is very useful for doctors to properly distinguish a higher risk groups. The last one (Figure 5.25) is a SHAP plot, it calculates the importance of a feature by comparing what a model predicts with and without the feature. Our application is available at the URL: https://hubertr21.shinyapps.io/Explainable_COVID19_Mortality_Predictor/ Figure 5.23: Break down profile, showing how particular features affect final death risk for one individual patient. Figure 5.24: Ceteris Paribus, showing how death risk changes with age, while keeping other features (chronic deseases) the same. Figure 5.25: SHAP profile, plot that visualises Shapley values for particular features. 5.3.7 Conclusions Figure 5.26: XAI stakeholders picture from article (Barredo Arrieta et al. 2020c). Thanks to the developed application, various XAI stakeholders, as presented in (Barredo Arrieta et al. 2020c) - Figure 5.26, may have easier access to transparent and explainable model that estimates mortality risk in case of being infected by COVID-19. Our application allows the afflicted to see which diseases are contributing the most to their outcome, without the need of doctor’s examination, such as any blood properties and other information, that can only be gathered by a specialist. Only by selecting diseases that one suffers from, reliable prediction can be obtained quickly and without leaving home. Domain experts, such as medical doctors might use it for more complex patient examination or to improve their understanding and knowledge of the disease. Model’s predictions can be used by politicians to determine what people might need faster medical treatment. Moreover, data scientist may use it for extending their skills or as an inspiration to examine the problem in a different way and improve the existing solutions. References Barda, N., Riesel, D., Akriv, A., Levy, J., Finkel, U., Yona, G., et al. (2020). Developing a COVID-19 mortality risk prediction model when individual-level data are not available. Nature Communications, 11. https://doi.org/10.1038/s41467-020-18297-9 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2020). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3, 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020c). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012 Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105, 2752--2761. https://doi.org/10.1210/clinem/dgaa346 Biecek, P. (2018b). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. https://jmlr.org/papers/v19/18-416.html Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F., Chang, C.-C., &amp; Lin, C.-C. (2021). e1071: Misc Functions of the Department of Statistics, Probability Theory Group. R package. https://CRAN.R-project.org/package=e1071 Quanjel, M. J. R., Holten, T. C. van, Vliet, P. C. G. der, Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in Dutch patients with COVID-19. Nature Machine Intelligence, 3, 23–24. https://doi.org/10.1038/s42256-020-00253-3 Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48, 137–141. https://link.springer.com/article/10.1007/s11747-019-00710-5 Wright, M. N., &amp; Ziegler, A. (2016). XGBoost: A Scalable Tree Boosting System. SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785 Wright, M. N., &amp; Ziegler, A. (2017b). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 "],["comparison-of-neural-networks-and-tree-based-models-in-the-clinical-prediction-of-the-course-of-covid-19-illness.html", "5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness", " 5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness Authors: Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk 5.4.1 Abstract The COVID-19 pandemic overwhelmed medical staff around the world, showing that effective and explainable models are needed to help allocate limited resources to those in need. Many published models for predicting COVID-19 related ICU admission and mortality were tree-based models or neural network models. We compared the two architectures in effectiveness, explainability and reproducibility. The two architectures appear to be similar with regards to their effectiveness, but the neural network model had significant reproducibility issues and worse explainability. 5.4.2 Introduction In 2020 many papers presenting models aimed to predict the course of COVID-19 in patients emerged (Ma et al. 2020; Yan et al. 2020c; Zheng et al. 2020b). Article we are referring to (Li et al. 2020) differed from the majority with the use of neural networks. In the past there were many attempts to compare effectiveness of artificial neural networks and tree-based models at various tasks using different types of data including tabular data (Ahmad et al. 2017; Arsad et al. 2013) and tasks connected to hospital patients outcomes (J. Wang et al. 2009). There were analyses showing superiority of forests over simple neural networks on small (up to 1000 records) tabular datasets (Klambauer et al. 2017). Data used by the referred article’s authors consisted of 1020 or 1106 observations depending on the researched problem. Authors note the importance of explainability of clinical models (Tonekaboni et al. 2019) due to the need of establishing clinician’s trust to successfully deploy a model. There are tools for explaining tree-based models (Biecek and Burzykowski 2021; Chen and Guestrin 2016) and some of methods are not available for neural networks. Tools to calculate SHAP values for forest models were developed (Scott M. Lundberg et al. 2019), while general algorithm to compute them exactly in a reasonable complexity for deep learning models, only approximations can be made. Explaining neural networks with SHAP values is an important issue in the field (R. Wang et al. 2021). The referred article does not provide a source code for the replication of the results. Taking care about the reproducibility is considered a major problem in the academic society (WUoT 2020). Motivated by the preceding we aim to contribute to the work started by the authors of the referred article. At first, we try to replicate models. Then we propose different network architectures and XGBoost models. Finally, we compare the effectiveness of all the models at the prediction tasks and their explanations based on the SHAP values. 5.4.3 Methods Three neural network models and XGBoost model were trained for both ICU admission prediction and death prediction. The tested neural network architectures were: replication of the architecture proposed by (Li et al. 2020) (referred to as the Baseline model), modified version of that architecture using binary cross entropy as the loss function (Baseline (crossentropy) or Modified)), basic neural network model using two hidden layers of 32 neurons each with binary cross entropy as the loss function (Basic). Neural network models were created using Keras Python library (Abadi et al. et al. 2015). All models were trained and tested on data provided in the article (Li et al. 2020). Feature selection was performed as described in the article. Data was split in a 75:25 (train:test) ration. To reduce overfitting, an internal 0.2 validation set size was used. Effectiveness of those models were compared using receiver operating characteristic area under curve (ROC AUC) metric (Hanley 2014). ROC AUC values were calculated using the test data held out from training. Neural network models were trained 25 times each to compare stability. SHAP values were calculated for the xgboost models using the R treeshap library (Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław 2020). Approximate SHAP values were approximated for neural network models using DeepExplainer from the Python SHAP library. Feature importance was calculated using mean SHAP values and compared. 5.4.4 Results ROC curve comparison between all ICU admission models is shown in the figure 5.27. XGBoost model was the best-performing model, based on the ROC AUC score. The best-performing neural network model was the Basic model (the neural network model with only 2 layers), with a ROC AUC score of 0.696. In the mortality prediction task, however, the Basic neural network model outperformed all other models, as shown in figure 5.28. It is worth noting that in both cases the Baseline model, which was the replication of the (Li et al. 2020) model had the lowest ROC AUC scored, and therefore is indicated to have performed the worst. Figure 5.27: ROC curves comparison for ICU admission prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, name and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.28: ROC curves comparison for mortality prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, model’s and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. To ensure the reliability of neural network models, each model was trained and tested 25 times. The resulting boxplots can be seen in the figure 5.29 for the ICU admission prediction data and in the figure 5.30 for the mortality data. The comparison score for both cases was ROC AUC score. As we can see in figure 5.29, the Baseline model trained on ICU admission data has proven to be especially unreliable, with AUC score ranging from 0.35 up to 0.7, while other 2 models outperformed it, with both better scores and lower variances. Figure 5.30 shows that models trained and tested on the mortality data had overall better ROC AUC scores than their ICU admission counterparts, but some outliers can be noticed for both Baseline and Modified, reaching as low as 0.5 ROC AUC score. Figure 5.29: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.30: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Feature importance comparison for ICU admission task is shown in figure 5.31. In all of these models Procalcitionin was the most important feature (it is worth reminding, that our models were trained only on the top 5 features for ICU admission task, and top 6 features for mortality prediction task), while other features’ significance differ between models. What is more, Procalcitionin was much more impactful for the neural network model, with mean SHAP value nearly doubling the value of the 2nd most import feature. Figure 5.31: a) Boruta algorithm results from the article. Only the top 5 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. Feature importance comparison for mortality prediction task is shown in figure 5.32. Age was the most important feature for all of these models. As in ICU admission prediction task, other features’ significance differ between models. Figure 5.32: a) Boruta algorithm results from the article. Only the top 6 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. 5.4.5 Discussion The COVID-19 pandemic has demonstrated a need for quick development, testing, validation and deployment of machine learning models. However, this can’t come at the expense of reproducibility, as the replication crisis still poses a serious issue. Parts of the description provided in the reference article can be imprecise enough to be misunderstood or have more than one meaning. In the field of machine learning both data and source code are necessary components to reproduce results. This is even more important when proposing unusual model architectures, as those can lead to new discoveries in the field. However, when proposing such an architecture, a baseline model should be used to demonstrate that the proposed architecture can preform better. Finally, different external validation techniques should be used, such as a leave-one-hospital-out cross-validation or working on combined data from multiple sources. While our team focused primarily on comparing model architectures, other chapters in the WB Book examine those aspects of fair artificial intelligence. In conclusion, we have shown the importance of providing reproducible code, as well as having a baseline model to compare the results. 5.4.6 Source code Source code for all models and figures mentioned in the article, including used data is available at the Github repository https://github.com/konrad-komisarczyk/wb2021 in the proj2 subdirectory. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al., et al. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. USENIX conference on Operating Systems Design and Implementation. https://dl.acm.org/doi/10.5555/3026877.3026899 Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. R package. https://github.com/ModelOriented/treeshap Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Lundberg, Scott M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020b). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML "],["topic.html", "6.1 Topic", " 6.1 Topic Authors: Adrian Stańdo, Maciej Pawlikowski, Mariusz Słapek (Warsaw University of Technology) 6.1.1 Abstract Technological advances were early adopted by healthcare with great benefits and developments. In many health-related realms machine learning is crucial such as: development of new medical procedures, the treatment of chronic diseases, the management of patient records and data. Explainable AI (XAI) gives invaluable tools in healthcare for understanding the models by humans. The aim of this article is to compare PDP profiles for a Rashomon set with a given metric. Following we use five different metrics of distance function and based on that we compare PDP curves. Additionally, the library in the Python language has been created, which automates this research. We have observed that the created models, which have similar scores, have different variable importances. In this paper these differences were measured and assessed to understand better the problem of predicting hospital mortality using data from MIMIC-III. 6.1.2 Literature review Rashomon is a intriguing Japanese movie in which four people witness an incident from different vantage points. When they come to testify in court, they all report the same facts, but their stories of what happened are very different. In machine learning Rashomon set is used to characterise problem in which many different models offer accurate results describing the same data. However, not every accurate model gives a right conclusion as described in (Breiman et al. 2001): “If the model is a poor emulation of nature, the conclusion might be worng”. Herein authors also explain basics of Rashomon sets on example. Much more in depth and mathematical description is provided in (Semenova et al. 2019). Another important topic related to Rashomon sets is analysing the feature importance of the model. It was described in this article (A. Fisher et al. 2019d), where authors suggested to study the maximum and minimum of variable importance across all models included in the Rashomon set. This technique was called MCR (Model Class Reliance). Furthermore, (Dong and Rudin 2020) presented technique to visualise the “cloud” of variable importance for models in the set, which could help us understand the Rashomon set and choose the one which give the best interpretation. The last question stated in the article (Rudin et al. 2021) was about choosing model from the Rashomon set. It might be a difficult task, especially when we lack good exploration tools. (Das et al. 2019) created a system called BEAMS that allows to choose the most important features. Next, the program searches the hypothesis space in order to find model which fits best to given constraints. Since this system works only with linear regression classifiers, (Rudin et al. 2021) stated a question if it is possible to design a simmilar system which will search only models within the Rashomon set. 6.1.3 Results 6.1.3.1 Results of models search 6.1.4 Best models 6.1.4.1 Boxplots of abs_sum metric for the best models 6.1.4.2 Boxplots of abs_sum metric for each feature 6.1.4.3 PDP curve for albumin_std 6.1.4.4 PDP curve for atempc_min References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019d). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["title.html", "6.2 Title", " 6.2 Title Authors: Jan Borowski, Konstanty Kraszewski, Krzysztof Wolny 6.2.1 Literature review In 1950 Japanese director, Akira Kurosawa, presented film Rashomon. Movie resolves around four witnesses, that describes the same crime in four different ways. This situation was called Rashomon effect after the name of the movie. In other words Rashomon effect is a situation when we have multiple different descriptions to the same event. This term is commonly used in multiple sciences like sociology, psychology or history. At the begging of the 21st century Rashomon effect was introduced to predictive modelling by Leo Breiman and his work ‘Statistical modeling: The Two Cultures’(Breiman et al. 2001). In this article he named Rashomon effect situation, where there are many approximately-equally accurate models. Although these models have similar results, they can differ, when it comes to the way they managed to achieve it. Breiman called for closer examination of the Rashomon effect and conclusions that can be drawn from it. Recently, we can observe growing interest in Rashomon effect, although there is still a lot to be discovered. One of the articles, that bring closer the problem is ‘A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning’[6-0-rashomon-intro]. It provides several approaches for estimating the size of the Rashomon effect as well as the usefulness of the Rashomon curve in model selection. References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. "],["rashomon-ml-with-addition-of-dimensional-reduction.html", "6.3 Rashomon ML with addition of dimensional reduction", " 6.3 Rashomon ML with addition of dimensional reduction Authors: Degórski Karol, Fic Piotr, Kamiński Adrian (Warsaw University of Technology) 6.3.1 Abstract Rashomon effect occurs when there are many different machine learning models with similar predictions. Therefore choosing only one model out of them may have impact on the final results, so it should be done consciously and carefully. One of the ways in selecting appropriate model can be Explainable Artificial Intelligence methods. In our study, we perform an analysis of different XGBoost models using dimensionality reduction and clustering technique, so that we explain the factors that influence on the final behavior of previously built models. For building rashomon sets we use publicly available MIMIC-III dataset, which contains medical information. The task that we focus on is an in-hospital mortality prediction, which was previously conducted by other scientists, although we expand their models. Our results suggest that XGB models from Rashomon set may be grouped into clusters in the reduced parameter space. 6.3.2 Introduction and related works Machine learning is used to analyze data and allows us to make predictions. A typical approach to this issue is building models and then, basing on some metrics, choosing the one that will be used in the future. However, very often there are many different descriptions giving about the same minimum error rate, so that we cannot point one model as the best (Breiman et al. 2001). They have created the term Rashomon effect to describe a situation when there are many different models with quite similar predictions. An example of this effect in reality may be Linear Regression model and finding 5 from 30 best describing variables of a given problem. In this case there are approximately 140,000 such subsets. Usually we choose the model which has best results on a test set, although there may be also different subsets of 5 variables that give very similar results. Moreover, this effect occurs in different models, such as decision trees or neural networks. Furthermore, (Semenova et al. 2019) contributed to expand the study about Rashomon effect. They found out that interpretability of model is connected with Rashomon sets, which are subsets of models that have similar performance to the best model in terms of loss function. Accordingly, when the Rashomon set is large there may exist simpler and higher performing model. Analysis of the Rashomon effect is still a new and open for development field of the interpretable machine learning. Because of that, there are remaining challenges and problems, which are missing a state-of-the-art approach. Some of them are a proper measure of the Rashomon set, the best techniques of its visualization and optimal choice of the model from the Rashomon set (Rudin et al. 2021). Analyzing Rashomon effect may be done by using various XAI techniques that explain the behavior of a single model. One of them for the mentioned tasks is a framework called Variable Importance (Dong and Rudin 2020), which can be used for studying the variable importance among Rashomon set. Secondly, partial-dependence profiles that show the effect of a particular explanatory variable on the dependent variable are also helpful in Rashomon study (Biecek and Burzykowski 2021). These techniques are especially important in medical problems. In modeling these problems not only do we need high results of metrics, but also we have to know what exactly influenced on this prediction. These techniques are especially important in medical problems. In modeling these problems not only do we need high results of metrics, but also we have to know what exactly influenced on this prediction. Therefore, we focus on an article of (Tang et al. 2018) and their in-hospital mortality prediction and expand their modeling by conducting Rashomon analysis. Using MIMIC-III dataset (Johnson et al. 2016) allowed us to train lots of XGBoost models with different hyperparameters and build a Rashomon set. The purpose of the following paper is to prove whether machine learning models in Rashomon set have different hyperparameters and how their predictions differ from each other according to different metrics. Our results may be important in predictions connected with patients’ health, since we performed hyperparameter analysis of Rashomon models, showing which of them have crucial impact on final predictions. Each hyperparameter has diverse influence on trained model, so that the ability to understand it may be important. 6.3.3 Methodology Our research consisted of many steps which touched subjects of reproducibility, hyperparameters optimization, construction of Rashomon set and comparing models’ characteristics. The whole process can be described in the following chronological order: Data set: We obtained access to the publicly available MIMIC-III database, which is a source of real medical treatment documentation, widely used in many researches in the field of medical applications of machine learning. From the whole database we extracted a data frame which could be passed to machine learning models. We chose to reproduce one of the approaches presented in (Tang et al. 2018). The features set which we used was called by the authors W48. It combined Word2Vec embeddings (Mikolov et al. 2013) of diagnostic history with summary features of time series data. Further the data set was divided into training and test parts. Necessary scripts for this step were provided by the authors in a public code repository github. Task: From a few benchmarking tasks considered in the mentioned work we focused on the only one: in-hospital mortality prediction. It is a binary classification problem in which we try to predict whether a patient survives a stay in hospital or dies. Model training: We focused on the one algorithm and further analyze its behavior depending on its hyperparameters. We used XGBoost classifier (“XGBoost” 2016) and optimized its parameters with random search cross validation obtaining 300 models. Search space was constructed according to (Probst et al. 2019) and consisted of 7 parameters. Metrics used for performance scoring were AUC, accuracy and F1. Rashomon set: From all trained models we selected a group of the ones which achieved at least 0.9 AUC score on the test set. In this way we got a Rashomon set of 72 best models. Analysis tools: We represented each model from the Rashomon set with its hyperparameters values as variables and performed PCA dimension reduction. Further we applied clustering algorithm on the reduced data and obtained clusters of models from the Rashomon set. In the next step we chose the best model from each cluster in terms of AUC score as their representants and used XAI techniques - variable importance, PDP and ALE plots - to compare their characteristics. 6.3.4 Results After performing steps listed above we obtained the following results. 6.3.4.1 Hyperparameters After executing PCA and KMeans clustering that were mentioned before, our Rashomon set looked as follows in Figure 6.1: Figure 6.1: Visualisation of models’ clusterization in Rashomon set. When we had the following clusters of models that differ in hyperparameters, we wanted to find out how the obtained clusters differ from each other. So we grouped models basing on their belonging to clusters and we gathered mean values of hyperparameters (Table 6.1) and metrics’ scores (Table 6.2). Table 6.1: Average hyperparameters values in each cluster. Table 6.2: Average metrics’ scores values in each cluster. Basing on the mean scores (Table 6.2) of three metrics that we used (roc_auc, f1, accuracy), we can see that on average the best models are in cluster 3 and then respectively 2, 4, 1. When we look closely at Table 6.1 with hyperparameters we can notice which of them are responsible for that difference. The biggest difference appears to be the value of alpha. We can see that cluster that achieved best results have value of that hyperparameters equal around 2.7 while the rest of the clusters have this value either too low or too high. Then we decided to check the impact of hyperparameters (alpha, colsample_bylevel, eta) on different metrics and we also added the best performing models from each cluster (in terms of roc_auc score) to see if there is any specific value of some hyperparameter that boost performance of our models. The results are presented in Figure 6.2. As we might have expected the magic value that allow us to achieve the best possible score does not exist. However, we can see some ‘spikes’ where scores are higher and usually around that area values of best models’ hyperparameter are located. 6.3.4.2 Variable Importance To look deeper in our models’ behavior, we calculated variable importance for the best model in each cluster. In the Figure 6.3 the best performing model according to roc_auc score is XGB 1. Unfortunately, since there were a lot of variables in our training set, we cannot observe that some variables were key factors in models’ predictions. Moreover, the differences between these 4 models are not very significant. Figure 6.3: Variable importance of best performing models in each cluster. Partial Dependence Plots Another important XAI technique is drawing Partial Dependence Plots, which show the effect of the specific feature on the model’s final prediction. In Figure 6.4 it can be observed that the best performing model has significantly distinct plot in all presented variables than the rest of clusters representants (the red line is below). For instance, in variable 40 the red curve (representing model 1) is distinct and in variable 97 it decreases to the lower values. Furthermore, the best model in cluster 3 is also different. For example, in variable 231, where it starts increasing faster than other models and in variable 115, where it increases more rapidly. What’s more there are not significant differences in the curves representing models 2 and 4. Very often these curves are close to each other. The only difference which is easy to observe is in feature 118 and 40, where the curve representing model 4 starts higher than the one representing model 2, although their shape is very similar. This models’ behavior may be explained by their similar hyperparameters (Table 6.1). Figure 6.4: Partial Dependence Plots of best performing models in each cluster. Accumulated Local Effects Plots The last XAI technique that we used were Accumulated Local Effects Plots. These plots also show the influence of each feature on model’s predictions, but their advantage to PD Plots is that they are unbiased. In the Figure 6.5 on the plots representing features 40 and 97 there is the biggest difference between model 1 and the others. Nevertheless, on the plots representing features 115 and 231 the best performing model is also distinct. Another important issue is that the curve of the model representing cluster 2 is distant in variable 118, and that it is between model 1 and models 3, 4 in variable 40. Furthermore, in features 231 and 115 curve representing model 3 is significantly above from others. What we have observed on PD plots is also true on ALE plots. Curves representing models 2 and 4 are similar and we cannot observe remarkable differences. Figure 6.5: Accumulated Local Effects Plots of best performing models in each cluster. 6.3.5 Summary and conclusions Our research gave us interesting results which proved our hypothesis. The XGBoost models which belonged to the Rashomon set proposed by us, were different in term of hyperparameters. Reduction of dimensionality and tools of explainable artificial intelligence helped us to show the variety of their characteristics and behaviors. Further clustering approach allowed us to group the models basing on these differences. Remembering that Rashomon set consisted of the best models in terms of AUC metric, we can conclude that the choice of the best model is a non-trivial decision. We have an opportunity to select slightly worse one but more suitable for us in terms of parameters and model characteristic. Moreover, proposed by us methods of analysis of the Rashomon set were helpful and useful and can be used in the future. References Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Johnson, A. E., Pollard, T. J., Shen, L., Li-Wei, H. L., Feng, M., Ghassemi, M., et al. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1–9. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546. http://arxiv.org/abs/1310.4546 Probst, P., Boulesteix, A.-L., &amp; Bischl, B. (2019). Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53), 1–32. http://jmlr.org/papers/v20/18-444.html Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. XGBoost: A scalable tree boosting system. (2016). CoRR, abs/1603.02754. http://arxiv.org/abs/1603.02754 "],["roshomon-sets-on-death-prediction-xgb-models-using-mimic-iii-database.html", "6.4 Roshomon sets on death prediction XGB models using MIMIC-III database", " 6.4 Roshomon sets on death prediction XGB models using MIMIC-III database Authors: Ada Gąssowska, Elżbieta Jowik (Warsaw University of Techcnology) 6.4.1 An initial literature review As the Rashomon Effect is not a common concept, any references to the term in the literature are somewhat limited. The phenomenon is considered to occur when the same matter can be explained equally aptly in multitudinous ways. Hence the core of the name concept is the title of the Kurosawa’s movie from 1950 in which each character has different perspective on the same crime. In relation to Machine Learning the Rashomon Effect term was first used in (Breiman et al. 2001) to introduce a class of problems where many differing, accurate models exist to describe the same data i.e. to describe the case where there exist many models that are non-identical but almost-equally-accurate for a given issue. Breiman emphasized that the observation of many different accurate models on specific datasets is a common phenomenon. However, from 2001 on the topic has rarely been discussed. While doing research on different machine learning models, data was quite often not taken into consideration at all. As stated in the recent article (Semenova et al. 2019) Rashomon Effect is directly linked to the topic of Explainable Machine Learning. According to the paper, large volume of data in the Rashomon set might imply the existence of multiple explainable model performing on the dataset equally accurately. The article aims to analyze the Rashomon effect on various datasets and attempt to formulate a statement regarding the information about the machine learning problem carried by the size of the Rashomon set. Another matter closely related to the Rashomon effect that needs to be addressed, is the variables importance analysis. This area of research is described in an (A. Fisher et al. 2019d) article. The publication emphasizes the existence the fields where Explainable Machine Learning (including Rashomon effect) is particularly important, as the non-explainable models may rely on undesirable variables. In an (Dong and Rudin 2020) paper, it is pointed out that only by comparing many models of similar performance the importance of a variable compared to other variables can be profoundly understood. The authors presented the concept of variable importance cloud and conducted the research showing that the variable importance may dramatically differ in approximately equally good models. References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019d). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["rashomon-sets-of-in-hospital-mortality-prediction-random-forest-models.html", "6.5 Rashomon sets of in-hospital mortality prediction random forest models", " 6.5 Rashomon sets of in-hospital mortality prediction random forest models Authors: Jeugeniusz Winiczenko, Mikolaj Malec, Patryk Wrona (Warsaw University of Techcnology) 6.5.1 Abstract The concept of the Rashomon set is gaining more and more popularity in the machine learning world. However, the most efficient ways of building and analyzing such sets are yet to be discovered. The main aim of this study was to develop several approaches to creating Rashomon sets, examining their characteristics, and using them for further predictions. In this article, the results of Rashomon sets obtained from the group of random forest classifiers trained for in-hospital mortality prediction task on physiological time-series and medical histories from the Medical Information Mart for Intensive Care (MIMIC-III) are presented. 6.5.2 Introduction The main goals of this study were to check if * Rashomon sets can be better at predictions than single best models, * the way of obtaining predictions from Rashomon set has any impact on their performance,* Rashomon sets that consist of different models performs better than those with high performance but similar models. In this study, Rashomon set concept with top performance or the top most different random forest classifiers was used. These classifiers were trained for the in-hospital mortality prediction tasks on two datasets: the first one containing only physiological time-series and the second one containing both physiological time-series and medical histories. Both datasets were created from preprocessed data from the MIMIC-III database. To apply the created Rashomon sets for predicting, several techniques such as mean, median, weighted mean, etc. of votes of classifiers in each set were used. For sets’ prediction assessment area under the receiver operating characteristic curve(AUC) was used. One of the main goals of this study was also to check if Rashomon sets that are formed from different models perform better than those where models were similar. Different models were defined as those which have quite different sets of important variables. Different models which pay attention to different features were thought to be better at predicting just as the team of experts where each knows a different field would be. To verify this aspect, analysis of feature importance plots for each model from each Rashomon set were conducted, and models with different important features were united into sets. Furthermore, for better accommodation with the article, its structure is provided: Abstract Introduction Related Work MIMIC-III dataset - contains a description of both training datasets and their origin Rashomon sets - contains a description of performance results of best models, voting Rashomon sets and methods of voting Results - contains a summary of the most interesting results of this study Conclusion. 6.5.3 Related work Until recently, the Rashomon sets have been rarely a subject of scientific research. In 2019 (Semenova et al. 2019) approached the issue of creating mathematical and statistical definitions and notations regarding such sets of models. They described Rashomon sets as subspaces of the hypothesis space, that is subsets of models having comparable performance as the best model concerning a loss function. Rashomon sets are sets of models performing extraordinarily well on a given task, and in machine learning, this term was used for the very first time in (Breiman et al. 2001). Just as the task could be any, like predicting patient mortality in (Tang et al. 2018), the use of given features to explain vary among many highly accurate models. Moreover, studying the linear regression model, Leo Breiman described this situation as the Rashomon Effect. Another emphasis on analyzing Rashomon sets and the importance of their features were made in (A. Fisher et al. 2019d). The authors suggested the Model Class Reliance - new variable importance (VI) tool to study the range of VI values across all highly accurate models - models included in Rashomon sets. Later, (Rudin et al. 2021) provided basic rules for interpretable machine learning and identified 10 technical challenge areas in interpretable machine learning. They emphasized the troubleshooting and easiness of using glass-box models today as well as their advantage over black-box models due to their inscrutable nature. In this article, Challenge number 9 involves understanding, exploring, and measuring the Rashomon set. The authors address questions about how to characterize and visualize Rashomon sets, and finally, how to pick the best model out of the Rashomon set. In our work, we use variable importance plots from DALEX R Package to interpret the impact of each variable on the medical model output. We also use several voting systems to determine if the mutual voting of some subspace of models could outperform the best models. Furthermore, we address the problem of searching the most crucial predictive variables among those Rashomon sets and investigate the impact of choosing subsets of input features on the whole process of determining Rashomon sets and their characteristics. 6.5.4 MIMIC-III Dataset MIMIC III Clinical Database is a large database comprising de-identified health-related data associated with tens of patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. This database consists of 26 tables issued from different measurements during patients’ stays in the hospital. The preprocessing of these tables was conducted just like in (Tang et al. 2018). 6.5.4.1 X48 Variable Set This set consisted of 27616 observations having 76 predictive variables in total. It was created from the icu_stay.csv file and is in fact a preprocessed raw MIMIC table according to the reproduced article. It contains averaged statistics of 48-hour patients’ measurements (heart rate etc.). For each measurement maximal value, averaged value, minimum value, and standard deviation are denoted as separate variables. 6.5.4.2 W48 Variable Set This set consisted of 27616 observations having 276 predictive variables in total and was created from icu_stay.csv and d_icd_diagnoses.csv files and was in fact a preprocessed raw MIMIC tables according to the reproduced article. Just like X48, it contains averaged statistics of 48-hour patients’ measurements (heart rate, etc.) but is also combined with diagnosis histories. It is the combination of the X48 variable set and w2v embedding of medical events of all ICD-9 group codes. 6.5.5 Rashomon Sets Rashomon sets are sets of machine learning models performing especially well in the task of predicting in-hospital mortality. They can be chosen using a given criterion or metric. In this work, Rashomon sets were created using the Area Under ROC Curve metric, but also by finding the most different treating of predicting variables. The first Rashomon set was named ‘best AUC models’ and the second Rashomon set was called ‘experts’ because of their expertise on different predictors. To find such best models, 100 models were trained for each dataset using 3-fold cross-validation. Each time the validation set consisted of 20% of full data and the testing set was made of 10% of full data. We tried to verify how AUC changes depending on the amount of training data and the number of models included in a Rashomon set to find the most optimal number of models in such a set. The training was performed on 1%, 5%, 10%, 30%, and 70% of observations in the dataset. Furthermore, from the mentioned 100 models, 6 models with the highest AUC value were the best AUC models and 6 models with the most different variable importance were chosen for the ‘experts’ set. With these pairs of 6 models for 2 datasets(4 model sets in total) variable importance plots were checked to compare how it changes across different variable sets(X48 and W48) and different approaches of choosing the Rashomon set(best AUC or most different variable importance). We give the hyperparameters of these models in the next paragraphs. 6.5.5.1 Sets of best AUC models 6 models in rashomon sets built according to the AUC value. Hiperparameters of these models were found among best 20 models from 3-fold cross-validation and are given: For X48 Variable Set: Table 6.1: Hiperparameters for best AUC models of X48 dataset param_n_estimators param_min_samples_split param_min_samples_leaf param_max_features param_max_depth param_bootstrap 1400 2 4 sqrt 80 FALSE 2000 5 4 sqrt NaN FALSE 1200 2 4 sqrt 70 FALSE 600 2 4 sqrt 30 FALSE 1200 10 4 sqrt 20 FALSE 800 5 4 sqrt 30 FALSE For W48 Variable Set: Table 6.2: Hiperparameters for best AUC models of W48 dataset param_n_estimators param_min_samples_split param_min_samples_leaf param_max_features param_max_depth param_bootstrap 1800 2 2 sqrt 60 FALSE 1600 2 4 sqrt 110 FALSE 1600 10 4 sqrt 40 FALSE 600 10 4 sqrt 90 FALSE 1200 10 2 sqrt 70 FALSE 800 2 1 sqrt 20 FALSE 6.5.5.2 Sets of experts 6 rashomon sets built according to the most different important variables so that each of them treasure other sets of predictors. Hiperparameters of these models were found among best 20 models from 3-fold cross-validation and are given: For X48 Variable Set: Table 6.3: Hiperparameters for expert models of X48 dataset param_n_estimators param_min_samples_split param_min_samples_leaf param_max_features param_max_depth param_bootstrap 1400 2 4 sqrt 80 FALSE 1400 5 2 sqrt NaN FALSE 600 2 2 sqrt 110 FALSE 800 2 2 sqrt 50 FALSE 800 10 2 sqrt 30 FALSE 1000 5 2 sqrt 100 FALSE For W48 Variable Set: Table 6.4: Hiperparameters for expert models of W48 dataset param_n_estimators param_min_samples_split param_min_samples_leaf param_max_features param_max_depth param_bootstrap 1800 2 2 sqrt 60 FALSE 1600 2 4 sqrt 110 FALSE 400 10 1 sqrt 60 FALSE 200 10 2 sqrt 50 TRUE 2000 5 2 sqrt 10 TRUE 200 5 2 sqrt 10 TRUE 6.5.5.3 Methods of voting As an experiment, we used 7 methods of voting in each Rashomon set. We wanted to find out if there is any significant difference between them. Methods of voting were as follows: mean predictions of models median predictions of models mean predictions of models with weights equal to the score of the model mean predictions of models with weights equal to the score of the model transformed into the interval [0,1] mean predictions of models with weights equal to the score of the model transformed into the interval [1,2] mean predictions of models with weights equal to the rank of the model transformed into the interval [0,1], with 1 being the weight of the best model mean predictions of models with weights equal to the rank of the model transformed into the interval [1,2], with 2 being the weight of the best model Our results showed that all voting mechanisms behaved more or less likewise, with simple mean and median being in our best method to create a prediction for the group. It is also worth noticing that transforming score or rank to the interval [0,1] doesn’t take into account the worst models, so they are not the best way for efficient voting. 6.5.6 Results 6.5.6.1 Number of models in Rashomon set - influence on AUC Below are presented the results of the top performance model sets for different cardinalities of those sets. As one may notice there are no significant differences in AUC values between different cardinalities of the Rashomon sets and datasets on which models were trained. The best number of models that one can deduce from this figure is in the range from 5 to 10 models in a Rashomon set. As we mentioned earlier all voting strategies performed more or less the same. 6.5.6.2 Variable Importance PlotsIn this section, the analysis of feature importance plots is performed. 6.5.6.2.1 Best AUC Rashomon Sets Below feature importance plots for 6 models with the highest AUC are presented. For X48 dataset: There is no noticeable difference between important features for the X48 dataset. For W48 dataset: Of course, there is a noticeable difference between the most important variables across models trained on different datasets, even among variables comprised solely in the X48 dataset( having numbers up to 76). Interestingly, variable 46(mean_inr) has lost its’ dominance in the W48 dataset. 6.5.6.2.2 Experts Rashomon Sets Below are presented feature importance plots for models with possibly most different important features. The top 2 or 3 important features usually remain the same across models trained on the same dataset. All expert models emphasize the influence of X48 variables. Moreover, in 3 cases (out of 6 models) variables being less important than the 46(mean_inr) variable now gained importance. These variables are 27(model 2) and 56(model 4 &amp; model 5). 6.5.6.3 Voting in mortality prediction Below results of Rashomon experts sets for different voting strategies are presented. Sets that were created from models trained on the W48 dataset demonstrate significantly better AUC values than those trained on the X48 dataset. Unfortunately, expert sets did not show any better results than sets of top AUC models (denoted as a vertical line in these plots). 6.5.7 Conclusion In this article 2 different ways of creating Rashomon sets were discussed: choose first n of top performance models choose first n of most different models. In addition to that, several voting strategies of creating Rashomon sets for further predictions were tried out. During this whole work, the set of most important features of MIMIC-III for the mortality prediction task was discovered, which also may be useful for further researches or could give rise to new medical conclusions. Summing the results of all experiments up, one can conclude that Rashomon expert sets are worth the attention of researchers even though in this study they have slightly underperformed top performance model sets. Because of this result, we suggest there also be no bigger difference in the performance of voting strategies that were presented, and this may be the point to inventing and testing new strategies by further researchers. Furthermore, adding new variables to a model, just like adding new variables to the X48 variable set, may cause the old variables to lower their importance on the output of models among the Rashomon sets. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al., et al. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. USENIX conference on Operating Systems Design and Implementation. https://dl.acm.org/doi/10.5555/3026877.3026899 Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Aivodji, U., Arai, H., Fortineau, O., Gambs, S., Hara, S., &amp; Tapp, A. (2019). Fairwashing: The risk of rationalization. In K. Chaudhuri &amp; R. Salakhutdinov (Eds.), Proceedings of the 36th international conference on machine learning (Vol. 97, pp. 161–170). PMLR. http://proceedings.mlr.press/v97/aivodji19a.html Alkahtani, A. S., &amp; Jilani, M. (2019). Predicting return donor and analyzing blood donation time series using data mining techniques. International Journal of Advanced Computer Science and Applications, 10(8). https://doi.org/10.14569/IJACSA.2019.0100816 Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Antonio, N., de Almeida, A., &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., de Almeida, A., &amp; Nunes, L. (2019a). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Antonio, N., de Almeida, A., &amp; Nunes, L. (2019b). Hotel booking demand datasets. Data in Brief, 22, 41–49. https://doi.org/10.1016/j.dib.2018.11.126 Apley, D. (2018). ALEPlot: Accumulated Local Effects (ALE) Plots and Partial Dependence (PD) Plots. https://CRAN.R-project.org/package=ALEPlot Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Arik, S. O., &amp; Pfister, T. (2021). TabNet: Attentive Interpretable Tabular Learning. AAAI Conference on Artificial Intelligence (AAAI). https://arxiv.org/abs/1908.07442 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In 2019 IEEE/CVF international conference on computer vision workshop (ICCVW) (pp. 406–415). https://doi.org/10.1109/ICCVW.2019.00052 Bahel, D., Ghosh, P., Sarkar, A., Lanham, M. A., &amp; Lafayette, W. (2017). Predicting blood donations using machine learning techniques. http://matthewalanham.com/Students/2017_MWDSI_Final_Bahel.pdf Baker, M. (2016). Reproducibility crisis. Nature, 533(26), 353–66. Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020b). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barda, N., Riesel, D., Akriv, A., Levy, J., Finkel, U., Yona, G., et al. (2020). Developing a COVID-19 mortality risk prediction model when individual-level data are not available. Nature Communications, 11. https://doi.org/10.1038/s41467-020-18297-9 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2020). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3, 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020c). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020b). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020a). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Barsoum, E., Zhang, C., Ferrer, C. C., &amp; Zhang, Z. (2016). Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM international conference on multimodal interaction (pp. 279–283). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2993148.2993165 Belke, A., &amp; Keil, J. (2017). Fundamental determinants of real estate prices: A panel study of german regions, (731). Ruhr Economic Papers. https://doi.org/10.4419/86788851 Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105, 2752--2761. https://doi.org/10.1210/clinem/dgaa346 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Biecek, P. (2018b). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. https://jmlr.org/papers/v19/18-416.html Biecek, P. (2018a). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. http://jmlr.org/papers/v19/18-416.html Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Bisong, E. (2019). Google colaboratory. In Building machine learning and deep learning models on google cloud platform (pp. 59–64). Springer. Bradley, A. P. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recogn., 30(7), 1145–1159. https://doi.org/10.1016/S0031-3203(96)00142-2 Breiman, L. (1999). Random forests. UC Berkeley TR567. Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Can, A. (1990). The measurement of neighborhood dynamics in urban house prices. Economic Geography, 66(3), 254–272. https://doi.org/10.2307/143400 Cao, Y., Liu, X., Xiong, L., &amp; Cai, K. (2020). Imaging and clinical features of patients with 2019 novel coronavirus SARS-CoV-2: A systematic review and meta-analysis. Journal of Medical Virology, 92(9), 1449–1459. https://doi.org/10.1002/jmv.25822 Casadevall, A., &amp; Fang, F. C. (2010). Reproducible science. Infection and Immunity, 78(12), 4972–4975. https://doi.org/10.1128/IAI.00908-10 Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Chollet, F. (2017). Deep learning with python. Manning. Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Chowdhury, M. E. H., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M. A., Mahbub, Z. B., et al. (2020). Can AI help in screening viral and COVID-19 pneumonia? IEEE Access, 8, 132665–132676. https://doi.org/10.1109/ACCESS.2020.3010287 Christe, A. A. M. D., Andreas MD∗; Peters. (2019). Computer-aided diagnosis of pulmonary fibrosis using deep learning and CT images. Investigative Radiology, 54, 627–632. https://doi.org/10.1097/RLI.0000000000000574 Chu, X., Ilyas, I. F., Krishnan, S., &amp; Wang, J. (2016). Data cleaning: Overview and emerging challenges. In Proceedings of the 2016 international conference on management of data (pp. 2201–2206). Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., et al. (2013). The cancer imaging archive (TCIA): Maintaining and operating a public information repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Cohen, J. P., Morrison, P., Dao, L., Roth, K., Duong, T. Q., &amp; Ghassemi, M. (2020). COVID-19 image data collection: Prospective predictions are the future. arXiv 2006.11988. https://github.com/ieee8023/covid-chestxray-dataset Computing Machinery, A. for. (2018). Artifact review and badging. https://www.acm.org/publications/policies/artifact-review badging Conway, J. (2018, January). Artificial Intelligence and Machine Learning : Current Applications in Real Estate (PhD thesis). Retrieved from https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Darwiche, M., Feuilloy, M., Bousaleh, G., &amp; Schang, D. (2010). Prediction of blood transfusion donation, 51–56. https://doi.org/10.1109/RCIS.2010.5507363 Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Davis, J., &amp; Goadrich, M. (2006). The relationship between precision-recall and ROC curves. In Proceedings of the 23rd international conference on machine learning (pp. 233–240). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1143844.1143874 Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.-J., Sandhu, S., et al. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. The American Journal of Cardiology, 64(5), 304–310. https://doi.org/10.1016/0002-9149(89)90524-9 Detrano, R., Yiannikas, J., Salcedo, E. E., Rincon, G., Go, R. T., Williams, G., &amp; Leatherman, J. (1984). Bayesian probability analysis: A prospective demonstration of its clinical utility in diagnosing coronary disease. Circulation, 69(3), 541—547. https://doi.org/10.1161/01.CIR.69.3.541 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Du, X., Cai, Y., Wang, S., &amp; Zhang, L. (2016). Overview of deep learning. In 2016 31st youth academic annual conference of chinese association of automation (YAC) (pp. 159–164). IEEE. Dua, D., &amp; Graff, C. (2017). UCI machine learning repository. http://archive.ics.uci.edu/ml Dubin, R. A. (1998). Predicting house prices using multiple listings data. The Journal of Real Estate Finance and Economics. https://doi.org/10.1023/A:1007751112669 Dupuis, C., De Montmollin, E., Neuville, M., Mourvillier, B., Ruckly, S., &amp; Timsit, J. F. (2021). Limited applicability of a COVID-19 specific mortality prediction rule to the intensive care setting. Nature Machine Intelligence, 3(1), 20–22. https://doi.org/10.1038/s42256-020-00252-4 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 England, R. (2019). Wine’s alcohol levels explained. https://www.wineinvestment.com/wine-blog/2019/05/wines-alcohol-levels-explained?fbclid=IwAR3xpQITEQZrQUPPaEt7-DbFHmvHE559-iVuLsgS6dDinOeWrl04MZiglbM. Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Fan, C., Cui, Z., &amp; Zhong, X. (2018). House prices prediction with machine learning algorithms. In Proceedings of the 2018 10th international conference on machine learning and computing (pp. 6–10). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3195106.3195133 Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/https://doi.org/10.1016/j.patrec.2005.10.010 Finlayson, S. G., Chung, H. W., Kohane, I. S., &amp; Beam, A. L. (2018). Adversarial attacks against medical deep learning systems. arXiv preprint arXiv:1804.05296. Fisher, A., Rudin, C., &amp; Dominici, F. (2018). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. arXiv. https://arxiv.org/abs/1801.01489 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Fisher, A., Rudin, C., &amp; Dominici, F. (2019c). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Fisher, A., Rudin, C., &amp; Dominici, F. (2019d). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Fisher, R. A. (1922). On the interpretation of χ2 from contingency tables, and the calculation of p. Journal of the Royal Statistical Society, 85(1), 87–94. http://www.jstor.org/stable/2340521 Friedman, J. H. (2000a). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Friedman, J. H. (2000b). Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics, 29, 1189–1232. https://doi.org/10.1214/aos/1013203451 Ge, X., Runeson, G., &amp; Lam, K. C. (2021). Forecasting hong kong housing prices: An artificial neural network approach. Genders, T. S. S., Steyerberg, E. W., Alkadhi, H., Leschka, S., Desbiolles, L., Nieman, K., et al. (2011). A clinical prediction rule for the diagnosis of coronary artery disease: Validation, updating, and extension. European Heart Journal, 32(11), 1316–1330. https://doi.org/10.1093/eurheartj/ehr014 Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Ghysels, E., Plazzi, A., Valkanov, R., &amp; Torous, W. (2013). Chapter 9 - forecasting real estate prices, 2, 509–580. https://doi.org/https://doi.org/10.1016/B978-0-444-53683-9.00009-8 Glauner, P. (2021). An assessment of the AI regulation proposed by the european commission. https://arxiv.org/abs/2105.15133 GOLDNER, M. C., ZAMORA, M. C., DI LEO LIRA, P., GIANNINOTO, H., &amp; BANDONI, A. (2009). EFFECT OF ETHANOL LEVEL IN THE PERCEPTION OF AROMA ATTRIBUTES AND THE DETECTION OF VOLATILE COMPOUNDS IN RED WINE. Journal of sensory studies, 24(2), 243–257. Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2014). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Goldstein, A., Kapelner, A., Bleich, J., &amp; Pitkin, E. (2015). Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation. Journal of Computational and Graphical Statistics, 24(1), 44–65. https://doi.org/10.1080/10618600.2014.907095 Goodman, B., &amp; Flaxman, S. (2017). European union regulations on algorithmic decision-making and a “right to explanation.” AI Magazine, 38(3), 50–57. https://doi.org/10.1609/aimag.v38i3.2741 Gosiewska, A., &amp; Biecek, P. (2019). Do Not Trust Additive Explanations. arXiv. https://arxiv.org/abs/1903.11420v3 Goyal, S. (2020, November). Credit card customers. Kaggle. https://www.kaggle.com/sakshigoyal7/credit-card-customers Greenwell, B. M. (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421–436. http://doi.org/10.32614/RJ-2017-016 Gregutt, P. (2003). Does a higher alcohol content mean it’s a better drinking wine? The Seattle Times. https://archive.seattletimes.com/archive/?date=20031008&amp;slug=wineqanda08&amp;fbclid=IwAR3lBlpdwUCUWjWKaH4Px21b9fJQwBT0aMTa8bNWCbx4ipo4otWzvR9_mTc Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Heyman, A., &amp; Sommervoll, D. (2019). House prices and relative location. Cities, 95, 102373. https://doi.org/10.1016/j.cities.2019.06.004 Holzinger, A. (2016). Interactive machine learning for health informatics: When do we need the human-in-the-loop? Brain Informatics, 3, 119–131. https://doi.org/10.1007/s40708-016-0042-6 Holzinger, A. (2021). Explainable AI and multi-modal causability in medicine. i-com, 19(3), 171–179. https://doi.org/10.1515/icom-2020-0024 Holzinger, A., Biemann, C., Pattichis, C., &amp; Kell, D. (2017). What do we need to build explainable AI systems for the medical domain? Holzinger, A., Langs, G., Denk, H., Zatloukal, K., &amp; Müller, H. (2019). Causability and explainabilty of artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9. https://doi.org/10.1002/widm.131 Jefford, A. (2010). Alcohol levels: The balancing act. https://www.decanter.com/features/alcohol-levels-the-balancing-act-246426/?fbclid=IwAR0bsIWug6-7l77rxb01Va8P1F_hVkaUTacNtlF-V-wRXb1HA3rJXpl74Pw. Johnson, A. E., Pollard, T. J., Shen, L., Li-Wei, H. L., Feng, M., Ghassemi, M., et al. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1–9. Jordão, A. M., Vilela, A., &amp; Cosme, F. (2015). From sugar of grape to alcohol of wine: Sensorial impact of alcohol in wine. Beverages, 1(4), 292–310. https://doi.org/10.3390/beverages1040292 Kaladharan, S., Vishvanathan, S., Gopalakrishnan, E. A., &amp; Kp, S. (2020). Explainable artificial intelligence for heart rate variability in ECG signal. Healthcare Technology Letters, 7, 146–154. https://doi.org/10.1049/htl.2020.0033 Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Khedkar, S., Subramanian, V., Shinde, G., &amp; Gandhi, P. (2019). Explainable AI in healthcare. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3367686 Kieseberg, P., Schantl, J., Fruehwirt, P., Weippl, E., &amp; Holzinger, A. (2015). Witnesses for the doctor in the loop, 9250, 369–378. https://doi.org/10.1007/978-3-319-23344-4_36 Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. R package. https://github.com/ModelOriented/treeshap Kowsari, K., Brown, D. E., Heidarysafa, M., Jafari Meimandi, K., Gerber, M. S. and, &amp; Barnes, L. E. (2017). HDLTex: Hierarchical deep learning for text classification. In Machine learning and applications (ICMLA), 2017 16th IEEE international conference on. IEEE. Kowsari, K., Heidarysafa, M., Brown, D. E., Meimandi, K. J., &amp; Barnes, L. E. (2018). Rmdl: Random multimodel deep learning for classification. In Proceedings of the 2nd international conference on information system and data mining (pp. 19–28). Krittanawong, C., Zhang, H., Wang, Z., Aydar, M., &amp; Kitai, T. (2017). Artificial intelligence in precision cardiovascular medicine. Journal of the American College of Cardiology, 69(21), 2657–2664. https://doi.org/10.1016/j.jacc.2017.03.571 Law, S. (2017). Defining street-based local area and measuring its effect on house price using a hedonic price approach: The case study of metropolitan london. Cities, 60, 166–179. https://doi.org/10.1016/j.cities.2016.08.008 Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Liaw, A., &amp; Wiener, M. (2002). Classification and regression by randomForest. R News, 2(3), 18–22. https://CRAN.R-project.org/doc/Rnews/ Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Loyola-González, O. (2019). Black-box vs. White-box: Understanding their advantages and weaknesses from a practical point of view. IEEE Access, 7, 154096–154113. https://doi.org/10.1109/ACCESS.2019.2949286 Łukasz Rączkowski, J. Z., Marcin Możejko. (2019). ARA: Accurate, reliable and active histopathological image classification framework with bayesian deep learning. Springer Nature, 14, 1–11. https://doi.org/10.1038/s41598-019-50587-1 Lundberg, Scott M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf M. Barhoom, A., Abu-Naser, S., Abu-Nasser, B., Alajrami, E., Musleh, M., &amp; Khalil, A. (2019). Blood donation prediction using artificial neural network, 1–7. https://philarchive.org/archive/BARBDP-14 Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2021). Landscape of r packages for eXplainable artificial intelligence. https://arxiv.org/abs/2009.13248 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F., Chang, C.-C., &amp; Lin, C.-C. (2021). e1071: Misc Functions of the Department of Statistics, Probability Theory Group. R package. https://CRAN.R-project.org/package=e1071 Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546. http://arxiv.org/abs/1310.4546 Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Neff, T., Payer, C., Stern, D., &amp; Urschler, M. (2017). Generative adversarial network based synthesis for supervised medical image segmentation. In Proc. OAGM and ARW joint workshop. Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., et al., et al. (2018). Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999. Pace, R. K., &amp; Barry, R. (1997). Sparse spatial autoregressions. Statistics &amp; Probability Letters, 33(3), 291–297. Pandala, S. R. (2019). Lazy Predict. Python package. https://github.com/shankarpandala/lazypredict Park, B., &amp; Bae, J. (2015). Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data. Expert Systems with Applications, 42. https://doi.org/10.1016/j.eswa.2014.11.040 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011a). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011b). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Pedregosa, Fabian, Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Pekala, K., Woznica, K., &amp; Biecek, P. (2021). Triplot: Model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure. CoRR, abs/2104.03403. https://arxiv.org/abs/2104.03403 Peng, Z., Huang, Q., &amp; Han, Y. (2019). Model research on forecast of second-hand house price in chengdu based on XGboost algorithm, 168–172. https://doi.org/10.1109/ICAIT.2019.8935894 Pineau, J., Vincent-Lamarre, P., Sinha, K., Larivière, V., Beygelzimer, A., d’Alché-Buc, F., et al. (2020). Improving reproducibility in machine learning research (A report from the NeurIPS 2019 reproducibility program). CoRR, abs/2003.12206. https://arxiv.org/abs/2003.12206 Probst, P., Boulesteix, A.-L., &amp; Bischl, B. (2019). Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53), 1–32. http://jmlr.org/papers/v20/18-444.html Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Quanjel, M. J. R., Holten, T. C. van, Vliet, P. C. G. der, Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in Dutch patients with COVID-19. Nature Machine Intelligence, 3, 23–24. https://doi.org/10.1038/s42256-020-00253-3 R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ Raghavan, V., Bollmann, P., &amp; Jung, G. S. (1989). A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3), 205–229. https://doi.org/10.1145/65943.65945 Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Abul Kashem, S. B., et al. (2021). Exploring the effect of image enhancement techniques on COVID-19 detection using chest x-ray images. Computers in Biology and Medicine, 132, 104319. https://doi.org/https://doi.org/10.1016/j.compbiomed.2021.104319 Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48, 137–141. https://link.springer.com/article/10.1007/s11747-019-00710-5 Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016a). \"Why should I trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, KDD san francisco, CA (pp. 1135–1144). New York, NY: Association for Computing Machinery. Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016b). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, san francisco, CA, USA, august 13-17, 2016 (pp. 1135–1144). https://doi.org/10.18653/v1/n16-3020 Ruder, S. (2017). An overview of multi-task learning in deep neural networks. https://arxiv.org/abs/1706.05098 Rudin, C. (2019b). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206–215. https://doi.org/10.1038/s42256-019-0048-x Rudin, C. (2019a). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1, 206–215. https://doi.org/10.1038/s42256-019-0048 Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Saarela, M., &amp; Jauhiainen, S. (2021). Comparison of feature importance measures as explanations for classification models. SN Applied Sciences, 3(2). https://doi.org/10.1007/s42452-021-04148-9 Saito, T., &amp; Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. Plos One, 10(3). https://doi.org/10.1371/journal.pone.0118432 Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Sandfort, V., Yan, K., Pickhardt, P. J., &amp; Summers, R. M. (2019). Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks. Scientific reports, 9(1), 1–9. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Slack, D., Hilgard, S., Jia, E., Singh, S., &amp; Lakkaraju, H. (2020). Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM conference on AI, ethics, and society (pp. 180–186). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3375627.3375830 Smith, S. J., Parsa, H. G., Bujisic, M., &amp; van der Rest, J.-P. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 Staniak, M., &amp; Biecek, P. (2018a). Explanations of model predictions with live and breakDown packages. Staniak, M., &amp; Biecek, P. (2018b). Explanations of Model Predictions with live and breakDown Packages. The R Journal, 10(2), 395–409. https://doi.org/10.32614/RJ-2018-072 Suzuki, K. (2017). Overview of deep learning in medical imaging. Radiological physics and technology, 10(3), 257–273. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. Tatman, R., VanderPlas, J., &amp; Dane, S. (2018). A practical taxonomy of reproducibility for machine learning research. Thompson, N. C., Greenewald, K., Lee, K., &amp; Manso, G. F. (2020). The computational limits of deep learning. arXiv preprint arXiv:2007.05558. Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Ucar, F., &amp; Korkmaz, D. (2020). COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from x-ray images. Medical Hypotheses, 140, 109761–109761. Vandewalle, P., Kovacevic, J., &amp; Vetterli, M. (2009). Reproducible research in signal processing. IEEE Signal Processing Magazine, 26(3), 37–47. Vanschoren, J., Rijn, J. N. van, Bischl, B., &amp; Torgo, L. (2013). OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2), 49–60. https://doi.org/10.1145/2641190.2641198 Varma, A., Sarma, A., Doshi, S., &amp; Nair, R. (2018). House price prediction using machine learning and neural networks, 1936–1939. https://doi.org/10.1109/ICICCT.2018.8473231 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762. Vayena E, C. I., Blasimme A. (2018). Machine learning in medicine: Addressing ethical challenges. PLOT Medicine, 15(11), 1–4. https://doi.org/10.1371/journal.pmed.1002689 Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, L., Lin, Z. Q., &amp; Wong, A. (2020a). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, L., Lin, Z. Q., &amp; Wong, A. (2020b). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Wright, M. N., &amp; Ziegler, A. (2016). XGBoost: A Scalable Tree Boosting System. SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785 Wright, M. N., &amp; Ziegler, A. (2017b). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 Wright, M. N., &amp; Ziegler, A. (2017a). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html XGBoost: A scalable tree boosting system. (2016). CoRR, abs/1603.02754. http://arxiv.org/abs/1603.02754 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. Zaimi Aldo, W. M., Herman, V., Antonsanti, P.-L., Perone, C. S., &amp; Cohen-Adad, J. (2018). AxonDeepSeg: Automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. https://doi.org/10.1038/s41598-018-22181-4 Zhao, Q., Meng, M., Kumar, R., Wu, Y., Huang, J., Deng, Y., et al. (2020). Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysis. International Journal of Infectious Diseases, 96, 131–135. https://doi.org/10.1016/j.ijid.2020.04.086 Zhao, Y., Chetty, G., &amp; Tran, D. (2019). Deep learning with XGBoost for real estate appraisal, 1396–1401. https://doi.org/10.1109/SSCI44817.2019.9002790 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020a). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(6), 100092. https://doi.org/10.1016/j.patter.2020.100092 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020b). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 Zhou, Z.-H., &amp; Feng, J. (2017). Deep forest: Towards an alternative to deep neural networks. CoRR, abs/1702.08835. http://arxiv.org/abs/1702.08835 Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., &amp; Liang, J. (2019). UNet++: Redesigning skip connections to exploit multiscale features in image segmentation. IEEE Transactions on Medical Imaging. References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Fisher, A., Rudin, C., &amp; Dominici, F. (2019d). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. "]]
